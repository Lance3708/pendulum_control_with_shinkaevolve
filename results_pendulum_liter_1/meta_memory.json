{
  "unprocessed_programs": [
    {
      "id": "68c67e8d-25dd-4122-b74d-eb72f3866179",
      "code": "import numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Energy-Aware Adaptive Cross-Coupled LQR with Enhanced Mid-Swing Damping,\n    Predictive Compensation, and Dynamic Integral Scaling\n    \n    Integrates top-performer base with rec1-5 innovations for breakthrough performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        self.m = M_POLE\n        self.M = M_CART\n        self.l = L_COM\n        self.g = G\n        self.Mtot = self.M + self.m\n        self.denom0 = self.l * (4.0 / 3.0 - self.m / self.Mtot)\n        self.b_c = FRICTION_CART\n        self.b_j = FRICTION_JOINT\n        self.L_pole = L_POLE\n\n        # Build base A matrix (friction-aware linearization)\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = self.g / self.denom0\n        A[3, 2] = self.b_c / (self.Mtot * self.denom0)\n        A[3, 3] = -self.b_j / (self.m * self.l * self.denom0)\n        A[2, 1] = -(self.m * self.l / self.Mtot) * A[3, 1]\n        A[2, 2] = -self.b_c / self.Mtot - (self.m * self.l / self.Mtot) * A[3, 2]\n        A[2, 3] = self.b_j / (self.Mtot * self.denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / self.Mtot + (self.m * self.l) / (self.Mtot**2 * self.denom0)\n        B[3, 0] = -1.0 / (self.Mtot * self.denom0)\n\n        # Base Q with cross-coupling\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        Q[1, 3] = 0.8\n        Q[3, 1] = 0.8\n        R = np.array([[1.0]])\n\n        self.A = A\n        self.B = B\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Extract gains for adaptive cross-coupling\n        self.K_theta = self.K[0, 1]\n        self.K_dtheta = self.K[0, 3]\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i_base = 0.85\n\n        # Natural frequency and thresholds\n        self.omega_n = np.sqrt(G / L_COM)\n        self.theta_thresh = 0.8 * np.sqrt(L_COM / 2.5)  # rec4: inertia-scaled\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Advanced adaptive control with all enhancements\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        \n        base_force = -self.K @ state_vec\n\n        # Adaptive cross-coupling enhancement (rec3)\n        cross_scale = 1.0 + 0.6 * np.tanh(2.5 * abs(theta))\n        theta_dtheta_effect = self.K_theta * theta + self.K_dtheta * dtheta\n        delta_cross = (cross_scale - 1.0) * theta_dtheta_effect\n        base_force_enhanced = base_force + delta_cross\n\n        # Adaptive gain scheduling\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force_enhanced * adaptive_gain\n\n        # Mid-swing damping (primary Gaussian)\n        mid_swing_activation = np.exp(-4.0 * (abs(theta) - 0.5)**2)\n        if abs(theta) > 0.25 and abs(theta) < 0.85:\n            K_d_midswing = 2.5\n            midswing_damp = -K_d_midswing * dtheta * mid_swing_activation\n            force += midswing_damp\n\n        # Supplemental derivative damping mid-swing (rec2: 0.3-0.8 rad focus)\n        damp_factor = 1.0 - np.tanh(5.0 * (abs(theta) - 0.55))\n        force += -0.18 * dtheta * damp_factor\n\n        # Predictive momentum compensation (rec5)\n        sin_theta = np.sin(theta)\n        cos_theta = np.cos(theta)\n        theta_acc_est = (self.g * sin_theta - self.b_j * dtheta / (self.m * self.l)) / \\\n                        (self.l * (4.0/3.0 - self.m * cos_theta**2 / self.Mtot))\n        dtheta_pred = dtheta + theta_acc_est * DT\n        pred_divergence = theta * dtheta_pred / (self.omega_n * self.l)\n        predictive_correction = -0.12 * pred_divergence * np.tanh(2.0 * abs(theta))\n        force += predictive_correction\n\n        # Swing-up assist with inertia-scaled threshold (rec4)\n        if abs(theta) > self.theta_thresh:\n            swing_activation = np.tanh(6.0 * (abs(theta) - self.theta_thresh))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (self.l * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force += u_swing\n\n        # Energy-aware dynamic Ki scaling (rec1)\n        E_kinetic = 0.5 * self.m * self.l**2 * dtheta**2\n        E_potential = self.m * self.g * self.l * (1.0 - np.cos(theta))\n        E_current = E_kinetic + E_potential\n        E_ref = self.m * self.g * self.l\n        energy_deficit = max(0.0, 1.0 - E_current / E_ref)\n        K_i_dynamic = self.K_i_base * (1.0 + 0.25 * energy_deficit)\n\n        # Dual-gated integral action\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n            self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n        else:\n            self.integral_x *= 0.95\n\n        integral_force = K_i_dynamic * integral_gate * self.integral_x\n        force += integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)",
      "language": "python",
      "parent_id": "c30ebaec-42b4-48d0-b4f0-f19c425b732b",
      "archive_inspiration_ids": [
        "25b4ae6f-8b51-43b3-8645-ef717a30c6a8",
        "5b887dbe-5782-4d18-b925-d3aba05a63a0",
        "78f9bdd2-4ccf-4023-853a-c0a66c6c4494",
        "713a353b-d8eb-4be9-b10f-e2b6e6760851"
      ],
      "top_k_inspiration_ids": [
        "b96c292c-76c6-47f8-87cb-d8f9c85c6bc5",
        "12a42191-3260-4309-89e5-f37375409215",
        "892e1461-ab04-4545-be59-ef1ec2e866b5"
      ],
      "island_idx": 0,
      "generation": 175,
      "timestamp": 1764096463.7146933,
      "code_diff": "--- a/original.py\n+++ b/original.py\n@@ -1,227 +1,252 @@\n import numpy as np\n \n # --- Physics Constants ---\n M_CART = 1.0       # Mass of the cart (kg)\n M_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\n L_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\n L_COM = L_POLE / 2 # Length to center of mass (m)\n G = 9.81           # Gravity (m/s^2)\n FRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\n FRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\n DT = 0.02          # Time step (s)\n MAX_STEPS = 1000   # 20 seconds simulation\n \n def simulate_pendulum_step(state, force, dt):\n     \"\"\"\n     Simulates one time step of the Single Inverted Pendulum.\n \n     State vector: [x, theta, dx, dtheta]\n     - x: Cart position (m)\n     - theta: Pole angle (rad), 0 is upright\n     - dx: Cart velocity (m/s)\n     - dtheta: Pole angular velocity (rad/s)\n \n     Args:\n         state: numpy array of shape (4,)\n         force: scalar float, force applied to the cart (N)\n         dt: float, time step (s)\n \n     Returns:\n         next_state: numpy array of shape (4,)\n     \"\"\"\n     x, theta, dx, dtheta = state\n \n     # Precompute trig terms\n     sin_theta = np.sin(theta)\n     cos_theta = np.cos(theta)\n \n     # Equations of Motion (Non-linear)\n     # derived from Lagrangian dynamics\n \n     # Total mass\n     M_total = M_CART + M_POLE\n \n     # Friction forces\n     f_cart = -FRICTION_CART * dx\n     f_joint = -FRICTION_JOINT * dtheta\n \n     # Denominator for solving linear system of accelerations\n     # Derived from solving the system:\n     # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n     # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n \n     temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n \n     theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                 (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n \n     x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n \n     # Euler integration\n     next_x = x + dx * dt\n     next_theta = theta + dtheta * dt\n     next_dx = dx + x_acc * dt\n     next_dtheta = dtheta + theta_acc * dt\n \n     return np.array([next_x, next_theta, next_dx, next_dtheta])\n \n \n # EVOLVE-BLOCK-START\n class Controller:\n     \"\"\"\n-    Robust Swing-Up LQR with Energy-Based Assist and Optimized Damping\n-    Combines precise LQR stabilization with intelligent swing-up assistance\n-    for faster convergence while maintaining energy efficiency.\n+    Energy-Aware Adaptive Cross-Coupled LQR with Enhanced Mid-Swing Damping,\n+    Predictive Compensation, and Dynamic Integral Scaling\n+    \n+    Integrates top-performer base with rec1-5 innovations for breakthrough performance.\n     \"\"\"\n \n     def __init__(self):\n         # System parameters\n-        m = M_POLE\n-        M = M_CART\n-        l = L_COM\n-        g = G\n-        Mtot = M + m\n-        denom0 = l * (4.0 / 3.0 - m / Mtot)\n-        b_c = FRICTION_CART\n-        b_j = FRICTION_JOINT\n-\n-        # Physically accurate linearized A matrix\n+        self.m = M_POLE\n+        self.M = M_CART\n+        self.l = L_COM\n+        self.g = G\n+        self.Mtot = self.M + self.m\n+        self.denom0 = self.l * (4.0 / 3.0 - self.m / self.Mtot)\n+        self.b_c = FRICTION_CART\n+        self.b_j = FRICTION_JOINT\n+        self.L_pole = L_POLE\n+\n+        # Build base A matrix (friction-aware linearization)\n         A = np.zeros((4, 4))\n         A[0, 2] = 1.0\n         A[1, 3] = 1.0\n-\n-        # theta_acc row (3) - matches simulator physics\n-        A[3, 1] = g / denom0\n-        A[3, 2] = b_c / (Mtot * denom0)\n-        A[3, 3] = -b_j / (m * l * denom0)\n-\n-        # x_acc row (2)\n-        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n-        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n-        A[2, 3] = b_j / (Mtot * denom0)\n+        A[3, 1] = self.g / self.denom0\n+        A[3, 2] = self.b_c / (self.Mtot * self.denom0)\n+        A[3, 3] = -self.b_j / (self.m * self.l * self.denom0)\n+        A[2, 1] = -(self.m * self.l / self.Mtot) * A[3, 1]\n+        A[2, 2] = -self.b_c / self.Mtot - (self.m * self.l / self.Mtot) * A[3, 2]\n+        A[2, 3] = self.b_j / (self.Mtot * self.denom0)\n \n         # B matrix\n         B = np.zeros((4, 1))\n-        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n-        B[3, 0] = -1.0 / (Mtot * denom0)\n-\n-        # Optimized LQR weights with enhanced angular velocity damping\n-        Q = np.diag([4.5, 44.0, 0.6, 3.28])  # Q[3] increased from 3.2 to 3.28\n+        B[2, 0] = 1.0 / self.Mtot + (self.m * self.l) / (self.Mtot**2 * self.denom0)\n+        B[3, 0] = -1.0 / (self.Mtot * self.denom0)\n+\n+        # Base Q with cross-coupling\n+        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n+        Q[1, 3] = 0.8\n+        Q[3, 1] = 0.8\n         R = np.array([[1.0]])\n \n-        # Solve LQR gains\n+        self.A = A\n+        self.B = B\n         self.K = self.solve_lqr(A, B, Q, R)\n \n-        # Precompute natural frequency for normalized falling severity\n+        # Extract gains for adaptive cross-coupling\n+        self.K_theta = self.K[0, 1]\n+        self.K_dtheta = self.K[0, 3]\n+\n+        # Integral control parameters\n+        self.integral_x = 0.0\n+        self.K_i_base = 0.85\n+\n+        # Natural frequency and thresholds\n         self.omega_n = np.sqrt(G / L_COM)\n-\n-        # Integral control state for eliminating steady-state cart position error\n-        self.integral_x = 0.0\n-        self.K_i = 0.8  # Increased integral gain for better steady-state error elimination\n+        self.theta_thresh = 0.8 * np.sqrt(L_COM / 2.5)  # rec4: inertia-scaled\n \n     def solve_lqr(self, A, B, Q, R):\n         \"\"\"Solve continuous-time LQR\"\"\"\n         from scipy.linalg import solve_continuous_are\n         P = solve_continuous_are(A, B, Q, R)\n         K = np.linalg.inv(R) @ B.T @ P\n         return K\n \n     def get_action(self, state):\n-        \"\"\"Robust LQR control with directional momentum swing-up and integral action\"\"\"\n+        \"\"\"Advanced adaptive control with all enhancements\"\"\"\n         x, theta, dx, dtheta = state\n \n-        # Robust angle normalization using arctan2 for consistent wrapping\n-        theta = np.arctan2(np.sin(theta), np.cos(theta))\n+        # Robust angle normalization\n+        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n \n         state_vec = np.array([x, theta, dx, dtheta])\n+        \n         base_force = -self.K @ state_vec\n \n-        # Enhanced gain scheduling with smoother transitions\n+        # Adaptive cross-coupling enhancement (rec3)\n+        cross_scale = 1.0 + 0.6 * np.tanh(2.5 * abs(theta))\n+        theta_dtheta_effect = self.K_theta * theta + self.K_dtheta * dtheta\n+        delta_cross = (cross_scale - 1.0) * theta_dtheta_effect\n+        base_force_enhanced = base_force + delta_cross\n+\n+        # Adaptive gain scheduling\n         pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n         vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n         adaptive_gain = pos_gain * vel_gain\n-        force = base_force * adaptive_gain\n-\n-        # Energy-based swing-up assist with directional momentum awareness\n-        if abs(theta) > 0.8:\n-            # Physics-informed assist with normalized falling severity\n-            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n-\n-            # Directional momentum: signed quantity preserving recovery vs divergence\n-            # Positive when diverging (theta and dtheta same sign), negative when recovering\n-            signed_momentum = theta * dtheta\n-            normalized_severity = signed_momentum / (L_COM * self.omega_n)\n-\n-            # Asymmetric response: stronger when diverging, gentler when recovering\n-            if normalized_severity > 0:\n-                falling_severity = 1.0 + np.tanh(3.0 * normalized_severity)\n-            else:\n-                # When recovering, reduce assist to avoid overshoot\n-                falling_severity = 1.0 + 0.5 * np.tanh(2.0 * normalized_severity)\n-\n-            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity\n-            force = force + u_swing\n-\n-        # Soft-switched integral action for cart position with enhanced gating\n-        # Only activates when pole is nearly upright AND moving slowly to prevent windup\n+\n+        force = base_force_enhanced * adaptive_gain\n+\n+        # Mid-swing damping (primary Gaussian)\n+        mid_swing_activation = np.exp(-4.0 * (abs(theta) - 0.5)**2)\n+        if abs(theta) > 0.25 and abs(theta) < 0.85:\n+            K_d_midswing = 2.5\n+            midswing_damp = -K_d_midswing * dtheta * mid_swing_activation\n+            force += midswing_damp\n+\n+        # Supplemental derivative damping mid-swing (rec2: 0.3-0.8 rad focus)\n+        damp_factor = 1.0 - np.tanh(5.0 * (abs(theta) - 0.55))\n+        force += -0.18 * dtheta * damp_factor\n+\n+        # Predictive momentum compensation (rec5)\n+        sin_theta = np.sin(theta)\n+        cos_theta = np.cos(theta)\n+        theta_acc_est = (self.g * sin_theta - self.b_j * dtheta / (self.m * self.l)) / \\\n+                        (self.l * (4.0/3.0 - self.m * cos_theta**2 / self.Mtot))\n+        dtheta_pred = dtheta + theta_acc_est * DT\n+        pred_divergence = theta * dtheta_pred / (self.omega_n * self.l)\n+        predictive_correction = -0.12 * pred_divergence * np.tanh(2.0 * abs(theta))\n+        force += predictive_correction\n+\n+        # Swing-up assist with inertia-scaled threshold (rec4)\n+        if abs(theta) > self.theta_thresh:\n+            swing_activation = np.tanh(6.0 * (abs(theta) - self.theta_thresh))\n+            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (self.l * self.omega_n))\n+            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n+            force += u_swing\n+\n+        # Energy-aware dynamic Ki scaling (rec1)\n+        E_kinetic = 0.5 * self.m * self.l**2 * dtheta**2\n+        E_potential = self.m * self.g * self.l * (1.0 - np.cos(theta))\n+        E_current = E_kinetic + E_potential\n+        E_ref = self.m * self.g * self.l\n+        energy_deficit = max(0.0, 1.0 - E_current / E_ref)\n+        K_i_dynamic = self.K_i_base * (1.0 + 0.25 * energy_deficit)\n+\n+        # Dual-gated integral action\n         angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n         velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n         integral_gate = angle_gate * velocity_gate\n \n         if integral_gate > 0.1:\n             self.integral_x += x * DT\n-            # Anti-windup: limit integral magnitude\n             self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n         else:\n-            # Angle-proportional decay: stronger decay when angle is large\n-            decay_rate = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n-            self.integral_x *= decay_rate\n-\n-        integral_force = self.K_i * integral_gate * self.integral_x\n-        force = force + integral_force\n+            self.integral_x *= 0.95\n+\n+        integral_force = K_i_dynamic * integral_gate * self.integral_x\n+        force += integral_force\n \n         return float(force[0])\n \n # Initialize controller\n controller = Controller()\n \n def get_control_action(state):\n     force = controller.get_action(state)\n-    # Apply force limits\n     return float(np.clip(force, -100.0, 100.0))\n # EVOLVE-BLOCK-END\n \n def run_simulation(seed=None):\n     \"\"\"\n     Runs the simulation loop.\n     \"\"\"\n     if seed is not None:\n         np.random.seed(seed)\n \n     # Initial state: 0.4 rad (~23 degrees)\n     # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n     state = np.array([0.0, 0.9, 0.0, 0.0])\n \n     states = [state]\n     forces = []\n \n     for _ in range(MAX_STEPS):\n         force = get_control_action(state)\n         # Clip force to realistic limits\n         force = np.clip(force, -100.0, 100.0)\n \n         next_state = simulate_pendulum_step(state, force, DT)\n \n         states.append(next_state)\n         forces.append(force)\n \n         state = next_state\n \n         # # Early termination checks\n         # if np.any(np.isnan(state)):\n         #     break\n         # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n         # if abs(state[1]) > 1.0:\n         #     break\n \n     return np.array(states), np.array(forces)",
      "combined_score": 0.0,
      "public_metrics": {
        "score": 0.0,
        "base_score": 10.939392272031206,
        "time_bonus": 1.0063878837075355,
        "energy_bonus": 3.759735647256262e-07,
        "success_bonus": 0.0,
        "stabilization_time": 1000,
        "stabilization_ratio": 1.0,
        "avg_energy_per_step": 0.9458864610649634,
        "total_energy": 945.8864610649634,
        "mean_step_reward": 0.010939392272031206,
        "final_theta_error": 3.112949117141774,
        "final_x_error": 4179.886374061643,
        "stable_theta_error": 2.079842915719763,
        "stable_x_error": 1626.7272611727374
      },
      "private_metrics": {
        "oscillation_penalty": -4.0,
        "angle_oscillation_penalty": -464.0,
        "boundary_penalty": -2000.0,
        "unstable_penalty": -500.0,
        "max_theta": 3.14093187729042,
        "max_x": 4179.886374061643,
        "max_force": 100.0,
        "force_std": 30.696044483138973
      },
      "text_feedback": "",
      "correct": false,
      "children_count": 0,
      "complexity": 1.0,
      "embedding": [
        -0.00874705,
        0.008740104,
        0.013912787,
        -0.09119484,
        -0.008372284,
        0.008354793,
        -0.012356368,
        0.013049646,
        0.017832713,
        -0.010247992,
        0.0051881396,
        -0.030188538,
        -0.0012468754,
        0.020991672,
        0.12864815,
        0.0019770847,
        0.0014605161,
        -0.030183451,
        -0.002529807,
        0.010497321,
        -0.007273606,
        0.002506505,
        -0.017176384,
        0.007435608,
        -0.005591899,
        -0.00035245038,
        0.02469355,
        0.0015891663,
        0.06145913,
        -0.00784548,
        0.0008675462,
        0.009724958,
        0.017657772,
        0.014384698,
        0.013638842,
        -0.01061435,
        0.0035352905,
        -0.0049215374,
        0.005813895,
        0.020522198,
        -0.0369671,
        0.01893312,
        0.0073716883,
        0.019346662,
        -0.010410105,
        0.0039238795,
        0.0054698726,
        -0.028476056,
        0.0005753007,
        0.029596439,
        0.013572869,
        -0.02110222,
        0.008235673,
        -0.17317535,
        0.008037306,
        0.0037055535,
        -0.008171569,
        -0.00035944802,
        -0.000511967,
        0.01837794,
        -0.020413568,
        0.027245801,
        0.005899518,
        0.0010929743,
        -0.0041051516,
        -0.0066857617,
        0.02073029,
        0.0045684613,
        -0.004862728,
        -0.013123219,
        0.011633636,
        -0.019984134,
        0.013040064,
        0.004467859,
        0.013813713,
        -0.027386937,
        0.020881295,
        -0.008136586,
        0.0073008724,
        0.04351637,
        -0.0072658933,
        0.011516639,
        -0.0025115497,
        -0.024652984,
        -0.002040068,
        -0.004040345,
        0.001699513,
        -0.0022490546,
        0.009137285,
        -0.0332814,
        -0.016404621,
        -0.01742404,
        0.006839997,
        0.008744906,
        0.0019609807,
        -0.0019694625,
        0.008583631,
        -0.0015312089,
        -0.028526107,
        -0.009949977,
        -0.025405103,
        -0.007963416,
        0.0014376737,
        -0.015524698,
        -0.021269457,
        -0.009242798,
        -0.0038370874,
        -0.0066769766,
        -0.0054390365,
        0.003536347,
        -0.0050340053,
        -0.017483858,
        -0.016324013,
        0.02414674,
        0.00448704,
        -0.18316098,
        -0.008237355,
        0.023314195,
        0.0029254528,
        -0.006643377,
        -0.011574942,
        0.016116668,
        0.019109502,
        0.024675734,
        0.005651792,
        -0.011676314,
        8.724985e-05,
        -0.030286143,
        0.011971877,
        0.024078118,
        0.000763992,
        -0.0021143302,
        -0.015527739,
        0.016012263,
        0.0007799503,
        0.019846456,
        -0.008190047,
        -0.00815213,
        -0.014185017,
        -0.011898084,
        -0.01512514,
        0.00050598424,
        0.019608647,
        0.010321948,
        -0.0103485035,
        -0.033763558,
        -0.0049039936,
        0.010937428,
        -0.0048295744,
        -0.024217425,
        0.008975283,
        0.0023028108,
        -0.0032131604,
        -0.024716966,
        0.03929217,
        -0.054562464,
        0.0066839517,
        0.015415115,
        0.01071761,
        -0.00041023918,
        0.0063584363,
        -0.012180184,
        -0.006048652,
        -0.00097582594,
        -0.012487517,
        -0.001064441,
        0.0019974548,
        -0.0048676557,
        0.007343217,
        -0.005298005,
        -0.017917892,
        0.009594607,
        0.010754127,
        -0.00405278,
        0.027491663,
        -0.023977403,
        -0.00558505,
        -0.008483727,
        -0.020001272,
        -0.004985754,
        -0.012945858,
        0.015578003,
        -0.020018939,
        0.0029325546,
        -0.010199429,
        -0.02008099,
        -0.0027067717,
        0.014525724,
        -0.011382689,
        -0.016408471,
        -0.0058896597,
        -0.017567992,
        -0.011461222,
        -0.029701859,
        -0.0031268473,
        0.012707571,
        -0.0074912463,
        0.007263435,
        0.0021742473,
        0.02081327,
        0.02222145,
        -0.029960483,
        -0.02865508,
        -0.020232333,
        0.0069588716,
        -0.021799263,
        0.008897868,
        -0.020081159,
        -0.013417083,
        0.012220999,
        -0.023188282,
        0.0024795067,
        0.0064906795,
        0.0032184226,
        0.008050014,
        -0.0036760108,
        0.002362902,
        0.021261057,
        0.0037458362,
        0.0148810875,
        0.033204135,
        0.0057693785,
        0.026055722,
        0.012723887,
        0.008521916,
        -0.005556095,
        0.009927785,
        -0.006574394,
        -0.023081902,
        0.008893257,
        0.0044786627,
        0.014255942,
        -0.0017514592,
        0.00984579,
        0.009789126,
        0.017025193,
        -0.00052831176,
        0.04270002,
        0.009512953,
        0.010601488,
        0.022736153,
        -0.006518482,
        0.006954612,
        0.0011343163,
        0.01198603,
        0.0182326,
        0.013677445,
        0.0045793955,
        0.0047926735,
        -0.018448882,
        -0.0011847791,
        0.008534836,
        -0.0021914123,
        -0.0027256992,
        0.014152382,
        -0.012194425,
        0.007847917,
        -0.026842544,
        -0.0010925925,
        0.0075163343,
        0.008322988,
        0.0013044273,
        0.0033778956,
        -0.019933807,
        -0.03268676,
        -0.007100378,
        -0.008281629,
        -0.002640785,
        -0.0010862007,
        0.0016977132,
        0.021558061,
        0.0139781255,
        -0.005023772,
        0.018027114,
        -0.017474,
        0.018889107,
        -0.08622977,
        0.041045435,
        -0.016617911,
        -0.035823904,
        0.0008180953,
        0.009182148,
        -0.0021255799,
        0.0025499477,
        0.001130344,
        0.027838968,
        -0.015321706,
        -0.012223227,
        0.011472896,
        -0.018694619,
        -0.007407008,
        0.015064497,
        0.0038031945,
        0.017645402,
        0.013996816,
        -0.0060204254,
        0.007158463,
        -0.002651429,
        0.0061784917,
        0.0052557946,
        0.03304751,
        0.015675731,
        0.019630348,
        0.03282226,
        -0.0042523956,
        -0.004947927,
        -0.0005940198,
        0.011564903,
        -0.005914323,
        -0.020605577,
        -0.005157696,
        0.0067386148,
        -0.014383001,
        -0.00067085226,
        0.0052031674,
        0.02083483,
        -0.011913437,
        -0.0355905,
        0.030198967,
        0.026971683,
        0.008579267,
        -0.011202404,
        -0.0033971737,
        0.019930253,
        0.0098785525,
        -0.0051990226,
        -0.015443618,
        0.027538754,
        0.000290271,
        0.00045751492,
        0.024281662,
        0.0100302575,
        0.035461895,
        -0.0045763464,
        0.013783613,
        0.016797142,
        0.019430952,
        -0.0018091105,
        -0.013418681,
        0.00026852364,
        -0.010459445,
        0.0075344946,
        0.00017013574,
        0.003593251,
        -0.014443481,
        0.0075047095,
        0.016089924,
        0.025884341,
        0.016762821,
        -0.026913675,
        0.0018562957,
        -0.0028729436,
        0.013314571,
        -0.0031831935,
        -0.01268345,
        0.042163026,
        0.006917803,
        0.0051743826,
        0.010329294,
        0.0018859275,
        -0.0039470484,
        -0.027379995,
        0.0037978003,
        -0.022912461,
        0.010845249,
        -0.006131997,
        0.0021663278,
        -0.011820556,
        -0.0075733233,
        0.024220867,
        -0.0069118906,
        0.0037963514,
        -0.0060305325,
        -0.018604506,
        0.009233717,
        0.009305308,
        -0.011821718,
        0.011433456,
        -0.007210454,
        0.0036663418,
        -0.0026822337,
        0.026692316,
        0.002429962,
        -0.00085783843,
        -0.025720859,
        0.005660244,
        0.019609354,
        0.008532585,
        -0.014752487,
        0.014377401,
        0.005772717,
        0.011189243,
        -0.011422996,
        -0.009440405,
        -0.010812597,
        -0.0057618134,
        -0.014930413,
        -0.008703633,
        -0.00037482296,
        0.0162486,
        0.021297093,
        0.0041150763,
        -0.01741862,
        0.0024560743,
        0.0031366,
        -0.034611456,
        -0.010650293,
        0.0058492296,
        0.004733521,
        -0.03695385,
        0.0019382339,
        0.00035773535,
        0.031001033,
        -0.004501961,
        -0.0040504965,
        0.014295786,
        -0.011707952,
        0.0036706566,
        -0.00090094475,
        -0.003408841,
        0.0122587355,
        0.0080755465,
        -0.037332457,
        0.020697765,
        -0.0018015364,
        -0.03895136,
        -0.015148471,
        -0.0052332347,
        0.0049080285,
        -0.014730385,
        0.0016458613,
        -0.00895598,
        -0.0054400023,
        -0.021336935,
        -0.008590978,
        -0.013344526,
        0.009358994,
        0.01393575,
        -0.020691546,
        0.0068539805,
        0.011055259,
        -0.007037775,
        0.008241259,
        0.0030014042,
        -0.0046026693,
        0.0031920867,
        0.005355748,
        -0.008099175,
        -0.015956026,
        0.00859263,
        -0.00017903898,
        -0.026748676,
        0.025719866,
        -0.007862149,
        -0.010105574,
        0.0099538565,
        0.0021784098,
        0.00806255,
        -0.008336399,
        -0.013838471,
        -0.0038820573,
        -0.003795127,
        0.022264093,
        0.0039392216,
        -0.0014464624,
        -0.017765388,
        0.0077371974,
        0.0033602344,
        0.001652901,
        0.012615633,
        -0.012050494,
        0.0043472713,
        0.001752198,
        0.008013316,
        0.008827423,
        0.009611974,
        -0.009129308,
        0.013043221,
        0.006812284,
        0.009645573,
        0.009165872,
        0.008428857,
        -0.02103984,
        -0.019444387,
        0.0056143603,
        0.02913219,
        -0.00852602,
        0.018552221,
        0.0033635448,
        -0.008694385,
        0.039127704,
        0.0005408209,
        -0.0090844715,
        0.02082774,
        -0.020338878,
        0.0054539773,
        -0.023838745,
        -0.012380784,
        -0.01219544,
        -0.0037124038,
        -0.012431108,
        -0.015759887,
        8.4895364e-05,
        -0.0074299118,
        -0.005398478,
        -0.0011283685,
        -0.015706256,
        -0.02284085,
        0.008094562,
        -0.010139075,
        -0.0028311312,
        -0.005966569,
        -0.008991335,
        0.021034613,
        -0.022989683,
        -0.014243519,
        -0.0062217074,
        -0.011098333,
        -0.027241392,
        0.013216955,
        0.014396153,
        -0.009285338,
        -0.022698013,
        0.00244811,
        -0.022372011,
        -0.006919527,
        0.008606024,
        0.001864969,
        0.02644552,
        0.020584209,
        -0.012662627,
        0.02088242,
        0.00041050636,
        -0.02813807,
        0.017230544,
        0.007030979,
        0.0021502865,
        -0.024803013,
        0.0066449107,
        0.015820604,
        0.0076079643,
        -0.013978475,
        -0.001595685,
        0.00024799386,
        0.020044263,
        -0.12298773,
        0.014647413,
        -0.010321559,
        -0.031226741,
        -0.0008424647,
        0.007986131,
        0.010962393,
        -0.02059283,
        0.018975433,
        -0.008608386,
        0.014013696,
        0.01565524,
        0.004751332,
        -0.006488212,
        -0.011931897,
        -0.014286985,
        -0.008983223,
        -0.020378115,
        0.011541901,
        -0.011181605,
        0.007045315,
        0.026045969,
        0.0035446528,
        0.012556179,
        -0.037310176,
        -0.017958524,
        0.003555172,
        0.009922466,
        -0.019237826,
        -0.0034258566,
        0.008641469,
        -0.0010233102,
        -0.0069402545,
        0.0027198177,
        0.0037394112,
        0.0053185276,
        0.0020388735,
        -0.020641562,
        0.006257237,
        0.0045475187,
        0.0017759975,
        0.006209207,
        0.00571604,
        -0.014349582,
        -0.01865364,
        -0.001360372,
        0.016477961,
        -0.009890935,
        -0.015791161,
        -0.006630972,
        -0.03251079,
        -0.013941565,
        -0.009767416,
        -0.025618728,
        -0.023833,
        -0.004068205,
        -0.027682107,
        -0.001100958,
        0.0064907814,
        0.00026951547,
        0.0013548288,
        -0.010896588,
        -0.000610481,
        0.016959902,
        -0.03336747,
        0.008550052,
        0.03323973,
        0.024080329,
        0.012353564,
        -0.0068299635,
        0.0035913303,
        -0.0076399012,
        0.012345092,
        0.004183575,
        -0.0028865943,
        0.016211923,
        -0.010279707,
        -0.005526574,
        0.014876019,
        0.008263928,
        -0.008755895,
        -0.0011569064,
        -0.077335924,
        -0.012385012,
        0.018845413,
        0.0065525593,
        -0.015904661,
        -0.0074216886,
        -0.033811126,
        0.004103403,
        -0.028301805,
        -0.013009253,
        0.012322234,
        -0.0024620343,
        0.0064032963,
        -0.021605128,
        -0.0073058787,
        0.009826671,
        -0.014147528,
        -0.025668887,
        -0.008127909,
        -0.0012872608,
        0.019354612,
        0.0018296066,
        0.035564914,
        0.00028668594,
        -0.014991795,
        0.038171444,
        -0.010312948,
        -0.020710507,
        -0.0037282882,
        0.0025134669,
        0.007857498,
        -0.15544124,
        0.003224448,
        -0.019556193,
        0.0045358306,
        0.010985017,
        0.0036423937,
        -0.015523067,
        -0.010104166,
        0.0009839748,
        0.0021838907,
        0.013481674,
        0.016571626,
        -0.0011333172,
        -0.010078535,
        -0.0023231471,
        0.14021367,
        -0.014705726,
        0.0092440685,
        -0.011417989,
        -0.017541816,
        -0.002388068,
        0.0019294331,
        0.00012264967,
        0.008524232,
        -0.028026382,
        -0.02832795,
        -0.010721615,
        0.011074105,
        -0.0016442508,
        -0.008866459,
        0.00062214996,
        0.008837506,
        0.037619196,
        0.0062095453,
        0.0056268335,
        -0.01717062,
        0.010341888,
        -0.0133220535,
        0.0009043288,
        0.013025527,
        -0.011317076,
        -0.015748251,
        0.0097490745,
        -0.015837654,
        -0.0011429404,
        0.027802361,
        -0.019668907,
        -0.0022826705,
        0.016980756,
        0.011788508,
        -0.030673172,
        -0.07124892,
        -0.011116766,
        0.0017992316,
        0.0012745305,
        -0.02051267,
        -0.015397197,
        -0.012407302,
        0.0026719493,
        0.0147582535,
        0.018616997,
        0.00082977826,
        -0.0017729003,
        0.001647309,
        -0.012358521,
        0.012321287,
        0.013611096,
        0.0005210356,
        -0.004089223,
        0.0010979897,
        -0.014014388,
        -0.004272489,
        0.010340937,
        -0.022930736,
        -0.0014777078,
        -0.012718509,
        -0.011353504,
        0.014071143,
        0.017945314,
        0.019470857,
        0.010252203,
        -0.006995446,
        0.006198677,
        -0.023677148,
        -0.0074906205,
        -0.007587099,
        -0.030974917,
        0.017741812,
        -0.021780558,
        0.020691888,
        0.012932506,
        -0.0016345736,
        0.008186019,
        0.03047373,
        -0.005836317,
        0.020062087,
        -0.009784574,
        -0.013863091,
        0.014648871,
        -0.007848458,
        0.011896975,
        -0.014817652,
        0.009139379,
        -0.01037055,
        -0.010952339,
        0.020069847,
        -0.01454753,
        0.020048467,
        -0.023887372,
        -0.011747154,
        -0.0044023525,
        0.014270029,
        0.0076943967,
        0.0059487224,
        0.016573526,
        0.0017642421,
        -0.010776238,
        -0.014271396,
        0.008380225,
        -0.0148043865,
        0.005847842,
        -0.011088433,
        -0.009880288,
        0.0087146275,
        0.0056823087,
        -0.00091503165,
        0.015681412,
        0.0054228147,
        -0.006682239,
        -0.013697192,
        0.008012047,
        0.0106976405,
        1.705615e-05,
        0.006864051,
        -0.015043567,
        0.0024164894,
        -0.019086849,
        -0.0013822085,
        -2.2125112e-05,
        -0.02138774,
        0.0115853,
        0.0018271635,
        0.0046301577,
        -0.0034063351,
        0.0019041407,
        0.0069657057,
        0.0069648284,
        0.009608097,
        0.0016365271,
        0.009492072,
        0.004358464,
        -0.016834578,
        -0.007337133,
        -0.005651036,
        -0.010141086,
        0.009498988,
        0.00537612,
        0.003613398,
        0.0011987347,
        -0.003042785,
        0.0071692714,
        -0.004098187,
        0.023081446,
        -0.0068120356,
        0.008319864,
        -0.0024109099,
        0.021416953,
        -0.0018965598,
        -9.612107e-05,
        -0.00047366656,
        0.01029536,
        0.015824575,
        -0.0090346355,
        -0.0006164498,
        -0.004329441,
        -0.0022619509,
        -0.008541687,
        -0.003399653,
        -0.005056172,
        -0.004890765,
        -0.0009153144,
        -0.008169207,
        -0.008526144,
        -0.007460949,
        0.01490997,
        0.000138015,
        0.007819542,
        0.0012656886,
        -0.019189298,
        0.0012539328,
        -0.0066270367,
        -0.01367113,
        0.018062232,
        -0.006906942,
        -3.5017576e-05,
        0.009466415,
        -0.0133973565,
        -0.014037659,
        0.014828297,
        0.0034632909,
        0.002134659,
        -0.00043795255,
        0.0042826612,
        0.0047101355,
        -0.008259345,
        0.014191507,
        0.0021966847,
        0.0073688584,
        0.00013986178,
        -0.0018225558,
        0.010838978,
        -0.00543202,
        -0.014205748,
        -0.012921538,
        0.0070263664,
        -0.00807929,
        -0.0023755725,
        -0.0085422285,
        -0.010424583,
        0.0029318887,
        -0.009911225,
        -0.010669627,
        0.007333948,
        -0.010259532,
        -0.0002079578,
        0.0024415117,
        -0.007113165,
        0.00012211973,
        0.0018095876,
        -0.0038490735,
        -0.0129240425,
        -0.0063526635,
        -0.0015752049,
        -0.00311841,
        -0.0056343595,
        -0.001167838,
        -0.0020837868,
        -0.008822965,
        0.003018353,
        -0.0007726731,
        -0.0028835933,
        6.3971114e-05,
        0.019585012,
        -0.0021562886,
        -0.0021990114,
        0.0076346844,
        0.0010717881,
        0.006669522,
        -0.008786273,
        0.0012990027,
        -0.009247644,
        0.014200476,
        0.019029072,
        0.0051453873,
        0.0033194711,
        -0.018025763,
        -0.007586606,
        -0.0034952217,
        -0.0014173083,
        -0.01803745,
        0.012940857,
        0.008156062,
        0.008430951,
        -0.00286573,
        0.01760489,
        -0.0003009926,
        -0.0003256271,
        -0.014816047,
        -0.0046131206,
        0.0034238042,
        -0.007706539,
        -0.014476229,
        0.0016799987,
        -0.00046877968,
        0.002531778,
        0.00031675698,
        -0.006883921,
        -0.0034107002,
        -0.0069611073,
        0.008343529,
        0.0009136672,
        0.0037486118,
        -0.00036945616,
        -0.004903517,
        -0.00048573661,
        0.013724291,
        -0.0047105923,
        0.012635581,
        0.010858722,
        -0.01572699,
        -0.004637406,
        0.013557228,
        -0.00600594,
        0.008102828,
        0.0009792249,
        -0.008152305,
        0.019852357,
        0.0025822488,
        -0.002632819,
        0.007048325,
        -0.0019640427,
        -0.006927033,
        -0.0026989277,
        -0.014506262,
        0.0009882192,
        -0.020405725,
        -0.008439244,
        -0.0026744339,
        -0.0051326347,
        -0.0042591547,
        0.11000041,
        0.0031364341,
        0.00371669,
        0.01453735,
        -0.0056554913,
        -0.002795184,
        0.0056000445,
        -0.022206921,
        -0.0019044505,
        0.0004625437,
        0.009227229,
        -0.004166526,
        -0.0014426947,
        0.007845903,
        0.0017196552,
        0.012610708,
        -0.007163035,
        0.00015752566,
        -0.0061175604,
        0.004999478,
        0.0025788127,
        0.02361775,
        0.015856924,
        -0.009332903,
        -0.0002801436,
        0.011408122,
        -0.0024045748,
        -0.0036747572,
        0.003317863,
        0.013002664,
        0.021415638,
        -0.011028239,
        -0.006833738,
        0.011597112,
        -0.00570739,
        -0.008240678,
        -0.011021972,
        -0.0075681745,
        0.004956899,
        0.008379924,
        -0.011578736,
        -0.0035606246,
        0.0028747877,
        -0.015128504,
        -0.00044867455,
        0.0049772435,
        0.00201141,
        0.0023437168,
        -0.0003546782,
        -0.00012824842,
        0.0024844897,
        0.002869819,
        -0.019141048,
        0.0013238646,
        0.00100999,
        0.00062593736,
        0.00059035484,
        -0.00993111,
        -0.007531768,
        -0.0071374215,
        -0.0037562815,
        0.014395252,
        0.0033554016,
        -0.0025208541,
        -0.00538176,
        -0.010312434,
        -0.0099624265,
        0.0073838425,
        -0.00838368,
        -0.016461669,
        0.008561942,
        0.006578163,
        0.00037261678,
        -0.004971301,
        0.046990834,
        0.0035153774,
        -0.005868051,
        0.011740638,
        0.0014742832,
        -0.0051845415,
        0.0023626438,
        0.011521811,
        -0.012777187,
        0.009929897,
        0.010511707,
        0.0016579919,
        -0.014421907,
        -0.0032215116,
        0.002450423,
        -0.0061924807,
        -0.0035696814,
        -0.015333882,
        0.009555178,
        0.0023164672,
        0.0071743224,
        0.009100146,
        0.1031708,
        -0.005681861,
        0.006518276,
        -0.0027669705,
        -0.0010554004,
        0.0040280167,
        0.012453239,
        -0.006916569,
        -0.00031966763,
        -0.0015657442,
        -0.00061986735,
        0.00029154622,
        0.021555422,
        -0.023597475,
        0.0024412079,
        -0.009984989,
        -0.00043484036,
        -0.0008974837,
        -0.004765369,
        -0.006768501,
        0.0123074185,
        -0.0020063946,
        -0.0012317458,
        0.017007403,
        0.011811892,
        0.004778383,
        -0.010231762,
        0.013123439,
        -0.014918695,
        -0.001474248,
        -0.0027616373,
        -0.009836596,
        0.0046527,
        0.0020832901,
        -0.0108831115,
        0.0100193685,
        0.016853908,
        -0.008128052,
        0.0029665763,
        -0.0032574888,
        -0.002970388,
        -0.00084476615,
        0.012665028,
        0.0049362443,
        -0.008043423,
        0.0041425186,
        0.014816481,
        0.005078052,
        -0.0041322247,
        -0.020839097,
        -0.010087862,
        0.000826495,
        0.021625029,
        -0.0026368278,
        0.010615021,
        0.0033941423,
        -0.0026446339,
        -0.0062471586,
        0.0032997585,
        0.011554271,
        0.003938789,
        0.007921897,
        0.008909097,
        0.011858385,
        -0.0030502905,
        -0.0021598686,
        0.011659833,
        0.0023633032,
        0.002462491,
        -0.0075533497,
        0.008326329,
        0.015354733,
        -0.0034168407,
        0.0047158687,
        0.012404579,
        -0.0006942268,
        0.0072720964,
        -0.0016762988,
        0.01336463,
        0.009427182,
        -0.012896873,
        -0.006632604,
        -0.008138914,
        0.010443935,
        -0.0060748784,
        0.009489437,
        0.004785562,
        0.0003980861,
        -0.0070004156,
        -0.01205872,
        0.01546232,
        0.003380696,
        0.009658454,
        0.0044184662,
        0.008157733,
        -0.0067725047,
        0.011058691,
        -0.013860416,
        -0.011483054,
        -0.0027354723,
        -0.008745191,
        0.01478055,
        -0.009093616,
        -0.0061603733,
        0.0034886505,
        0.0030504202,
        0.004371426,
        -0.0141265765,
        0.0029262605,
        0.015798379,
        -0.009220341,
        0.0025958263,
        0.0069200313,
        -0.017849455,
        -0.009994006,
        -0.014281553,
        0.01140679,
        0.02015693,
        0.0036264649,
        0.021187378,
        0.009788202,
        0.0021029927,
        0.008331479,
        -0.009179509,
        -0.01030225,
        -0.0085350005,
        0.0029275042,
        -0.0005694082,
        0.011373082,
        0.0070921183,
        0.00059206353,
        -0.012008887,
        0.0044888817,
        -0.000838387,
        0.013149757,
        -0.0052649914,
        -0.0074650273,
        0.005033732,
        -0.029026935,
        0.0010679711,
        -0.0012786549,
        -0.006730567,
        -0.012763672,
        0.0015261761,
        -0.008844623,
        0.011456741,
        -0.0034170595,
        -0.0024967615,
        0.017052066,
        -0.009041082,
        -0.01225775,
        0.0012188238,
        -0.013667356,
        -0.001992209,
        -0.020342916,
        -0.00092363753,
        0.008935397,
        0.0014739815,
        0.006470183,
        0.022331899,
        0.0066910004,
        -0.071758114,
        -0.0015554058,
        0.008288172,
        -0.008487337,
        -0.006916033,
        -0.0011449042,
        -0.0035105038,
        -0.01218617,
        0.0052145813,
        -0.0010718012,
        0.006436684,
        -0.004212165,
        0.0031883016,
        -0.00078960735,
        0.00041933596,
        0.0078137275,
        -0.0038021063,
        0.023617687,
        -0.00068003306,
        -0.021584773,
        -0.0021510979,
        0.011933956,
        -0.008391218,
        0.008579635,
        -0.010341176,
        0.0019555103,
        0.0024139804,
        0.007453072,
        -0.021571083,
        0.011754809,
        -0.0009360616,
        0.003192982,
        0.01858271,
        0.009447346,
        -0.001413647,
        -0.015485421,
        -0.008725207,
        0.0015244529,
        -0.0003074771,
        -0.009207599,
        0.008229954,
        -0.017108351,
        -0.0052255266,
        -0.0068675345,
        0.011019987,
        -0.010315609,
        0.020896215,
        -0.0029910114,
        0.01478223,
        -0.006273839,
        -0.0087764235,
        0.0121673,
        -0.0035684826,
        0.006315524,
        0.0093105845,
        -0.013821425,
        0.009431879,
        -0.013262614,
        -0.022508211,
        0.0148111135,
        0.004916652,
        0.015018823,
        0.0032468305,
        -0.0025593624,
        -0.005634379,
        -0.013474661,
        0.004006388,
        0.00023104351,
        -0.0119477175,
        -0.0033434946,
        0.013847329,
        -0.0034121776,
        -0.012974549,
        -0.0006082984,
        -0.00095003325,
        0.00072488666,
        0.002715974,
        -0.009059329,
        -0.005756802,
        0.012757525,
        0.0035633661,
        -0.0030739254,
        -0.0057310886,
        -0.01461739,
        0.007915953,
        0.010558941,
        -0.01678233,
        -0.0005133999,
        -0.011145176,
        0.013072379,
        -0.013090627,
        0.008385944,
        -0.00528091,
        -0.0060826614,
        0.003696126,
        0.00791612,
        -0.008320685,
        -0.00652334,
        0.014276353,
        0.00910401,
        0.0025084694,
        -0.009071483,
        -0.0030053696,
        0.0018677071,
        -0.01361604,
        0.0016507888,
        -0.0016849643,
        -0.00898616,
        0.006143557,
        0.0048374054,
        0.008592062,
        -0.0037504747,
        -0.01934766,
        -0.00063896703,
        -0.007077836,
        -0.003388441,
        0.0005927586,
        -0.0061387685,
        -0.0035650304,
        -0.015120814,
        -0.0010443502,
        0.012932041,
        -0.0196129,
        -0.015814217,
        0.007677028,
        -0.015855115,
        0.009998279,
        0.008501988,
        0.01775218,
        -0.005315056,
        0.014130181,
        0.00429974,
        -0.003073817,
        -0.00014565865,
        -0.018753095,
        0.0056180265,
        0.010699456,
        0.019711496,
        -0.010952801,
        -0.005693158,
        0.01597117,
        -0.009635471,
        0.00476231,
        0.00038250824,
        -0.004945918,
        -0.0049709966,
        0.0042080525,
        0.009826005,
        0.00013282572,
        -0.0036589715,
        0.0015653801,
        0.005073708,
        0.0033281557,
        -0.00455615,
        -1.6880691e-05,
        0.011392291,
        0.00038578,
        -0.011555904,
        -0.0076085585,
        -0.016399514,
        0.022403631,
        0.01088773,
        0.010194791,
        -0.012054318,
        0.008663091,
        -0.0005660857,
        -0.010876136,
        -0.004075997,
        -0.016716275,
        -0.009679197,
        -0.00820213,
        0.0017822455,
        -0.0019364671,
        -0.012386811,
        -0.00073502626,
        0.0010621063,
        0.022128003,
        -0.011914653,
        0.0012235959,
        -0.009035757,
        0.009031152,
        0.00094188465,
        -0.0005697308,
        0.008637008,
        0.0010158841,
        -0.010943379,
        -0.009478906,
        0.0023858978,
        0.0051952554,
        0.008324894,
        0.0017890173,
        0.002843172,
        0.019129742,
        -0.0009956222,
        0.0035978593,
        0.0046546627,
        0.015331522,
        0.0052044764,
        0.003118065,
        -0.0010936115,
        0.0001574379,
        -7.791014e-05,
        0.006880338,
        0.0005572511,
        -0.00072693214,
        -0.12348014,
        -0.0024685457,
        -0.002143782,
        -0.019195352,
        -0.015447515,
        0.0013341713,
        0.0248687,
        -0.0045674564,
        0.011280912,
        0.001953189,
        -0.022138868,
        -0.0005684132,
        0.0061180205,
        -0.026363196,
        -0.0028517395,
        0.0020415753,
        0.0045914543,
        -0.011305915,
        0.0007974374,
        -0.011687381,
        -0.0013573652,
        0.012720851,
        -0.007349744,
        0.0025499696,
        -0.010603354,
        -0.005080745,
        -0.035497013,
        -0.004631629,
        0.0056539923,
        -0.0050730044,
        -0.012075039,
        0.0037623881,
        0.00856035,
        0.0033012687,
        0.018777244,
        -0.0057458384,
        0.008991086,
        0.011153184,
        -0.15174012,
        -0.0025728631,
        -0.0049666176,
        0.009712883,
        -0.0051069576,
        -0.0042373086,
        0.008561996,
        0.006750548,
        0.015661927,
        0.0007838598,
        -0.020002993,
        0.0061675515,
        -0.0018083191,
        -0.008010028,
        0.009705556,
        0.005476673,
        -0.014016867,
        0.018817604,
        -0.0031663927,
        0.016418401,
        -0.004530365,
        -0.006786196,
        0.00087474275,
        0.017385963,
        -0.005677774,
        0.009066783,
        -0.0073299026,
        -0.0023865914,
        0.0027521013,
        -0.0064942217,
        0.0064645503,
        -0.011646096,
        0.0031078449,
        -0.02090325,
        0.0038339486,
        0.003948665,
        -0.002940522,
        0.007150754,
        0.01636715,
        -0.01798095,
        0.0024824545,
        -0.013104298,
        0.0030241844,
        0.006070181,
        0.00067782047,
        0.0027053552,
        0.012341219,
        0.0130011765,
        -0.009307426,
        0.0028885198,
        0.0060320017,
        0.003857125,
        -0.008228446,
        0.003044311,
        0.00028174362,
        0.002222036,
        0.0051135006,
        0.007995438,
        0.009281364,
        -0.0018849641,
        -0.00018194242,
        -0.0012653124,
        0.008576683,
        -0.020058658,
        -0.008738564,
        0.0005644843,
        -0.003641851,
        0.028096247,
        0.0104618585,
        -0.01428824,
        -0.008295421,
        0.016932938,
        -0.0033830917,
        -0.0024687285,
        0.011993469,
        -0.022216862,
        0.0030990113,
        -0.0021081383,
        0.01133423,
        -0.015896806,
        -0.009283554,
        0.011888254,
        -0.0077181156,
        0.01960289,
        -0.0009877504,
        -0.009712713,
        -0.0032195153,
        -0.007275634,
        -0.012080039,
        -0.054076474,
        -0.00095494057,
        -0.0036945813,
        0.0014713997,
        0.017020168,
        0.0121679325,
        0.00024406536,
        -0.0047232606,
        0.002916523,
        -0.008590669,
        -0.0032339771,
        0.016845884,
        0.01248082,
        -0.010496404,
        -0.0015060607,
        0.00021019111,
        -0.00064480276,
        -0.0012781358,
        -0.010530149,
        0.0013273585,
        -0.02375109,
        -0.005794997,
        0.003978317,
        0.020645931,
        0.0216738,
        -0.018877609,
        0.0057391995,
        -0.010420876,
        0.0014309208,
        0.018243408,
        -0.023464108,
        0.0050993203,
        0.0012271163,
        -0.003905081,
        -0.014603352,
        0.0029291215,
        0.0055948733,
        0.02047112,
        -0.014324097,
        -0.0137420725,
        -0.0010297503,
        0.01467038,
        0.0007882047,
        0.00022730505,
        0.0039997734,
        -0.0053155757,
        -0.025956,
        -0.004225816,
        0.008795415,
        -0.008972564,
        -0.014820605,
        0.0017780041,
        0.0038597235,
        -0.008313773,
        0.00015561635,
        0.022829937,
        -0.01709577,
        -0.0015163368,
        0.016532933,
        -0.0045767464,
        0.019922033,
        -0.015537543,
        0.008105301,
        0.002657687,
        0.013137526,
        -0.0054702256,
        0.005039715,
        -0.008162474,
        -0.014403966,
        0.0059263557,
        0.0067762593,
        -0.0007800722,
        0.009271395,
        -0.0069651227,
        -0.016922368,
        -0.0033291532,
        0.019880183,
        0.004982557,
        0.0057378514,
        -0.011685658,
        0.0020317931,
        0.0016682012,
        -0.00015647049,
        0.01790537,
        0.017776696,
        -0.0090276515,
        0.009847313,
        -0.0036610032,
        -0.014017655,
        0.018122898,
        0.007080164,
        -0.027429646,
        -0.0012816581,
        0.006837618,
        0.014850691,
        0.018041825,
        -0.0017537503,
        0.009215416,
        0.0070009953,
        -0.006828712,
        0.021556126,
        0.008347607,
        0.0047403355,
        0.00022923577,
        -0.003120093,
        -0.0113907065,
        0.00032455302,
        -0.0020307084,
        0.018247081,
        0.004503806,
        0.0029526462,
        0.008701914,
        0.010181492,
        -0.018360514,
        -0.1477751,
        -0.003978854,
        0.004910437,
        -0.0120393075,
        -0.01834389,
        -0.017955257,
        0.012115859,
        -0.0076586013,
        0.0037336363,
        0.0078576915,
        -0.013570691,
        0.008771794,
        -0.007722929,
        -0.029475793,
        0.023312626,
        0.0010538329,
        0.009791543,
        0.006240016,
        -0.009451577,
        0.0019973018,
        -0.0027166689,
        -0.010624118,
        -0.0072741555,
        -0.0012829845,
        -0.00019572955,
        -8.475988e-05,
        0.021446308,
        -0.007928128,
        -0.0130392015,
        -0.0111713195,
        -0.0060526594,
        0.006025493,
        -0.0032666621,
        0.008860275,
        -0.008678875,
        -0.0046799444,
        -0.0102069015,
        0.0029125256,
        -0.010447743,
        -0.0070869694,
        -0.0025464061,
        -0.004886638,
        -0.01316413,
        -0.011694316,
        -0.0055920947,
        0.00030738706,
        0.0027963496,
        -0.02781882,
        -0.010303417,
        -0.00070489757,
        0.0074579436,
        -0.011518501,
        -0.017729266,
        -0.0011976238,
        0.0038284091,
        -0.0020675743,
        0.008149268,
        0.012203294,
        -0.0039899205,
        0.0064829285,
        0.0031709813,
        -0.025732176,
        -0.003343848,
        0.007275122,
        0.015873194,
        -0.009388359,
        -0.009180598,
        0.18771471,
        0.002347073,
        0.038423583,
        0.007373058,
        0.011692702,
        0.009878146,
        0.004264046,
        0.028733669,
        0.00379558,
        -0.00014346605,
        0.00030623053,
        0.0076915,
        -0.015080378,
        0.0015969765,
        -0.0070682163,
        -0.01044287,
        0.0008719328,
        -0.011042623,
        0.0073423553,
        0.0035136647,
        0.009977703,
        0.012541453,
        0.022904031,
        0.00470995,
        -0.00046797076,
        -0.011279238,
        -0.021495115,
        -0.003180082,
        -0.006436539,
        0.034761455,
        0.020711817,
        -0.0047347606,
        -0.022367893,
        0.003977426,
        0.0055155456,
        0.0032965764,
        0.014082784,
        -0.015310701,
        -0.02129548,
        0.0037749426,
        -0.013608349,
        0.011662107,
        0.0047643892,
        -0.016606491,
        -0.0037439226,
        -0.007564824,
        -0.008573241,
        0.015771748,
        0.021966806,
        0.0030025253,
        -0.0059447936,
        -0.004626936,
        -0.001451749,
        0.009328392,
        0.00633803,
        -0.0025693434,
        0.012827562,
        -0.01789501,
        0.0031570392,
        0.01934656,
        -0.01799693,
        -0.003880477,
        -0.0011861724,
        0.028567025,
        0.0032850755,
        0.006121809,
        0.016015759,
        0.004160715,
        -0.012050362,
        -0.16262512,
        0.013410324,
        -0.0066263825,
        0.0076612043,
        -0.007831085,
        -0.0031365042,
        0.01243253,
        0.027935896,
        -0.008120309,
        0.0069079786,
        -0.012114128,
        -0.027690355,
        1.06095185e-05,
        0.019809349,
        0.011260049,
        0.007184887,
        0.0062985653,
        0.0028459958,
        0.01753447,
        -0.0005843817,
        -0.013388108,
        -0.00726364,
        0.01469229,
        -0.02192327,
        -0.016106106,
        0.01523777,
        -0.011982256,
        -0.021437842,
        0.021001874,
        0.008999806,
        -0.021359308,
        -0.0026240381,
        0.006063153,
        0.014351169,
        -0.019496549,
        0.007965831,
        -0.030726563,
        0.01656153,
        0.014448817,
        -0.028134704,
        0.012484216,
        -0.0034924564,
        0.011616968,
        0.018752947,
        -0.003721143,
        -0.0017004228,
        -0.015057523,
        -0.000394445,
        -0.019882662,
        0.014785998,
        0.015649937,
        0.0044700876,
        -0.005205846,
        -0.011614091,
        -0.024194278,
        0.008663585,
        0.015753148,
        -0.017172527,
        0.0112209525,
        -0.011071128,
        0.02100212,
        0.025011072,
        0.0073575354,
        -0.009532989,
        -0.002812963,
        -0.001810322,
        -0.012599632,
        -0.0023881888,
        0.018772742,
        0.015811358,
        -0.005174474,
        0.010323917,
        0.0032156669,
        0.011134874,
        0.0138163,
        0.014423369,
        0.0009003183,
        0.0069645382,
        0.01306179,
        0.01828132,
        0.008057464,
        -0.021251509,
        0.0051709525,
        0.0054903547,
        0.04079012,
        -0.010502475,
        -0.01125032,
        0.0042363666,
        0.0145272715,
        0.0006696825,
        0.005037286,
        0.0012354003,
        2.612004e-05,
        0.014651734,
        -0.023566017,
        -0.012699132,
        -0.012446161,
        0.0043914323,
        0.0019952776,
        -0.029093381,
        -0.020142216,
        0.00032744816,
        0.029881878,
        -0.007619494,
        0.005992523,
        -0.018691132,
        0.0008612242,
        -0.01795926,
        0.009459564,
        -0.00044184792,
        -0.0050968095,
        -0.0030759831,
        0.00758132,
        -0.008026408,
        0.003924439,
        0.0022794546,
        0.0005010153,
        -0.007468848,
        -0.0034514025,
        -0.017747235,
        -0.01266951,
        0.0036508348,
        0.0049307654,
        0.0038066746,
        0.006473405,
        -0.0033980103,
        0.004437356,
        0.006291692,
        0.013997338,
        0.020535916,
        -0.026762946,
        0.002714232,
        0.00023763129,
        0.0068788123,
        0.009123729,
        -0.008156189,
        0.0076598213,
        -0.004480322,
        0.018183935,
        -0.00010413545,
        -0.0053357496,
        -0.005743602,
        -0.0027117797,
        0.004815672,
        -0.009455843,
        0.0057177017,
        0.005627126,
        0.012796107,
        -0.0014839311,
        0.0046071494,
        0.0054494143,
        -0.0052968264,
        0.00079480215,
        -0.004616654,
        0.013530739,
        0.0076523605,
        0.0049739257,
        -0.0025273978,
        -0.011813107,
        -0.017805932,
        -0.009866168,
        0.008193736,
        0.00061497244,
        -0.0049785767,
        0.012550125,
        -0.0050196378,
        0.0024935952,
        -0.0016652121,
        -0.014835478,
        -0.009369841,
        0.008822038,
        -0.013541143,
        0.020043358,
        -0.0025945837,
        -0.015739458,
        -0.008022894,
        -0.005373791,
        0.0043094005,
        0.012581826,
        -0.0955788,
        -0.009859806,
        -0.0043868935,
        0.0016330107,
        0.0010945821,
        -0.014436653,
        -0.019680182,
        0.014154288,
        0.0005269425,
        -0.017046148,
        -0.003754175,
        -0.019241124,
        -0.014094449,
        -0.010545038,
        -0.006417341,
        -0.0076925,
        0.007958968,
        0.01384701,
        -0.01610224,
        -0.020588879,
        0.0053786663,
        0.00927045,
        0.009743834,
        0.008629328,
        -0.008415531,
        -0.009013493,
        0.0059529613,
        -0.0107998215,
        0.0065614055,
        0.0005624235,
        -0.0018008826,
        -0.0048751463,
        0.015778607,
        0.0112477,
        0.0055254577,
        0.011961657,
        -0.0069170846,
        -0.014792065,
        0.020425322,
        -0.047383577,
        -0.027820555,
        -0.022497335,
        -0.119764365,
        -0.0060382313,
        -0.012518634,
        0.012229076,
        0.0032521156,
        -0.0043377792,
        0.001513411,
        0.010529774,
        -0.002960018,
        0.0007730904,
        -0.009869015,
        0.0054291766,
        -0.0077722818,
        3.9308893e-06,
        0.00012243314,
        0.009811665,
        -0.018584318,
        -0.0014563682,
        -0.028152797,
        -0.010079946,
        -0.017706059,
        -0.015496338,
        -0.006187134,
        0.00476442,
        -0.005214056,
        0.011611723,
        -0.0018640568,
        0.01451426,
        0.0018615441,
        0.0058894865,
        -0.01884131,
        0.0049720327,
        0.008285487,
        -0.003125346,
        -0.0053052213,
        0.004773304,
        0.0015289735,
        0.0028040428,
        0.01429797,
        -0.0050560436,
        0.0001410097,
        0.028622136,
        0.0005359378,
        -0.025068209,
        0.0095838895,
        -0.1192798,
        -0.0154622365,
        0.017073158,
        -0.00681849,
        0.0011743449,
        -0.0008161396,
        0.002430686,
        0.11366334,
        0.0016750921,
        0.008623531,
        0.007475305,
        -0.011529648,
        -0.0028474403,
        -0.0009955869,
        -0.005299318,
        0.02403933,
        -0.00062240916,
        0.020366618,
        0.019572297,
        0.0036188222,
        0.01786254,
        -0.0030677672,
        -0.014071498,
        -0.022529397,
        -0.002474222,
        -0.06859713,
        -0.0017768624,
        -0.00703875,
        -0.0026124087,
        0.00016349234,
        -0.001645209,
        -0.008352772,
        -0.017591527,
        -0.01263653,
        0.0029530777,
        0.001420487,
        0.0139812855,
        0.012372416,
        -0.0065927845,
        -0.00024990467,
        0.012145157,
        -0.010543989,
        -0.0058393963,
        -0.007348851,
        0.0022004095,
        0.008071054,
        -0.011132786,
        0.010462519,
        0.021750338,
        0.0014196332,
        -0.0019430425,
        -0.01780508,
        -0.0035751192,
        -0.0029908232,
        0.0021202315,
        0.013519433,
        0.0020389366,
        0.003117801,
        0.009569038,
        0.008687199,
        -0.029741235,
        0.00048142902,
        0.0059171193,
        0.009514526,
        0.0074748774,
        0.016488586,
        -0.0028396905,
        -0.004845964,
        -0.03909006,
        0.018714558,
        0.0072413785,
        0.0047346605,
        -0.0012910372,
        -0.0054494864,
        0.008070474,
        0.019054743,
        0.01013716,
        0.0029326498,
        -0.008107466,
        0.010991718,
        -0.026119616,
        0.015574175,
        -0.011373061,
        -0.0128880795,
        0.01143684,
        0.0062588155,
        -0.002670954,
        0.018154055,
        0.019872785,
        0.0057573076,
        0.0072128847,
        -0.011087315,
        -0.0053853584,
        0.019660622,
        -0.008615579,
        0.005046483,
        0.0072106486,
        0.0045558787,
        -0.005432372,
        -0.00252713,
        0.0071706986,
        -0.012955356,
        0.0037137277,
        -0.0071349344,
        0.017953595,
        0.0016401378,
        -0.0029942037,
        0.0078067933,
        0.0023902727,
        0.004138357,
        0.011194709,
        -0.023507155,
        0.0017547932,
        -0.0012434571,
        0.0079756295,
        -0.011870143,
        0.014661981,
        0.000117040116,
        -0.026647411,
        0.001951752,
        -0.011248701,
        -0.01384909,
        0.0144509645,
        -0.013369366,
        0.003919549,
        0.016558316,
        -0.024671454,
        0.0017617168,
        0.00081269705,
        0.012800179,
        -0.02362394,
        0.019894121,
        0.011168967,
        -0.017163968,
        -0.0048363027,
        -0.006487852,
        -0.027386226,
        0.011207347,
        0.0015094043,
        -0.011459642,
        -0.0025774539,
        -0.004399763,
        -0.010392318,
        -0.015727421,
        0.0117846085,
        -0.005919539,
        -0.011917764,
        0.006145129,
        0.0071861106,
        -0.0011281809,
        0.013664591,
        0.011586697,
        -0.001862386,
        -0.0034673961,
        -0.0049570673,
        -0.008864905,
        0.0052316575,
        -0.013872881,
        0.0018860338,
        -0.0045763124,
        -0.0072459183,
        -0.011428435,
        0.01149448,
        -0.003857876,
        -0.009533871,
        0.018239893,
        0.023597552,
        0.009908653,
        0.023978833,
        -0.014262604,
        0.007850328,
        -0.00038809748,
        0.0010404447,
        0.005900106,
        0.0065192967,
        -0.00064622046,
        0.013964386,
        0.0065402617,
        0.009386526,
        -3.28472e-05,
        -0.0048107705,
        -0.005728606,
        -0.016575227,
        0.0110038165,
        -0.013398007,
        -0.020082852,
        -0.004606713,
        -0.011722172,
        0.010823044,
        0.01906743,
        0.030803423,
        -0.022067133,
        0.01515698,
        0.008819531,
        -0.0033403283,
        0.009054254,
        0.0075468593,
        0.0045971503,
        0.0039537917,
        0.031545386,
        -0.012622412,
        -0.012144629,
        -0.009231268,
        0.0139666395,
        -0.001130718,
        0.013423151,
        -0.026219636,
        0.0033782346,
        0.018094195,
        0.014517427,
        0.015100677,
        -0.014643574,
        -0.0011882641,
        0.009110513,
        -0.012919775,
        -0.007919689,
        0.013788498,
        0.006483987,
        0.011346646,
        0.0050730743,
        0.0116452,
        0.007265742,
        0.010892953,
        -0.011090601,
        0.001816721,
        -0.00047848822,
        -0.0058572446,
        -0.0070967134,
        0.005820588,
        0.009654125,
        0.002716971,
        0.005563856,
        -0.003944228,
        0.0068586203,
        -0.0015735934,
        -0.0038011312,
        -0.011706368,
        -0.00056319294,
        0.017925167,
        -0.0060877847,
        -0.0068418593,
        0.00258238,
        -0.0026426462,
        0.0060820226,
        0.0025026738,
        -0.013989669,
        -0.01068764,
        -0.015893161,
        -0.00917523,
        0.031189134,
        0.01219282,
        0.0069461428,
        -0.0058103804,
        0.014831203,
        -0.0068200724,
        -0.0021882735,
        -0.005692482,
        0.0020306627,
        -0.0037125058,
        -0.0090902345,
        0.0054248534,
        0.009879066,
        -0.015855588,
        0.021907628,
        -0.020550633,
        -0.01780533,
        0.012621567,
        0.00061854813,
        -0.003734774,
        0.003022464,
        0.010841192,
        -0.012026434,
        -0.020713272,
        -0.0061513917,
        0.010253119,
        -0.0019708858,
        -0.013218029,
        -0.0075638266,
        -0.0011724856,
        0.008968582,
        -0.00383918,
        -0.0054759216,
        0.0012237886,
        -0.006395788,
        0.017362528,
        -0.00489977,
        -0.016092047,
        0.019529987,
        -0.013075128,
        -0.0049103755,
        0.01164179,
        -0.0063067754,
        -0.015308906,
        0.005881263,
        -0.0069448645,
        0.024798524,
        -0.0043009478,
        0.008990652,
        0.009466874,
        0.007989362,
        -0.005065263,
        0.0006006848,
        -0.0004486381,
        0.0204495,
        -0.011115504,
        -0.0063787308,
        0.001778985,
        -0.028822538,
        -0.023541607,
        -0.009836963,
        -0.0035362418,
        -0.0015177411,
        -0.008343587,
        0.013548282,
        0.007089847,
        -0.016531797,
        0.008750143,
        0.0004759558,
        0.01587933,
        -0.0075211166,
        -0.016702263,
        0.0012642496,
        0.019064762,
        -0.011169065,
        0.013016206,
        0.009683916,
        -0.009069834,
        0.0018161649,
        -0.010990568,
        0.006462736,
        0.01215626,
        -0.0038017728,
        -0.0064848275,
        -0.011066064,
        0.011485414,
        0.014686354,
        -0.0051947897,
        0.018401355,
        0.0031044667,
        0.0096384,
        -0.01037895,
        -0.014308272,
        0.021320041,
        -0.016200887,
        0.007741003,
        -0.005728812,
        0.011368738,
        0.0016971756,
        -0.013851594,
        -0.0090605775,
        0.006572874,
        -0.0040097944,
        -0.002056941,
        0.00890547,
        -0.013774845,
        0.00851115,
        -0.0011922794,
        -0.0010533684,
        0.01520855,
        0.0005503493,
        -0.0034908631,
        0.01143906,
        0.012916783,
        0.011549723,
        0.00051295006,
        -2.360495e-05,
        -0.0010922084,
        -0.014505256,
        0.008501533,
        -0.00948401,
        -0.009872903,
        0.001394132,
        -0.002115919,
        0.004146402,
        -0.016679388,
        0.018977234,
        -0.007016453,
        -0.009200843,
        0.013891094,
        -0.004393921,
        -0.01572449,
        0.032595687,
        -0.0017079662,
        0.0025926523,
        -0.0024163574,
        0.0027557134,
        0.021296835,
        0.011950653,
        0.001600997,
        -0.016384076,
        0.0004644135,
        0.016415313,
        0.007267314,
        -0.016230926,
        0.0046315384,
        0.0024266164,
        0.009477957,
        -0.0016034002,
        -0.011644739,
        -0.008459037,
        0.008514674,
        -0.025454633,
        -0.0059956433,
        -0.0038970555,
        0.008283206,
        0.010471649,
        0.005681842,
        -0.015282064,
        0.014799945,
        -0.004026701,
        0.0019000889,
        0.00014468393,
        -0.011235222,
        0.012505571,
        -0.020850843,
        0.035718787,
        -0.0056628003,
        0.008837066,
        -0.0019798675,
        -0.013639875,
        -0.006038332,
        0.013534108,
        0.002117038,
        0.0022882598,
        0.0099798925,
        -0.0029011464,
        0.011769853,
        0.009018895,
        0.012765677,
        0.011156674,
        0.023360113,
        -0.012316278,
        -0.0148098795,
        -0.0079531,
        -0.0068590846,
        0.004938255,
        0.007643793,
        0.002021952,
        -0.015162341,
        -0.0049820715,
        -0.025287831,
        -0.0016047843,
        -0.0025822732,
        0.005494958,
        -0.0067297122,
        0.008621551,
        -0.0049379095,
        0.004793097,
        0.00623571,
        0.010511415,
        -0.022140745,
        -0.033933852,
        -0.012616108,
        -0.012635222,
        -0.03500243,
        -0.0038412758,
        -0.0011643438,
        0.0017507663,
        0.018561674,
        -0.017414855,
        0.003212941,
        -0.0066463263,
        0.0016619171,
        0.013313412,
        -0.004787646,
        0.010247968,
        0.0038949337,
        0.0043629855,
        0.015057483,
        0.006337003,
        -0.03129895,
        -0.003610984,
        -0.05091284,
        -0.008748238,
        -0.020337598,
        -0.0058208527,
        -0.006306664,
        -0.009481598,
        -0.01237445,
        -0.030917004,
        0.017213948,
        0.014027728,
        0.00276433,
        0.010849688,
        -0.00014643828,
        -0.011641871,
        -0.005853825,
        -0.012422254,
        -0.00041694724,
        -0.010196505,
        0.0049161552,
        0.010091995,
        0.0083131045,
        -0.012611061,
        0.0039486256,
        0.008340338,
        -0.01462681,
        -0.011746316,
        0.021497251,
        0.011760014,
        0.006323356,
        -0.005492085,
        -0.00039061217,
        -0.015801447,
        0.003933467,
        0.017419402,
        0.00559088,
        -0.01156597,
        -0.005425932,
        -0.024710292,
        -0.026448658,
        0.01235047,
        0.008495913,
        0.0018894249,
        0.020947756,
        -0.008738938,
        0.017603373,
        -0.009484298,
        -0.006546355,
        -0.007895934,
        -0.015692221,
        0.011744957,
        -0.013047785,
        0.008591408,
        0.017671343,
        -0.01612309,
        0.005070555,
        0.01271509,
        0.0066319127,
        0.0021877538,
        0.02237668,
        -0.014917902,
        -0.005888372,
        0.014163126,
        -0.0027338392,
        -0.0035396048,
        0.0039777616,
        -0.021462891,
        -0.008474817,
        -0.016143175,
        -0.005487694,
        -0.002156873,
        0.015833363,
        -0.014209996,
        0.03476469,
        0.0075478093,
        -0.011067309,
        -0.0025916365,
        -0.017585896,
        -0.016722882,
        -0.00011443727,
        0.0006079806,
        -0.010900707,
        0.0030540212,
        -0.0003630239,
        0.0090170875,
        0.0028732684,
        0.023109363,
        0.013000892,
        -0.016357094,
        -0.0011574422,
        0.002877019,
        -0.009705814,
        0.005942579,
        -0.025810651,
        -0.012934445,
        -0.0029816069,
        -0.0048503894,
        0.0051134406,
        0.018883381,
        -0.017839637,
        0.011799917,
        0.003930317,
        0.009021935,
        0.018245036,
        0.0004652134,
        0.009875643,
        0.0033323788,
        -0.007612334,
        -0.01287953,
        -0.008919745,
        0.006381013,
        0.01170492,
        0.013548471,
        0.02026351,
        -0.01425867,
        -0.011965883,
        0.011863469,
        -0.0009797164,
        0.014121634,
        0.00017505966,
        -0.011531477,
        -0.010522303,
        -0.0022260677,
        -0.011205372,
        -0.0098744,
        -0.017670864,
        -0.011268561,
        0.016594887,
        0.0010413781,
        0.0014154916,
        -0.001409015,
        0.0001288384,
        0.007103857,
        0.013181485,
        0.0070086215,
        0.0007470535,
        0.018723559,
        0.003682997,
        -0.0037363032,
        -0.030297097,
        0.02998081,
        -0.0072047072,
        0.0174811,
        0.0032116035,
        0.00034680922,
        -0.013183717,
        -0.00069995003,
        0.00493259,
        -0.015783817,
        -0.012316878,
        -0.010290984,
        0.004021987,
        -0.0043793847,
        0.007930736,
        -0.016577201,
        0.02540876,
        0.018979328,
        -0.01062981,
        0.005017148,
        -0.007827406,
        0.0012720597,
        -0.014342112,
        0.017936945,
        -0.010703676,
        0.023660695,
        0.0016899182,
        0.01536246,
        0.00077477045,
        0.0077168755,
        0.02211662,
        0.013276051,
        0.0033497193,
        0.0037130953,
        -0.007982334,
        0.0070138625,
        0.006286951,
        0.013987261,
        -0.010763328,
        0.0022472513,
        0.007158072,
        0.011146028,
        -0.0013461465,
        -0.011289475,
        0.012425557,
        0.0025830178,
        0.0013253683,
        0.014246785,
        0.010240196,
        -0.0041106115,
        -0.017312504,
        0.0029663697,
        -0.021267964,
        0.004826452,
        0.008854752,
        0.0015314526,
        0.0030575036,
        0.013031114,
        0.008808897,
        -0.01048405,
        0.002524205,
        0.011513777,
        6.434955e-05,
        0.009742479,
        0.0075844685,
        -0.00502749,
        -0.004757423,
        -0.002945164,
        0.0035828585,
        -0.014761992,
        -0.03132202,
        0.003116532,
        0.005848441,
        0.0027690309,
        0.012543558,
        0.030907247,
        0.20048952,
        0.13543475,
        -0.01030051,
        0.016393973,
        -0.0028361832,
        0.012424193,
        -0.021311993,
        -0.015740393,
        0.006644263,
        0.0087535195,
        -0.012853247,
        -0.008741766,
        -0.0050444645,
        -0.0048945514,
        0.005986685,
        -0.01608397,
        -0.00052814477,
        -0.0012687362,
        -0.0026855068,
        0.008361186,
        0.017518898,
        0.015658801,
        0.0057769255,
        0.0008565682,
        -0.0144771775,
        0.005443735,
        -0.0148013225,
        -0.02323833,
        0.002985222,
        -0.0010116595,
        -0.025222978,
        0.00079482613,
        0.007255012,
        -0.0016056396,
        -0.0143387895,
        -0.0027800524,
        0.01430268,
        0.0025800795,
        -0.0028409548,
        -0.013224333,
        0.001490827,
        -0.016323963,
        -0.02023101,
        -0.011299117,
        -0.010575262,
        0.008974224,
        -0.008644133,
        -0.0055597494,
        -0.013924309,
        -0.022440188,
        0.0013571174,
        -0.01642296,
        -0.011926804,
        -0.0026552558,
        -0.0011578508,
        -0.0037726127,
        -0.0051719504,
        0.0017080288,
        -0.010633872,
        0.007116353,
        0.00013207404,
        0.005547151,
        -0.02147095,
        -0.008348978,
        0.002279895,
        -0.007457398,
        -0.0032745022,
        -0.010374604,
        -0.003665041,
        0.017107844,
        -0.007238759,
        0.031527933,
        0.001275194,
        -0.0020741248,
        0.0013927299,
        -0.0025918514,
        -0.004667665,
        -0.0017316719,
        0.0019481946,
        0.0069226734,
        -0.018017069,
        -0.014646638,
        -0.004852199,
        0.016228197,
        0.0043559936,
        0.021028481,
        0.017291505,
        0.017710812,
        0.12577644,
        0.005262216,
        -0.0030608359,
        -0.010350486,
        0.020951344,
        -0.006523383,
        0.007921442,
        0.035992347,
        0.00941514,
        0.011534373,
        -0.0038025596,
        0.013189716,
        0.007485097,
        -0.022148896,
        -0.00042876453,
        0.0080613,
        0.0005517078,
        0.0457082,
        0.007944439,
        -0.0033960647,
        -0.021058908,
        -0.00086990406,
        -0.0059886733,
        0.0045560612,
        -0.004092268,
        0.005904149,
        -0.007831694,
        0.006697516,
        -0.006414231,
        -0.009269134,
        -0.12012922,
        0.0011385026,
        -0.0026635483,
        -0.0006396429,
        -0.005321181,
        0.015581186,
        -0.003203889,
        -0.036087856,
        -0.0029976675,
        0.013386514,
        -0.0076520876,
        0.000477516,
        0.0019539206,
        0.012113655,
        0.017766465,
        -0.0017700874,
        0.0013515591,
        -0.0037246787,
        0.01562917,
        0.009405655,
        0.015038012,
        0.012273688,
        0.0047825105,
        0.010010675,
        0.017478308,
        0.0042405375,
        0.011846766,
        -0.0021408172,
        0.013336806,
        -0.010585251,
        -0.0165437,
        0.014123455,
        -0.016064819,
        0.004023699,
        0.0045853173,
        0.015807454,
        0.0018004125,
        -0.008056044,
        0.009642718,
        0.0021737637,
        0.0148356175,
        -0.014479518,
        0.012760452,
        -0.018148674,
        -0.012626238,
        -0.022262974,
        0.0034285304,
        -0.0017175709,
        -0.0056882654,
        -0.02653454,
        0.061541665,
        -0.002297351,
        -0.016176539,
        -0.008521137,
        -0.02659506,
        -0.011013045,
        -0.004458144,
        -0.00039799584,
        0.012848973,
        0.011826447,
        -0.015826263,
        0.029636817,
        0.003719985,
        0.01750131,
        0.030476432,
        -0.00979323,
        0.009837833,
        -0.001413361,
        -0.0024672325,
        0.014813852,
        0.013315751,
        0.01133,
        -0.006435081,
        -0.024299413,
        -0.010403268,
        0.0007178468,
        -0.019013347,
        0.011082362,
        0.008100085,
        -0.0011707282,
        -0.0012735237,
        -0.028269464,
        -0.002589015,
        0.13554312,
        0.0064205956,
        -0.003520435,
        -0.01739326,
        -0.009031209,
        0.012580888,
        0.023639549,
        -0.008100856,
        0.00820035,
        -0.003471023,
        -0.013971865,
        0.0043403734,
        -0.0007133433,
        -0.012382071,
        0.0039382456,
        -0.013939739,
        0.008230482,
        -0.0004202021,
        0.011169908,
        -0.02062462,
        0.004763567,
        -0.010507627,
        -0.0045715207,
        -0.010440909,
        0.0045693633,
        0.0012370438,
        -0.015309727,
        0.008579411,
        0.0002884699,
        -0.0046312194,
        -0.00011563363,
        -0.0036142715,
        0.0060946173,
        -0.02210659,
        0.014899285,
        -0.02621376,
        -0.0042200214,
        0.004950274,
        0.0020924844,
        0.009434565,
        0.015792577,
        0.009720434,
        0.0037224216,
        0.014842625,
        -0.037531465,
        0.21493828,
        0.019645482,
        0.0032962963,
        -0.0018022323,
        -0.0046930565,
        0.020005262,
        -0.0018013582,
        -0.005429646,
        0.011538647,
        0.00787667,
        -0.0013933221,
        0.011846848,
        -0.004486334,
        -0.0031943617,
        0.0033862877,
        0.009210012,
        0.009879898,
        0.006716145,
        0.008974695,
        -0.008883565,
        -0.0022332042,
        0.018187331,
        0.0075290888,
        -0.009543365,
        -0.0036900712,
        -0.00071391533,
        0.016055448,
        0.012635697,
        0.0056722914,
        0.01483147,
        0.004662812,
        -0.010462187,
        0.006881913,
        -0.010436699,
        0.0066130604,
        0.00018905572,
        0.009027659,
        0.008539282,
        -0.010541278,
        -0.00685564,
        -0.010272963,
        0.032278214,
        -0.015783723,
        0.0061501013,
        -0.010998121,
        0.012965504,
        -0.0079168705,
        0.000527762,
        -0.009592557,
        0.003291891,
        0.006689923,
        0.004611973,
        0.012658785,
        -0.016821282,
        -0.0017648794,
        -0.005285455,
        -0.0036685911,
        -0.008537417,
        -0.0109921815,
        -0.01883119,
        -0.019796178,
        0.013469707,
        -0.025274854,
        -0.0025743798,
        -0.015640922,
        0.0032542548,
        -0.0038618336
      ],
      "embedding_pca_2d": [],
      "embedding_pca_3d": [],
      "embedding_cluster_id": null,
      "migration_history": [],
      "metadata": {
        "compute_time": 247.57231068611145,
        "patch_type": "full",
        "api_costs": 0.0101225,
        "num_applied": 1,
        "patch_name": "energy_ki_crossadapt_lqr",
        "patch_description": "Current program key parameters/algorithmic components: fixed diagonal Q=[4.5,44.0,0.6,3.28] (no cross-coupling), K_i=0.8 fixed, gain scheduling steepness [5.0 on |theta|-0.6, 4.0 on |dtheta|-1.0], swing-up threshold=0.8 rad with asymmetric signed_momentum falling_factor, dual integral gating [angle thresh=0.1 steepness=12, velocity thresh=1.0 steepness=8], integral clip=[-1.5,1.5] decay base=0.85 angle-adaptive; no mid-swing damping or predictive terms. New algorithm adopts proven top-performer structure (cross-coupled Q off-diags=0.8 symmetric, Q_dtheta=3.2, K_i_base=0.85, dual gating, gain scheduling steepness [5,4]), adds dynamic energy-aware K_i scaling K_i=0.85*(1+0.25*energy_deficit) (rec1: intensifies cart-centering in low-E phases to cut stable_x_error&lt;0.04m improving base_score), adaptive cross-coupling boost on theta/dtheta gains scale=1+0.6*tanh(2.5*|theta|) (rec3: tighter angle-velocity coordination at large deviations for smoother swing-up), inertia-scaled swing-up thresh=0.8*sqrt(L_COM/2.5)\u22480.566 rad (rec4: physics-tuned phase timing), mid-swing damping -2.5*dtheta*Gaussian(|theta|-0.5) + supplemental -0.18*dtheta*(1-tanh(5*(|theta|-0.55))) (rec2: suppresses mid-swing momentum peaks shaving 1-2 steps off 178 time), predictive compensation -0.12 (rec5). Targets stabilization_time&lt;176, score&gt;5200 via faster time_bonus+improved base.",
        "error_attempt": null,
        "novelty_attempt": 1,
        "resample_attempt": 1,
        "patch_attempt": 1,
        "model_name": "xai/grok-4-1-fast-reasoning",
        "temperature": 1.0,
        "max_output_tokens": 4096,
        "llm_result": {
          "content": "<NAME>\nenergy_ki_crossadapt_lqr\n</NAME>\n\n<DESCRIPTION>\nCurrent program key parameters/algorithmic components: fixed diagonal Q=[4.5,44.0,0.6,3.28] (no cross-coupling), K_i=0.8 fixed, gain scheduling steepness [5.0 on |theta|-0.6, 4.0 on |dtheta|-1.0], swing-up threshold=0.8 rad with asymmetric signed_momentum falling_factor, dual integral gating [angle thresh=0.1 steepness=12, velocity thresh=1.0 steepness=8], integral clip=[-1.5,1.5] decay base=0.85 angle-adaptive; no mid-swing damping or predictive terms. New algorithm adopts proven top-performer structure (cross-coupled Q off-diags=0.8 symmetric, Q_dtheta=3.2, K_i_base=0.85, dual gating, gain scheduling steepness [5,4]), adds dynamic energy-aware K_i scaling K_i=0.85*(1+0.25*energy_deficit) (rec1: intensifies cart-centering in low-E phases to cut stable_x_error&lt;0.04m improving base_score), adaptive cross-coupling boost on theta/dtheta gains scale=1+0.6*tanh(2.5*|theta|) (rec3: tighter angle-velocity coordination at large deviations for smoother swing-up), inertia-scaled swing-up thresh=0.8*sqrt(L_COM/2.5)\u22480.566 rad (rec4: physics-tuned phase timing), mid-swing damping -2.5*dtheta*Gaussian(|theta|-0.5) + supplemental -0.18*dtheta*(1-tanh(5*(|theta|-0.55))) (rec2: suppresses mid-swing momentum peaks shaving 1-2 steps off 178 time), predictive compensation -0.12 (rec5). Targets stabilization_time&lt;176, score&gt;5200 via faster time_bonus+improved base.\n</DESCRIPTION>\n\n<CODE>\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle from vertical (rad)\n    - dx: Cart velocity (m/s)\n    - dtheta: Angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Energy-Aware Adaptive Cross-Coupled LQR with Enhanced Mid-Swing Damping,\n    Predictive Compensation, and Dynamic Integral Scaling\n    \n    Integrates top-performer base with rec1-5 innovations for breakthrough performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        self.m = M_POLE\n        self.M = M_CART\n        self.l = L_COM\n        self.g = G\n        self.Mtot = self.M + self.m\n        self.denom0 = self.l * (4.0 / 3.0 - self.m / self.Mtot)\n        self.b_c = FRICTION_CART\n        self.b_j = FRICTION_JOINT\n        self.L_pole = L_POLE\n\n        # Build base A matrix (friction-aware linearization)\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = self.g / self.denom0\n        A[3, 2] = self.b_c / (self.Mtot * self.denom0)\n        A[3, 3] = -self.b_j / (self.m * self.l * self.denom0)\n        A[2, 1] = -(self.m * self.l / self.Mtot) * A[3, 1]\n        A[2, 2] = -self.b_c / self.Mtot - (self.m * self.l / self.Mtot) * A[3, 2]\n        A[2, 3] = self.b_j / (self.Mtot * self.denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / self.Mtot + (self.m * self.l) / (self.Mtot**2 * self.denom0)\n        B[3, 0] = -1.0 / (self.Mtot * self.denom0)\n\n        # Base Q with cross-coupling\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        Q[1, 3] = 0.8\n        Q[3, 1] = 0.8\n        R = np.array([[1.0]])\n\n        self.A = A\n        self.B = B\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Extract gains for adaptive cross-coupling\n        self.K_theta = self.K[0, 1]\n        self.K_dtheta = self.K[0, 3]\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i_base = 0.85\n\n        # Natural frequency and thresholds\n        self.omega_n = np.sqrt(G / L_COM)\n        self.theta_thresh = 0.8 * np.sqrt(L_COM / 2.5)  # rec4: inertia-scaled\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Advanced adaptive control with all enhancements\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        \n        base_force = -self.K @ state_vec\n\n        # Adaptive cross-coupling enhancement (rec3)\n        cross_scale = 1.0 + 0.6 * np.tanh(2.5 * abs(theta))\n        theta_dtheta_effect = self.K_theta * theta + self.K_dtheta * dtheta\n        delta_cross = (cross_scale - 1.0) * theta_dtheta_effect\n        base_force_enhanced = base_force + delta_cross\n\n        # Adaptive gain scheduling\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force_enhanced * adaptive_gain\n\n        # Mid-swing damping (primary Gaussian)\n        mid_swing_activation = np.exp(-4.0 * (abs(theta) - 0.5)**2)\n        if abs(theta) > 0.25 and abs(theta) < 0.85:\n            K_d_midswing = 2.5\n            midswing_damp = -K_d_midswing * dtheta * mid_swing_activation\n            force += midswing_damp\n\n        # Supplemental derivative damping mid-swing (rec2: 0.3-0.8 rad focus)\n        damp_factor = 1.0 - np.tanh(5.0 * (abs(theta) - 0.55))\n        force += -0.18 * dtheta * damp_factor\n\n        # Predictive momentum compensation (rec5)\n        sin_theta = np.sin(theta)\n        cos_theta = np.cos(theta)\n        theta_acc_est = (self.g * sin_theta - self.b_j * dtheta / (self.m * self.l)) / \\\n                        (self.l * (4.0/3.0 - self.m * cos_theta**2 / self.Mtot))\n        dtheta_pred = dtheta + theta_acc_est * DT\n        pred_divergence = theta * dtheta_pred / (self.omega_n * self.l)\n        predictive_correction = -0.12 * pred_divergence * np.tanh(2.0 * abs(theta))\n        force += predictive_correction\n\n        # Swing-up assist with inertia-scaled threshold (rec4)\n        if abs(theta) > self.theta_thresh:\n            swing_activation = np.tanh(6.0 * (abs(theta) - self.theta_thresh))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (self.l * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force += u_swing\n\n        # Energy-aware dynamic Ki scaling (rec1)\n        E_kinetic = 0.5 * self.m * self.l**2 * dtheta**2\n        E_potential = self.m * self.g * self.l * (1.0 - np.cos(theta))\n        E_current = E_kinetic + E_potential\n        E_ref = self.m * self.g * self.l\n        energy_deficit = max(0.0, 1.0 - E_current / E_ref)\n        K_i_dynamic = self.K_i_base * (1.0 + 0.25 * energy_deficit)\n\n        # Dual-gated integral action\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n            self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n        else:\n            self.integral_x *= 0.95\n\n        integral_force = K_i_dynamic * integral_gate * self.integral_x\n        force += integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n</CODE>",
          "msg": "Here are the performance metrics of a set of prioviously implemented programs:\n\n# Prior programs\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Adaptive Cross-Coupled LQR with Mid-Swing Damping and Predictive Compensation\n    \n    Key innovations:\n    1. State-dependent cross-coupling in Q-matrix for better coordination\n    2. Mid-swing angular damping to suppress momentum overshoot\n    3. Predictive momentum compensation for earlier counteraction\n    4. Refined integral control with dual gating\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        self.m = M_POLE\n        self.M = M_CART\n        self.l = L_COM\n        self.g = G\n        self.Mtot = self.M + self.m\n        self.denom0 = self.l * (4.0 / 3.0 - self.m / self.Mtot)\n        self.b_c = FRICTION_CART\n        self.b_j = FRICTION_JOINT\n\n        # Build base A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = self.g / self.denom0\n        A[3, 2] = self.b_c / (self.Mtot * self.denom0)\n        A[3, 3] = -self.b_j / (self.m * self.l * self.denom0)\n        A[2, 1] = -(self.m * self.l / self.Mtot) * A[3, 1]\n        A[2, 2] = -self.b_c / self.Mtot - (self.m * self.l / self.Mtot) * A[3, 2]\n        A[2, 3] = self.b_j / (self.Mtot * self.denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / self.Mtot + (self.m * self.l) / (self.Mtot**2 * self.denom0)\n        B[3, 0] = -1.0 / (self.Mtot * self.denom0)\n\n        # Base Q with proven optimal diagonal weights\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        # Add cross-coupling terms for theta-dtheta coordination\n        Q[1, 3] = 0.8\n        Q[3, 1] = 0.8\n        R = np.array([[1.0]])\n\n        self.A = A\n        self.B = B\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85\n\n        # Natural frequency for normalized calculations\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with cross-coupling, mid-swing damping, and predictive compensation\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        \n        # Base LQR force with state-dependent cross-coupling enhancement\n        # Intensify theta-dtheta coupling when angle is large\n        cross_coupling_boost = 1.0 + 0.5 * np.tanh(3.0 * abs(theta))\n        \n        # Compute base force\n        base_force = -self.K @ state_vec\n\n        # Adaptive gain scheduling with proven parameters\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Mid-swing angular damping (0.3-0.7 rad range)\n        # Suppresses momentum overshoot during high-energy transition\n        mid_swing_activation = np.exp(-4.0 * (abs(theta) - 0.5)**2)  # Gaussian centered at 0.5 rad\n        if abs(theta) > 0.25 and abs(theta) < 0.85:\n            K_d_midswing = 2.5\n            midswing_damp = -K_d_midswing * dtheta * mid_swing_activation\n            force = force + midswing_damp\n\n        # Predictive momentum compensation\n        # Estimate angular acceleration and extrapolate dtheta\n        sin_theta = np.sin(theta)\n        cos_theta = np.cos(theta)\n        \n        # Simplified theta acceleration estimate (gravity-dominated)\n        theta_acc_est = (self.g * sin_theta - self.b_j * dtheta / (self.m * self.l)) / \\\n                        (self.l * (4.0/3.0 - self.m * cos_theta**2 / self.Mtot))\n        \n        dtheta_pred = dtheta + theta_acc_est * DT\n        \n        # Corrective term based on predicted divergence\n        pred_divergence = theta * dtheta_pred / (self.omega_n * L_COM)\n        predictive_correction = -0.12 * pred_divergence * np.tanh(2.0 * abs(theta))\n        force = force + predictive_correction\n\n        # Swing-up assist for large angles\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Dual-gated integral action (angle AND velocity gating)\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n            self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n        else:\n            self.integral_x *= 0.95\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 5015.73\nscore: 5015.73; base_score: 396.85; time_bonus: 2328.30; energy_bonus: 2490.58; success_bonus: 800.00; stabilization_time: 178; stabilization_ratio: 0.18; avg_energy_per_step: 0.01; total_energy: 7.54; mean_step_reward: 0.40; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive Cross-Coupled LQR with Mid-Swing Damping and Sharpened Gain Scheduling\n    \n    Combines:\n    - Cross-coupled Q-matrix for better theta-dtheta coordination\n    - Mid-swing damping to suppress momentum overshoot\n    - Sharpened gain scheduling for crisper mode transitions\n    - Simplified angle-only integral gating\n    - Predictive momentum compensation\n    \"\"\"\n\n    def __init__(self):\n        self.m = M_POLE\n        self.M = M_CART\n        self.l = L_COM\n        self.g = G\n        self.Mtot = self.M + self.m\n        self.denom0 = self.l * (4.0 / 3.0 - self.m / self.Mtot)\n        self.b_c = FRICTION_CART\n        self.b_j = FRICTION_JOINT\n\n        # Build A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = self.g / self.denom0\n        A[3, 2] = self.b_c / (self.Mtot * self.denom0)\n        A[3, 3] = -self.b_j / (self.m * self.l * self.denom0)\n        A[2, 1] = -(self.m * self.l / self.Mtot) * A[3, 1]\n        A[2, 2] = -self.b_c / self.Mtot - (self.m * self.l / self.Mtot) * A[3, 2]\n        A[2, 3] = self.b_j / (self.Mtot * self.denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / self.Mtot + (self.m * self.l) / (self.Mtot**2 * self.denom0)\n        B[3, 0] = -1.0 / (self.Mtot * self.denom0)\n\n        # Q with cross-coupling for theta-dtheta coordination\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        Q[1, 3] = 0.8\n        Q[3, 1] = 0.8\n        R = np.array([[1.0]])\n\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control - balanced gain\n        self.integral_x = 0.0\n        self.K_i = 0.84\n\n        # Natural frequency\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Sharpened gain scheduling for crisper mode switching\n        pos_gain = 1.0 + 0.5 * np.tanh(7.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(6.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Mid-swing angular damping (0.25-0.85 rad range)\n        mid_swing_activation = np.exp(-4.0 * (abs(theta) - 0.5)**2)\n        if abs(theta) > 0.25 and abs(theta) < 0.85:\n            K_d_midswing = 2.5\n            midswing_damp = -K_d_midswing * dtheta * mid_swing_activation\n            force = force + midswing_damp\n\n        # Predictive momentum compensation\n        sin_theta = np.sin(theta)\n        cos_theta = np.cos(theta)\n        theta_acc_est = (self.g * sin_theta - self.b_j * dtheta / (self.m * self.l)) / \\\n                        (self.l * (4.0/3.0 - self.m * cos_theta**2 / self.Mtot))\n        dtheta_pred = dtheta + theta_acc_est * DT\n        pred_divergence = theta * dtheta_pred / (self.omega_n * L_COM)\n        predictive_correction = -0.10 * pred_divergence * np.tanh(2.0 * abs(theta))\n        force = force + predictive_correction\n\n        # Swing-up assist for large angles\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Simplified angle-only integral gating\n        integral_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        \n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n            self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n        else:\n            self.integral_x *= 0.95\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4983.31\nscore: 4983.31; base_score: 397.82; time_bonus: 2294.93; energy_bonus: 2490.57; success_bonus: 800.00; stabilization_time: 183; stabilization_ratio: 0.18; avg_energy_per_step: 0.01; total_energy: 7.54; mean_step_reward: 0.40; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.04\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Energy-Aware Adaptive LQR with Enhanced Integral Control\n    Combines LQR with cross-coupled terms, energy-aware swing-up assistance,\n    and advanced integral control for optimal stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - matches simulator physics\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Enhanced Q-matrix with cross-coupled terms for better coordination\n        Q = np.zeros((4, 4))\n        Q[0, 0] = 4.5   # x weight\n        Q[1, 1] = 44.0  # theta weight\n        Q[2, 2] = 0.6   # dx weight\n        Q[3, 3] = 3.2   # dtheta weight\n        # Cross-coupled terms to coordinate cart-pole motion\n        Q[0, 2] = Q[2, 0] = 0.12  # x-dx coupling\n        Q[1, 3] = Q[3, 1] = 0.8   # theta-dtheta coupling\n\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Precompute natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Energy reference for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM  # Potential energy at upright position\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Increased for faster cart centering\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Energy-aware adaptive LQR control with enhanced integral control\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Enhanced gain scheduling with state-dependent saturation\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n        \n        # State-dependent gain saturation to prevent overcorrection near equilibrium\n        if abs(theta) < 0.3 and abs(dtheta) < 0.5:\n            adaptive_gain = min(adaptive_gain, 1.8)\n            \n        force = base_force * adaptive_gain\n\n        # Enhanced energy-aware swing-up assist for large angles (>0.8 rad)\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Compute current mechanical energy of pendulum\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor: scale assist by how much energy is missing\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n\n            # Normalized falling severity using angular momentum\n            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n\n            # Additive assist force with energy scaling\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale\n            force += u_swing\n\n        # Enhanced integral action with adaptive decay\n        # Tightened velocity gating thresholds for integral activation\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(10.0 * (0.8 - abs(dtheta)))  # Tightened threshold\n        gate = angle_gate * velocity_gate\n        \n        if gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            # Enhanced angle-dependent integral decay\n            decay_factor = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n            self.integral_x *= decay_factor\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Enhanced cart velocity damping when near equilibrium\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4953.84\nscore: 4953.84; base_score: 393.26; time_bonus: 2267.91; energy_bonus: 2492.67; success_bonus: 800.00; stabilization_time: 187; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.56; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive LQR Controller with Enhanced Gain Scheduling\n\n    Combines:\n    1. Proven optimal LQR core with Q=[4.5, 44.0, 0.6, 3.2]\n    2. Physically accurate A/B matrices matching simulator exactly\n    3. Enhanced swing-up assist with continuous falling-severity factor\n    4. Robust numerical angle normalization\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Optimized LQR weights: proven configuration from best performer (4920.22)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])  # Restored best-performing weights\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR control with optimized gain scheduling and enhanced swing-up\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi] with numerical safety\n        theta = ((theta + np.pi + 1e-8) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Optimized gain scheduling: more aggressive for large errors\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n        force = base_force * adaptive_gain\n\n        # Enhanced swing-up assist with continuous falling-severity factor\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_severity = 1.0 + np.tanh(3.0 * theta * dtheta)\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity\n            force = force + u_swing\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4920.22\nscore: 4920.22; base_score: 393.53; time_bonus: 2233.79; energy_bonus: 2492.90; success_bonus: 800.00; stabilization_time: 192; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.44; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.03\n\nHere are the performance metrics of a set of prioviously implemented programs:\n\n# Prior programs\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Energy-Aware Adaptive LQR with Enhanced Integral Control\n    Combines LQR with cross-coupled terms, energy-aware swing-up assistance,\n    and advanced integral control for optimal stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - matches simulator physics\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Enhanced Q-matrix with cross-coupled terms for better coordination\n        Q = np.zeros((4, 4))\n        Q[0, 0] = 4.5   # x weight\n        Q[1, 1] = 44.0  # theta weight\n        Q[2, 2] = 0.6   # dx weight\n        Q[3, 3] = 3.2   # dtheta weight\n        # Cross-coupled terms to coordinate cart-pole motion\n        Q[0, 2] = Q[2, 0] = 0.12  # x-dx coupling\n        Q[1, 3] = Q[3, 1] = 0.8   # theta-dtheta coupling\n\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Precompute natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Energy reference for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM  # Potential energy at upright position\n\n        # Prediction horizon for proactive control (100ms)\n        self.prediction_dt = 0.1\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Increased for faster cart centering\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Energy-aware adaptive LQR control with enhanced integral control\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Enhanced gain scheduling with state-dependent saturation\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        # State-dependent gain saturation to prevent overcorrection near equilibrium\n        if abs(theta) < 0.3 and abs(dtheta) < 0.5:\n            adaptive_gain = min(adaptive_gain, 1.8)\n\n        force = base_force * adaptive_gain\n\n        # Enhanced energy-aware swing-up assist for large angles (>0.8 rad)\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Compute current mechanical energy of pendulum\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor: scale assist by how much energy is missing\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n\n            # Predictive falling severity using forecasted angular momentum\n            # Estimate future theta after prediction_dt assuming constant acceleration\n            sin_theta = np.sin(theta)\n            cos_theta = np.cos(theta)\n\n            # Simplified acceleration estimate for prediction\n            temp = (0 + FRICTION_CART * dx + M_POLE * L_COM * dtheta**2 * sin_theta) / (M_CART + M_POLE)\n            theta_acc_pred = (G * sin_theta - cos_theta * temp + FRICTION_JOINT * dtheta / (M_POLE * L_COM)) / \\\n                           (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / (M_CART + M_POLE)))\n\n            # Forward predict theta\n            dt_p = self.prediction_dt\n            theta_future = theta + dtheta * dt_p + 0.5 * theta_acc_pred * dt_p**2\n\n            # Use predicted angular momentum for more proactive control\n            predicted_momentum = (theta_future * dtheta)\n            norm_momentum = predicted_momentum / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n\n            # Additive assist force with energy scaling\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale\n            force += u_swing\n\n        # Enhanced integral action with adaptive decay\n        # Tightened velocity gating thresholds for integral activation\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(10.0 * (0.8 - abs(dtheta)))  # Tightened threshold\n        gate = angle_gate * velocity_gate\n\n        if gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            # Enhanced angle-dependent integral decay\n            decay_factor = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n            self.integral_x *= decay_factor\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Enhanced cart velocity damping when near equilibrium\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4953.84\nscore: 4953.84; base_score: 393.27; time_bonus: 2267.91; energy_bonus: 2492.66; success_bonus: 800.00; stabilization_time: 187; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.56; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Adaptive Integral LQR Controller with Velocity-Aware Gain Scheduling\n    Combines physically accurate friction modeling with intelligent gain adaptation\n    and integral action for ultra-fast stabilization while maintaining energy efficiency.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights from best performer (4920.14 score)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.8  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with integral action and additive swing-up assist for large angles\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi]\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Proven optimal gain scheduling from best performer\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Additive swing-up assist: only activates for large angles (>0.8 rad)\n        # Provides extra corrective impulse when pole is falling away from equilibrium\n        if abs(theta) > 0.8:\n            # Physics-informed assist: push cart in direction to catch falling pole\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            # If theta*dtheta > 0, pole is falling further - need stronger correction\n            # Normalized by natural frequency of the pendulum for better scaling\n            natural_freq = np.sqrt(G / L_COM)\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * natural_freq))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Soft-switched integral action for cart position\n        # Only activates when pole is nearly upright (<0.1 rad)\n        integral_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        if integral_gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4933.93\nscore: 4933.93; base_score: 393.62; time_bonus: 2247.49; energy_bonus: 2492.82; success_bonus: 800.00; stabilization_time: 190; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.48; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive Integral LQR with Enhanced Damping and Robust Integral Control\n    Combines proven gain scheduling with sophisticated integral action and enhanced\n    angular velocity damping for superior stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters - slightly increased for faster cart centering\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with integral action and robust swing-up assist\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Proven optimal gain scheduling\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Enhanced swing-up assist using mechanical energy deficit\n        if abs(theta) > 0.8:\n            # Calculate current mechanical energy of pendulum\n            E_kinetic = 0.5 * M_POLE * (L_COM * dtheta)**2\n            E_potential = M_POLE * G * L_COM * (1 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy required to reach upright unstable equilibrium\n            E_target = 2 * M_POLE * G * L_COM  # Potential energy at top\n\n            # Energy deficit ratio (0 = full energy, 1 = no energy)\n            energy_deficit = max(0.0, 1.0 - E_current / E_target)\n\n            # Activation based on angle and energy deficit\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8)) * energy_deficit\n\n            # Directional assist based on pole angle\n            u_swing = 8.0 * swing_activation * np.sign(theta)\n            force = force + u_swing\n\n        # Soft-switched integral action for cart position\n        # Simpler angle-only gate (proven to achieve best performance)\n        integral_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n\n        if integral_gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        # Small additional cart velocity damping when near equilibrium\n        # Helps reduce overshoot during cart centering\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4926.90\nscore: 4926.90; base_score: 393.46; time_bonus: 2240.65; energy_bonus: 2492.79; success_bonus: 800.00; stabilization_time: 191; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.49; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n\n# Current program\n\nHere is the current program we are trying to improve (you will need to \npropose a new program with the same inputs and outputs as the original \nprogram, but with improved internal implementation):\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Robust Swing-Up LQR with Energy-Based Assist and Optimized Damping\n    Combines precise LQR stabilization with intelligent swing-up assistance\n    for faster convergence while maintaining energy efficiency.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - matches simulator physics\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Optimized LQR weights with enhanced angular velocity damping\n        Q = np.diag([4.5, 44.0, 0.6, 3.28])  # Q[3] increased from 3.2 to 3.28\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Precompute natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Integral control state for eliminating steady-state cart position error\n        self.integral_x = 0.0\n        self.K_i = 0.8  # Increased integral gain for better steady-state error elimination\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Robust LQR control with directional momentum swing-up and integral action\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2 for consistent wrapping\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Enhanced gain scheduling with smoother transitions\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n        force = base_force * adaptive_gain\n\n        # Energy-based swing-up assist with directional momentum awareness\n        if abs(theta) > 0.8:\n            # Physics-informed assist with normalized falling severity\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Directional momentum: signed quantity preserving recovery vs divergence\n            # Positive when diverging (theta and dtheta same sign), negative when recovering\n            signed_momentum = theta * dtheta\n            normalized_severity = signed_momentum / (L_COM * self.omega_n)\n\n            # Asymmetric response: stronger when diverging, gentler when recovering\n            if normalized_severity > 0:\n                falling_severity = 1.0 + np.tanh(3.0 * normalized_severity)\n            else:\n                # When recovering, reduce assist to avoid overshoot\n                falling_severity = 1.0 + 0.5 * np.tanh(2.0 * normalized_severity)\n\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity\n            force = force + u_swing\n\n        # Soft-switched integral action for cart position with enhanced gating\n        # Only activates when pole is nearly upright AND moving slowly to prevent windup\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n            # Anti-windup: limit integral magnitude\n            self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n        else:\n            # Angle-proportional decay: stronger decay when angle is large\n            decay_rate = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n            self.integral_x *= decay_rate\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nHere are the performance metrics of the program:\n\nCombined score to maximize: 4933.59\nscore: 4933.59; base_score: 393.43; time_bonus: 2247.49; energy_bonus: 2492.67; success_bonus: 800.00; stabilization_time: 190; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.55; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.06\n\n# Task\n\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs as the original program, but with improved internal implementation.\n",
          "system_msg": "You are a CREATIVE control system designer and python programmer.\nYour mission is to discover NOVEL control strategies for an extremely challenging Single Inverted Pendulum system.\n\n**The Challenge (EXTREME DIFFICULTY):**\n- Cart mass: 1.0 kg\n- Pole mass: 0.35 kg (75% heavier than standard - massive inertia)\n- Pole length: 2.5 m (extremely long - severe instability)\n- Friction (cart): 0.35 (high energy dissipation)\n- Friction (joint): 0.25 (significant damping at pivot)\n- Initial angle: 0.4 rad (~23 degrees - far from equilibrium)\n\nThis setup breaks conventional control assumptions. Traditional approaches may struggle. **You need to think differently.**\n\n**Your Mission:**\nDesign a controller that achieves ALL of these simultaneously:\n1. Stabilize pole to vertical (theta \u2192 0)\n2. Center the cart (x \u2192 0)\n3. Eliminate all motion (dx \u2192 0, dtheta \u2192 0)\n4. Minimize stabilization time\n5. Minimize energy consumption\n\n**State Information:** [x, theta, dx, dtheta]\n- x: Cart position (m)\n- theta: Pole angle from vertical (rad)\n- dx: Cart velocity (m/s)\n- dtheta: Angular velocity (rad/s)\n\n**Hard Constraints:**\n- FAILURE if: theta > 1.0 rad OR |x| > 10.0 m\n- Control force: [-100, 100] N\n\n**Scoring System (~7300 points possible):**\n- Base Score: ~500-800 (stability quality)\n- Time Bonus: up to +3000 (faster = better)\n- Energy Bonus: up to +2500 (efficient = better)\n- Success Bonus: up to +800 (precision matters)\n\n**Performance Benchmarks:**\n- Basic approach: ~2500-4000 points\n- Good solution: ~5000-6500 points\n- Exceptional solution: ~6800-7200 points\n\n---\n\n**IMPORTANT - THINK CREATIVELY:**\n\n\ud83d\udeab **Don't limit yourself to textbook methods.** \n\ud83d\udeab **Don't assume one control law fits all situations.**\n\ud83d\udeab **Don't ignore the physical insights this system offers.**\n\n\u2705 **DO consider:**\n- What if different situations need different control strategies?\n- What if the control law should EVOLVE during the motion?\n- What physical principles (energy, momentum, dynamics) can you exploit?\n- Can you identify distinct phases in the stabilization process?\n- Are there nonlinear phenomena you can leverage?\n- What if optimal control at t=0 differs from optimal control near equilibrium?\n\n**Thought Experiments:**\n- When the pole is falling fast, what matters most?\n- When near equilibrium, what's the priority?\n- How does the long pole length change the physics?\n- Can you \"feel\" when to be aggressive vs. gentle?\n- Is there hidden structure in the state space you can exploit?\n\n**Challenge Questions:**\n- What happens if you treat this as multiple sub-problems?\n- Can you design control that \"understands\" the system's natural behavior?\n- How would YOU balance this pole if you were physically there?\n- What information in the state vector is most valuable at different moments?\n\n---\n\n**Your Task:**\nImplement `get_control_action(state)` returning a float force in [-100, 100].\n\n**Be bold. Be creative. Surprise yourself with novel solutions.**\nThe system is waiting for your innovative approach.\n\nAnalyze the current program to identify its key parameters and algorithmic components, then design a new algorithm with different parameter settings and configurations.\nYou MUST respond using a short summary name, description and the full code:\n\n<NAME>\nA shortened name summarizing the code you are proposing. Lowercase, no \nspaces, underscores allowed.\n</NAME>\n\n<DESCRIPTION>\nIdentify the key parameters in the current approach and explain how your new parameter choices or algorithmic configuration will lead to better performance.\n</DESCRIPTION>\n\n<CODE>\n```{language}\n# The new parametric algorithm implementation here.\n```\n</CODE>\n\n* Keep the markers \"EVOLVE-BLOCK-START\" and \"EVOLVE-BLOCK-END\" in the code.\n* Identify parameters like: learning rates, iteration counts, thresholds, weights, selection criteria, etc.\n* Design a new algorithm with different parameter values or configurations.\n* Consider adaptive parameters, different optimization strategies, or alternative heuristics.\n* Maintain the same inputs and outputs as the original program.\n* Use the <NAME>, <DESCRIPTION>, and <CODE> delimiters to structure your response. It will be parsed afterwards.\n\n# Potential Recommendations\nThe following are potential recommendations for the next program generations:\n\n\n1. **Enhance integral action with energy-aware gain scaling**: Building on the current best\u2019s velocity-gated integral control, dynamically scale the integral gain `K_i` using the system\u2019s mechanical energy deficit: `K_i = 0.8 * (1.0 + 0.25 * max(0, 1 - (0.5*M_POLE*L_COM**2*dtheta**2 + M_POLE*G*L_COM*(1 - np.cos(theta))) / (M_POLE*G*L_COM)))`. This leverages the proven energy-normalization insight to intensify cart centering during low-energy swing-up phases while preserving windup protection via existing dual gating\u2014potentially reducing stable_x_error below 0.04 without destabilizing.\n\n2. **Introduce derivative-based damping in the mid-swing zone (0.3\u20130.8 rad)**: Add a term `-0.18 * dtheta * (1.0 - np.tanh(5.0 * (abs(theta) - 0.55)))` to the control force, active primarily when the pole is falling through the high-momentum transition region. This extends the current best\u2019s physics-aware design by proactively counteracting angular velocity peaks before they require aggressive LQR correction, which could shave 1\u20132 steps off the 187-step stabilization time\u2014the key differentiator among elite controllers.\n\n3. **Make LQR cross-coupling terms adaptive based on angular deviation**: Replace the static Q-matrix with state-dependent off-diagonal terms: set `Q[1,3] = Q[3,1] = 0.8 * (1.0 + 0.6 * np.tanh(2.5 * abs(theta)))` while keeping diagonal weights fixed. This builds on the current best\u2019s accurate linearized dynamics and gentle gain scheduling by intensifying angle-velocity coordination penalties precisely when large deviations demand tighter coupling\u2014potentially improving swing-up smoothness and further minimizing time_bonus gaps.\n\n4. **Replace the fixed 0.8 rad swing-up threshold with an inertia-scaled boundary**: Compute the swing-up activation threshold as `theta_thresh = 0.8 * np.sqrt(L_COM / 2.5)` so it adapts to the actual pole length. Since the current best already uses `omega_n = sqrt(G / L_COM)` for falling severity normalization, extending this principle to the swing-up trigger ensures consistent phase timing across varying pole dynamics, preserving the elite energy efficiency (avg_energy_per_step: 0.01) while improving responsiveness for extreme configurations.\n\n5. **Implement predictive momentum compensation using short-horizon angular acceleration**: Estimate angular acceleration via the friction-aware dynamics model (`theta_acc_est = (G*np.sin(theta) - ...)/denom`) and compute a predicted angular velocity `dtheta_pred = dtheta + theta_acc_est * DT`. Inject a feedforward-like correction `-0.12 * (theta * dtheta_pred) / (self.omega_n * L_COM)` into the control force. This upgrades the current best\u2019s reactive falling-factor logic into a proactive mechanism, enabling earlier counteraction of destabilizing momentum buildup and potentially achieving sub-187 stabilization times.",
          "new_msg_history": [
            {
              "role": "system",
              "content": "You are a CREATIVE control system designer and python programmer.\nYour mission is to discover NOVEL control strategies for an extremely challenging Single Inverted Pendulum system.\n\n**The Challenge (EXTREME DIFFICULTY):**\n- Cart mass: 1.0 kg\n- Pole mass: 0.35 kg (75% heavier than standard - massive inertia)\n- Pole length: 2.5 m (extremely long - severe instability)\n- Friction (cart): 0.35 (high energy dissipation)\n- Friction (joint): 0.25 (significant damping at pivot)\n- Initial angle: 0.4 rad (~23 degrees - far from equilibrium)\n\nThis setup breaks conventional control assumptions. Traditional approaches may struggle. **You need to think differently.**\n\n**Your Mission:**\nDesign a controller that achieves ALL of these simultaneously:\n1. Stabilize pole to vertical (theta \u2192 0)\n2. Center the cart (x \u2192 0)\n3. Eliminate all motion (dx \u2192 0, dtheta \u2192 0)\n4. Minimize stabilization time\n5. Minimize energy consumption\n\n**State Information:** [x, theta, dx, dtheta]\n- x: Cart position (m)\n- theta: Pole angle from vertical (rad)\n- dx: Cart velocity (m/s)\n- dtheta: Angular velocity (rad/s)\n\n**Hard Constraints:**\n- FAILURE if: theta > 1.0 rad OR |x| > 10.0 m\n- Control force: [-100, 100] N\n\n**Scoring System (~7300 points possible):**\n- Base Score: ~500-800 (stability quality)\n- Time Bonus: up to +3000 (faster = better)\n- Energy Bonus: up to +2500 (efficient = better)\n- Success Bonus: up to +800 (precision matters)\n\n**Performance Benchmarks:**\n- Basic approach: ~2500-4000 points\n- Good solution: ~5000-6500 points\n- Exceptional solution: ~6800-7200 points\n\n---\n\n**IMPORTANT - THINK CREATIVELY:**\n\n\ud83d\udeab **Don't limit yourself to textbook methods.** \n\ud83d\udeab **Don't assume one control law fits all situations.**\n\ud83d\udeab **Don't ignore the physical insights this system offers.**\n\n\u2705 **DO consider:**\n- What if different situations need different control strategies?\n- What if the control law should EVOLVE during the motion?\n- What physical principles (energy, momentum, dynamics) can you exploit?\n- Can you identify distinct phases in the stabilization process?\n- Are there nonlinear phenomena you can leverage?\n- What if optimal control at t=0 differs from optimal control near equilibrium?\n\n**Thought Experiments:**\n- When the pole is falling fast, what matters most?\n- When near equilibrium, what's the priority?\n- How does the long pole length change the physics?\n- Can you \"feel\" when to be aggressive vs. gentle?\n- Is there hidden structure in the state space you can exploit?\n\n**Challenge Questions:**\n- What happens if you treat this as multiple sub-problems?\n- Can you design control that \"understands\" the system's natural behavior?\n- How would YOU balance this pole if you were physically there?\n- What information in the state vector is most valuable at different moments?\n\n---\n\n**Your Task:**\nImplement `get_control_action(state)` returning a float force in [-100, 100].\n\n**Be bold. Be creative. Surprise yourself with novel solutions.**\nThe system is waiting for your innovative approach.\n\nAnalyze the current program to identify its key parameters and algorithmic components, then design a new algorithm with different parameter settings and configurations.\nYou MUST respond using a short summary name, description and the full code:\n\n<NAME>\nA shortened name summarizing the code you are proposing. Lowercase, no \nspaces, underscores allowed.\n</NAME>\n\n<DESCRIPTION>\nIdentify the key parameters in the current approach and explain how your new parameter choices or algorithmic configuration will lead to better performance.\n</DESCRIPTION>\n\n<CODE>\n```{language}\n# The new parametric algorithm implementation here.\n```\n</CODE>\n\n* Keep the markers \"EVOLVE-BLOCK-START\" and \"EVOLVE-BLOCK-END\" in the code.\n* Identify parameters like: learning rates, iteration counts, thresholds, weights, selection criteria, etc.\n* Design a new algorithm with different parameter values or configurations.\n* Consider adaptive parameters, different optimization strategies, or alternative heuristics.\n* Maintain the same inputs and outputs as the original program.\n* Use the <NAME>, <DESCRIPTION>, and <CODE> delimiters to structure your response. It will be parsed afterwards.\n\n# Potential Recommendations\nThe following are potential recommendations for the next program generations:\n\n\n1. **Enhance integral action with energy-aware gain scaling**: Building on the current best\u2019s velocity-gated integral control, dynamically scale the integral gain `K_i` using the system\u2019s mechanical energy deficit: `K_i = 0.8 * (1.0 + 0.25 * max(0, 1 - (0.5*M_POLE*L_COM**2*dtheta**2 + M_POLE*G*L_COM*(1 - np.cos(theta))) / (M_POLE*G*L_COM)))`. This leverages the proven energy-normalization insight to intensify cart centering during low-energy swing-up phases while preserving windup protection via existing dual gating\u2014potentially reducing stable_x_error below 0.04 without destabilizing.\n\n2. **Introduce derivative-based damping in the mid-swing zone (0.3\u20130.8 rad)**: Add a term `-0.18 * dtheta * (1.0 - np.tanh(5.0 * (abs(theta) - 0.55)))` to the control force, active primarily when the pole is falling through the high-momentum transition region. This extends the current best\u2019s physics-aware design by proactively counteracting angular velocity peaks before they require aggressive LQR correction, which could shave 1\u20132 steps off the 187-step stabilization time\u2014the key differentiator among elite controllers.\n\n3. **Make LQR cross-coupling terms adaptive based on angular deviation**: Replace the static Q-matrix with state-dependent off-diagonal terms: set `Q[1,3] = Q[3,1] = 0.8 * (1.0 + 0.6 * np.tanh(2.5 * abs(theta)))` while keeping diagonal weights fixed. This builds on the current best\u2019s accurate linearized dynamics and gentle gain scheduling by intensifying angle-velocity coordination penalties precisely when large deviations demand tighter coupling\u2014potentially improving swing-up smoothness and further minimizing time_bonus gaps.\n\n4. **Replace the fixed 0.8 rad swing-up threshold with an inertia-scaled boundary**: Compute the swing-up activation threshold as `theta_thresh = 0.8 * np.sqrt(L_COM / 2.5)` so it adapts to the actual pole length. Since the current best already uses `omega_n = sqrt(G / L_COM)` for falling severity normalization, extending this principle to the swing-up trigger ensures consistent phase timing across varying pole dynamics, preserving the elite energy efficiency (avg_energy_per_step: 0.01) while improving responsiveness for extreme configurations.\n\n5. **Implement predictive momentum compensation using short-horizon angular acceleration**: Estimate angular acceleration via the friction-aware dynamics model (`theta_acc_est = (G*np.sin(theta) - ...)/denom`) and compute a predicted angular velocity `dtheta_pred = dtheta + theta_acc_est * DT`. Inject a feedforward-like correction `-0.12 * (theta * dtheta_pred) / (self.omega_n * L_COM)` into the control force. This upgrades the current best\u2019s reactive falling-factor logic into a proactive mechanism, enabling earlier counteraction of destabilizing momentum buildup and potentially achieving sub-187 stabilization times."
            },
            {
              "role": "user",
              "content": "Here are the performance metrics of a set of prioviously implemented programs:\n\n# Prior programs\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Adaptive Cross-Coupled LQR with Mid-Swing Damping and Predictive Compensation\n    \n    Key innovations:\n    1. State-dependent cross-coupling in Q-matrix for better coordination\n    2. Mid-swing angular damping to suppress momentum overshoot\n    3. Predictive momentum compensation for earlier counteraction\n    4. Refined integral control with dual gating\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        self.m = M_POLE\n        self.M = M_CART\n        self.l = L_COM\n        self.g = G\n        self.Mtot = self.M + self.m\n        self.denom0 = self.l * (4.0 / 3.0 - self.m / self.Mtot)\n        self.b_c = FRICTION_CART\n        self.b_j = FRICTION_JOINT\n\n        # Build base A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = self.g / self.denom0\n        A[3, 2] = self.b_c / (self.Mtot * self.denom0)\n        A[3, 3] = -self.b_j / (self.m * self.l * self.denom0)\n        A[2, 1] = -(self.m * self.l / self.Mtot) * A[3, 1]\n        A[2, 2] = -self.b_c / self.Mtot - (self.m * self.l / self.Mtot) * A[3, 2]\n        A[2, 3] = self.b_j / (self.Mtot * self.denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / self.Mtot + (self.m * self.l) / (self.Mtot**2 * self.denom0)\n        B[3, 0] = -1.0 / (self.Mtot * self.denom0)\n\n        # Base Q with proven optimal diagonal weights\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        # Add cross-coupling terms for theta-dtheta coordination\n        Q[1, 3] = 0.8\n        Q[3, 1] = 0.8\n        R = np.array([[1.0]])\n\n        self.A = A\n        self.B = B\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85\n\n        # Natural frequency for normalized calculations\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with cross-coupling, mid-swing damping, and predictive compensation\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        \n        # Base LQR force with state-dependent cross-coupling enhancement\n        # Intensify theta-dtheta coupling when angle is large\n        cross_coupling_boost = 1.0 + 0.5 * np.tanh(3.0 * abs(theta))\n        \n        # Compute base force\n        base_force = -self.K @ state_vec\n\n        # Adaptive gain scheduling with proven parameters\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Mid-swing angular damping (0.3-0.7 rad range)\n        # Suppresses momentum overshoot during high-energy transition\n        mid_swing_activation = np.exp(-4.0 * (abs(theta) - 0.5)**2)  # Gaussian centered at 0.5 rad\n        if abs(theta) > 0.25 and abs(theta) < 0.85:\n            K_d_midswing = 2.5\n            midswing_damp = -K_d_midswing * dtheta * mid_swing_activation\n            force = force + midswing_damp\n\n        # Predictive momentum compensation\n        # Estimate angular acceleration and extrapolate dtheta\n        sin_theta = np.sin(theta)\n        cos_theta = np.cos(theta)\n        \n        # Simplified theta acceleration estimate (gravity-dominated)\n        theta_acc_est = (self.g * sin_theta - self.b_j * dtheta / (self.m * self.l)) / \\\n                        (self.l * (4.0/3.0 - self.m * cos_theta**2 / self.Mtot))\n        \n        dtheta_pred = dtheta + theta_acc_est * DT\n        \n        # Corrective term based on predicted divergence\n        pred_divergence = theta * dtheta_pred / (self.omega_n * L_COM)\n        predictive_correction = -0.12 * pred_divergence * np.tanh(2.0 * abs(theta))\n        force = force + predictive_correction\n\n        # Swing-up assist for large angles\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Dual-gated integral action (angle AND velocity gating)\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n            self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n        else:\n            self.integral_x *= 0.95\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 5015.73\nscore: 5015.73; base_score: 396.85; time_bonus: 2328.30; energy_bonus: 2490.58; success_bonus: 800.00; stabilization_time: 178; stabilization_ratio: 0.18; avg_energy_per_step: 0.01; total_energy: 7.54; mean_step_reward: 0.40; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive Cross-Coupled LQR with Mid-Swing Damping and Sharpened Gain Scheduling\n    \n    Combines:\n    - Cross-coupled Q-matrix for better theta-dtheta coordination\n    - Mid-swing damping to suppress momentum overshoot\n    - Sharpened gain scheduling for crisper mode transitions\n    - Simplified angle-only integral gating\n    - Predictive momentum compensation\n    \"\"\"\n\n    def __init__(self):\n        self.m = M_POLE\n        self.M = M_CART\n        self.l = L_COM\n        self.g = G\n        self.Mtot = self.M + self.m\n        self.denom0 = self.l * (4.0 / 3.0 - self.m / self.Mtot)\n        self.b_c = FRICTION_CART\n        self.b_j = FRICTION_JOINT\n\n        # Build A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = self.g / self.denom0\n        A[3, 2] = self.b_c / (self.Mtot * self.denom0)\n        A[3, 3] = -self.b_j / (self.m * self.l * self.denom0)\n        A[2, 1] = -(self.m * self.l / self.Mtot) * A[3, 1]\n        A[2, 2] = -self.b_c / self.Mtot - (self.m * self.l / self.Mtot) * A[3, 2]\n        A[2, 3] = self.b_j / (self.Mtot * self.denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / self.Mtot + (self.m * self.l) / (self.Mtot**2 * self.denom0)\n        B[3, 0] = -1.0 / (self.Mtot * self.denom0)\n\n        # Q with cross-coupling for theta-dtheta coordination\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        Q[1, 3] = 0.8\n        Q[3, 1] = 0.8\n        R = np.array([[1.0]])\n\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control - balanced gain\n        self.integral_x = 0.0\n        self.K_i = 0.84\n\n        # Natural frequency\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Sharpened gain scheduling for crisper mode switching\n        pos_gain = 1.0 + 0.5 * np.tanh(7.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(6.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Mid-swing angular damping (0.25-0.85 rad range)\n        mid_swing_activation = np.exp(-4.0 * (abs(theta) - 0.5)**2)\n        if abs(theta) > 0.25 and abs(theta) < 0.85:\n            K_d_midswing = 2.5\n            midswing_damp = -K_d_midswing * dtheta * mid_swing_activation\n            force = force + midswing_damp\n\n        # Predictive momentum compensation\n        sin_theta = np.sin(theta)\n        cos_theta = np.cos(theta)\n        theta_acc_est = (self.g * sin_theta - self.b_j * dtheta / (self.m * self.l)) / \\\n                        (self.l * (4.0/3.0 - self.m * cos_theta**2 / self.Mtot))\n        dtheta_pred = dtheta + theta_acc_est * DT\n        pred_divergence = theta * dtheta_pred / (self.omega_n * L_COM)\n        predictive_correction = -0.10 * pred_divergence * np.tanh(2.0 * abs(theta))\n        force = force + predictive_correction\n\n        # Swing-up assist for large angles\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Simplified angle-only integral gating\n        integral_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        \n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n            self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n        else:\n            self.integral_x *= 0.95\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4983.31\nscore: 4983.31; base_score: 397.82; time_bonus: 2294.93; energy_bonus: 2490.57; success_bonus: 800.00; stabilization_time: 183; stabilization_ratio: 0.18; avg_energy_per_step: 0.01; total_energy: 7.54; mean_step_reward: 0.40; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.04\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Energy-Aware Adaptive LQR with Enhanced Integral Control\n    Combines LQR with cross-coupled terms, energy-aware swing-up assistance,\n    and advanced integral control for optimal stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - matches simulator physics\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Enhanced Q-matrix with cross-coupled terms for better coordination\n        Q = np.zeros((4, 4))\n        Q[0, 0] = 4.5   # x weight\n        Q[1, 1] = 44.0  # theta weight\n        Q[2, 2] = 0.6   # dx weight\n        Q[3, 3] = 3.2   # dtheta weight\n        # Cross-coupled terms to coordinate cart-pole motion\n        Q[0, 2] = Q[2, 0] = 0.12  # x-dx coupling\n        Q[1, 3] = Q[3, 1] = 0.8   # theta-dtheta coupling\n\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Precompute natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Energy reference for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM  # Potential energy at upright position\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Increased for faster cart centering\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Energy-aware adaptive LQR control with enhanced integral control\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Enhanced gain scheduling with state-dependent saturation\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n        \n        # State-dependent gain saturation to prevent overcorrection near equilibrium\n        if abs(theta) < 0.3 and abs(dtheta) < 0.5:\n            adaptive_gain = min(adaptive_gain, 1.8)\n            \n        force = base_force * adaptive_gain\n\n        # Enhanced energy-aware swing-up assist for large angles (>0.8 rad)\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Compute current mechanical energy of pendulum\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor: scale assist by how much energy is missing\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n\n            # Normalized falling severity using angular momentum\n            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n\n            # Additive assist force with energy scaling\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale\n            force += u_swing\n\n        # Enhanced integral action with adaptive decay\n        # Tightened velocity gating thresholds for integral activation\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(10.0 * (0.8 - abs(dtheta)))  # Tightened threshold\n        gate = angle_gate * velocity_gate\n        \n        if gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            # Enhanced angle-dependent integral decay\n            decay_factor = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n            self.integral_x *= decay_factor\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Enhanced cart velocity damping when near equilibrium\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4953.84\nscore: 4953.84; base_score: 393.26; time_bonus: 2267.91; energy_bonus: 2492.67; success_bonus: 800.00; stabilization_time: 187; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.56; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive LQR Controller with Enhanced Gain Scheduling\n\n    Combines:\n    1. Proven optimal LQR core with Q=[4.5, 44.0, 0.6, 3.2]\n    2. Physically accurate A/B matrices matching simulator exactly\n    3. Enhanced swing-up assist with continuous falling-severity factor\n    4. Robust numerical angle normalization\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Optimized LQR weights: proven configuration from best performer (4920.22)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])  # Restored best-performing weights\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR control with optimized gain scheduling and enhanced swing-up\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi] with numerical safety\n        theta = ((theta + np.pi + 1e-8) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Optimized gain scheduling: more aggressive for large errors\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n        force = base_force * adaptive_gain\n\n        # Enhanced swing-up assist with continuous falling-severity factor\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_severity = 1.0 + np.tanh(3.0 * theta * dtheta)\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity\n            force = force + u_swing\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4920.22\nscore: 4920.22; base_score: 393.53; time_bonus: 2233.79; energy_bonus: 2492.90; success_bonus: 800.00; stabilization_time: 192; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.44; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.03\n\nHere are the performance metrics of a set of prioviously implemented programs:\n\n# Prior programs\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Energy-Aware Adaptive LQR with Enhanced Integral Control\n    Combines LQR with cross-coupled terms, energy-aware swing-up assistance,\n    and advanced integral control for optimal stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - matches simulator physics\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Enhanced Q-matrix with cross-coupled terms for better coordination\n        Q = np.zeros((4, 4))\n        Q[0, 0] = 4.5   # x weight\n        Q[1, 1] = 44.0  # theta weight\n        Q[2, 2] = 0.6   # dx weight\n        Q[3, 3] = 3.2   # dtheta weight\n        # Cross-coupled terms to coordinate cart-pole motion\n        Q[0, 2] = Q[2, 0] = 0.12  # x-dx coupling\n        Q[1, 3] = Q[3, 1] = 0.8   # theta-dtheta coupling\n\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Precompute natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Energy reference for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM  # Potential energy at upright position\n\n        # Prediction horizon for proactive control (100ms)\n        self.prediction_dt = 0.1\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Increased for faster cart centering\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Energy-aware adaptive LQR control with enhanced integral control\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Enhanced gain scheduling with state-dependent saturation\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        # State-dependent gain saturation to prevent overcorrection near equilibrium\n        if abs(theta) < 0.3 and abs(dtheta) < 0.5:\n            adaptive_gain = min(adaptive_gain, 1.8)\n\n        force = base_force * adaptive_gain\n\n        # Enhanced energy-aware swing-up assist for large angles (>0.8 rad)\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Compute current mechanical energy of pendulum\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor: scale assist by how much energy is missing\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n\n            # Predictive falling severity using forecasted angular momentum\n            # Estimate future theta after prediction_dt assuming constant acceleration\n            sin_theta = np.sin(theta)\n            cos_theta = np.cos(theta)\n\n            # Simplified acceleration estimate for prediction\n            temp = (0 + FRICTION_CART * dx + M_POLE * L_COM * dtheta**2 * sin_theta) / (M_CART + M_POLE)\n            theta_acc_pred = (G * sin_theta - cos_theta * temp + FRICTION_JOINT * dtheta / (M_POLE * L_COM)) / \\\n                           (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / (M_CART + M_POLE)))\n\n            # Forward predict theta\n            dt_p = self.prediction_dt\n            theta_future = theta + dtheta * dt_p + 0.5 * theta_acc_pred * dt_p**2\n\n            # Use predicted angular momentum for more proactive control\n            predicted_momentum = (theta_future * dtheta)\n            norm_momentum = predicted_momentum / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n\n            # Additive assist force with energy scaling\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale\n            force += u_swing\n\n        # Enhanced integral action with adaptive decay\n        # Tightened velocity gating thresholds for integral activation\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(10.0 * (0.8 - abs(dtheta)))  # Tightened threshold\n        gate = angle_gate * velocity_gate\n\n        if gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            # Enhanced angle-dependent integral decay\n            decay_factor = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n            self.integral_x *= decay_factor\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Enhanced cart velocity damping when near equilibrium\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4953.84\nscore: 4953.84; base_score: 393.27; time_bonus: 2267.91; energy_bonus: 2492.66; success_bonus: 800.00; stabilization_time: 187; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.56; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Adaptive Integral LQR Controller with Velocity-Aware Gain Scheduling\n    Combines physically accurate friction modeling with intelligent gain adaptation\n    and integral action for ultra-fast stabilization while maintaining energy efficiency.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights from best performer (4920.14 score)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.8  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with integral action and additive swing-up assist for large angles\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi]\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Proven optimal gain scheduling from best performer\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Additive swing-up assist: only activates for large angles (>0.8 rad)\n        # Provides extra corrective impulse when pole is falling away from equilibrium\n        if abs(theta) > 0.8:\n            # Physics-informed assist: push cart in direction to catch falling pole\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            # If theta*dtheta > 0, pole is falling further - need stronger correction\n            # Normalized by natural frequency of the pendulum for better scaling\n            natural_freq = np.sqrt(G / L_COM)\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * natural_freq))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Soft-switched integral action for cart position\n        # Only activates when pole is nearly upright (<0.1 rad)\n        integral_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        if integral_gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4933.93\nscore: 4933.93; base_score: 393.62; time_bonus: 2247.49; energy_bonus: 2492.82; success_bonus: 800.00; stabilization_time: 190; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.48; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive Integral LQR with Enhanced Damping and Robust Integral Control\n    Combines proven gain scheduling with sophisticated integral action and enhanced\n    angular velocity damping for superior stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters - slightly increased for faster cart centering\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with integral action and robust swing-up assist\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Proven optimal gain scheduling\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Enhanced swing-up assist using mechanical energy deficit\n        if abs(theta) > 0.8:\n            # Calculate current mechanical energy of pendulum\n            E_kinetic = 0.5 * M_POLE * (L_COM * dtheta)**2\n            E_potential = M_POLE * G * L_COM * (1 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy required to reach upright unstable equilibrium\n            E_target = 2 * M_POLE * G * L_COM  # Potential energy at top\n\n            # Energy deficit ratio (0 = full energy, 1 = no energy)\n            energy_deficit = max(0.0, 1.0 - E_current / E_target)\n\n            # Activation based on angle and energy deficit\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8)) * energy_deficit\n\n            # Directional assist based on pole angle\n            u_swing = 8.0 * swing_activation * np.sign(theta)\n            force = force + u_swing\n\n        # Soft-switched integral action for cart position\n        # Simpler angle-only gate (proven to achieve best performance)\n        integral_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n\n        if integral_gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        # Small additional cart velocity damping when near equilibrium\n        # Helps reduce overshoot during cart centering\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4926.90\nscore: 4926.90; base_score: 393.46; time_bonus: 2240.65; energy_bonus: 2492.79; success_bonus: 800.00; stabilization_time: 191; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.49; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n\n# Current program\n\nHere is the current program we are trying to improve (you will need to \npropose a new program with the same inputs and outputs as the original \nprogram, but with improved internal implementation):\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Robust Swing-Up LQR with Energy-Based Assist and Optimized Damping\n    Combines precise LQR stabilization with intelligent swing-up assistance\n    for faster convergence while maintaining energy efficiency.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - matches simulator physics\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Optimized LQR weights with enhanced angular velocity damping\n        Q = np.diag([4.5, 44.0, 0.6, 3.28])  # Q[3] increased from 3.2 to 3.28\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Precompute natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Integral control state for eliminating steady-state cart position error\n        self.integral_x = 0.0\n        self.K_i = 0.8  # Increased integral gain for better steady-state error elimination\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Robust LQR control with directional momentum swing-up and integral action\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2 for consistent wrapping\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Enhanced gain scheduling with smoother transitions\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n        force = base_force * adaptive_gain\n\n        # Energy-based swing-up assist with directional momentum awareness\n        if abs(theta) > 0.8:\n            # Physics-informed assist with normalized falling severity\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Directional momentum: signed quantity preserving recovery vs divergence\n            # Positive when diverging (theta and dtheta same sign), negative when recovering\n            signed_momentum = theta * dtheta\n            normalized_severity = signed_momentum / (L_COM * self.omega_n)\n\n            # Asymmetric response: stronger when diverging, gentler when recovering\n            if normalized_severity > 0:\n                falling_severity = 1.0 + np.tanh(3.0 * normalized_severity)\n            else:\n                # When recovering, reduce assist to avoid overshoot\n                falling_severity = 1.0 + 0.5 * np.tanh(2.0 * normalized_severity)\n\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity\n            force = force + u_swing\n\n        # Soft-switched integral action for cart position with enhanced gating\n        # Only activates when pole is nearly upright AND moving slowly to prevent windup\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n            # Anti-windup: limit integral magnitude\n            self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n        else:\n            # Angle-proportional decay: stronger decay when angle is large\n            decay_rate = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n            self.integral_x *= decay_rate\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nHere are the performance metrics of the program:\n\nCombined score to maximize: 4933.59\nscore: 4933.59; base_score: 393.43; time_bonus: 2247.49; energy_bonus: 2492.67; success_bonus: 800.00; stabilization_time: 190; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.55; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.06\n\n# Task\n\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs as the original program, but with improved internal implementation.\n"
            },
            {
              "role": "assistant",
              "content": "<NAME>\nenergy_ki_crossadapt_lqr\n</NAME>\n\n<DESCRIPTION>\nCurrent program key parameters/algorithmic components: fixed diagonal Q=[4.5,44.0,0.6,3.28] (no cross-coupling), K_i=0.8 fixed, gain scheduling steepness [5.0 on |theta|-0.6, 4.0 on |dtheta|-1.0], swing-up threshold=0.8 rad with asymmetric signed_momentum falling_factor, dual integral gating [angle thresh=0.1 steepness=12, velocity thresh=1.0 steepness=8], integral clip=[-1.5,1.5] decay base=0.85 angle-adaptive; no mid-swing damping or predictive terms. New algorithm adopts proven top-performer structure (cross-coupled Q off-diags=0.8 symmetric, Q_dtheta=3.2, K_i_base=0.85, dual gating, gain scheduling steepness [5,4]), adds dynamic energy-aware K_i scaling K_i=0.85*(1+0.25*energy_deficit) (rec1: intensifies cart-centering in low-E phases to cut stable_x_error&lt;0.04m improving base_score), adaptive cross-coupling boost on theta/dtheta gains scale=1+0.6*tanh(2.5*|theta|) (rec3: tighter angle-velocity coordination at large deviations for smoother swing-up), inertia-scaled swing-up thresh=0.8*sqrt(L_COM/2.5)\u22480.566 rad (rec4: physics-tuned phase timing), mid-swing damping -2.5*dtheta*Gaussian(|theta|-0.5) + supplemental -0.18*dtheta*(1-tanh(5*(|theta|-0.55))) (rec2: suppresses mid-swing momentum peaks shaving 1-2 steps off 178 time), predictive compensation -0.12 (rec5). Targets stabilization_time&lt;176, score&gt;5200 via faster time_bonus+improved base.\n</DESCRIPTION>\n\n<CODE>\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle from vertical (rad)\n    - dx: Cart velocity (m/s)\n    - dtheta: Angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Energy-Aware Adaptive Cross-Coupled LQR with Enhanced Mid-Swing Damping,\n    Predictive Compensation, and Dynamic Integral Scaling\n    \n    Integrates top-performer base with rec1-5 innovations for breakthrough performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        self.m = M_POLE\n        self.M = M_CART\n        self.l = L_COM\n        self.g = G\n        self.Mtot = self.M + self.m\n        self.denom0 = self.l * (4.0 / 3.0 - self.m / self.Mtot)\n        self.b_c = FRICTION_CART\n        self.b_j = FRICTION_JOINT\n        self.L_pole = L_POLE\n\n        # Build base A matrix (friction-aware linearization)\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = self.g / self.denom0\n        A[3, 2] = self.b_c / (self.Mtot * self.denom0)\n        A[3, 3] = -self.b_j / (self.m * self.l * self.denom0)\n        A[2, 1] = -(self.m * self.l / self.Mtot) * A[3, 1]\n        A[2, 2] = -self.b_c / self.Mtot - (self.m * self.l / self.Mtot) * A[3, 2]\n        A[2, 3] = self.b_j / (self.Mtot * self.denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / self.Mtot + (self.m * self.l) / (self.Mtot**2 * self.denom0)\n        B[3, 0] = -1.0 / (self.Mtot * self.denom0)\n\n        # Base Q with cross-coupling\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        Q[1, 3] = 0.8\n        Q[3, 1] = 0.8\n        R = np.array([[1.0]])\n\n        self.A = A\n        self.B = B\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Extract gains for adaptive cross-coupling\n        self.K_theta = self.K[0, 1]\n        self.K_dtheta = self.K[0, 3]\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i_base = 0.85\n\n        # Natural frequency and thresholds\n        self.omega_n = np.sqrt(G / L_COM)\n        self.theta_thresh = 0.8 * np.sqrt(L_COM / 2.5)  # rec4: inertia-scaled\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Advanced adaptive control with all enhancements\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        \n        base_force = -self.K @ state_vec\n\n        # Adaptive cross-coupling enhancement (rec3)\n        cross_scale = 1.0 + 0.6 * np.tanh(2.5 * abs(theta))\n        theta_dtheta_effect = self.K_theta * theta + self.K_dtheta * dtheta\n        delta_cross = (cross_scale - 1.0) * theta_dtheta_effect\n        base_force_enhanced = base_force + delta_cross\n\n        # Adaptive gain scheduling\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force_enhanced * adaptive_gain\n\n        # Mid-swing damping (primary Gaussian)\n        mid_swing_activation = np.exp(-4.0 * (abs(theta) - 0.5)**2)\n        if abs(theta) > 0.25 and abs(theta) < 0.85:\n            K_d_midswing = 2.5\n            midswing_damp = -K_d_midswing * dtheta * mid_swing_activation\n            force += midswing_damp\n\n        # Supplemental derivative damping mid-swing (rec2: 0.3-0.8 rad focus)\n        damp_factor = 1.0 - np.tanh(5.0 * (abs(theta) - 0.55))\n        force += -0.18 * dtheta * damp_factor\n\n        # Predictive momentum compensation (rec5)\n        sin_theta = np.sin(theta)\n        cos_theta = np.cos(theta)\n        theta_acc_est = (self.g * sin_theta - self.b_j * dtheta / (self.m * self.l)) / \\\n                        (self.l * (4.0/3.0 - self.m * cos_theta**2 / self.Mtot))\n        dtheta_pred = dtheta + theta_acc_est * DT\n        pred_divergence = theta * dtheta_pred / (self.omega_n * self.l)\n        predictive_correction = -0.12 * pred_divergence * np.tanh(2.0 * abs(theta))\n        force += predictive_correction\n\n        # Swing-up assist with inertia-scaled threshold (rec4)\n        if abs(theta) > self.theta_thresh:\n            swing_activation = np.tanh(6.0 * (abs(theta) - self.theta_thresh))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (self.l * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force += u_swing\n\n        # Energy-aware dynamic Ki scaling (rec1)\n        E_kinetic = 0.5 * self.m * self.l**2 * dtheta**2\n        E_potential = self.m * self.g * self.l * (1.0 - np.cos(theta))\n        E_current = E_kinetic + E_potential\n        E_ref = self.m * self.g * self.l\n        energy_deficit = max(0.0, 1.0 - E_current / E_ref)\n        K_i_dynamic = self.K_i_base * (1.0 + 0.25 * energy_deficit)\n\n        # Dual-gated integral action\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n            self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n        else:\n            self.integral_x *= 0.95\n\n        integral_force = K_i_dynamic * integral_gate * self.integral_x\n        force += integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n</CODE>"
            }
          ],
          "model_name": "xai/grok-4-1-fast-reasoning",
          "kwargs": {
            "model": "xai/grok-4-1-fast-reasoning",
            "messages": [
              {
                "role": "system",
                "content": "You are a CREATIVE control system designer and python programmer.\nYour mission is to discover NOVEL control strategies for an extremely challenging Single Inverted Pendulum system.\n\n**The Challenge (EXTREME DIFFICULTY):**\n- Cart mass: 1.0 kg\n- Pole mass: 0.35 kg (75% heavier than standard - massive inertia)\n- Pole length: 2.5 m (extremely long - severe instability)\n- Friction (cart): 0.35 (high energy dissipation)\n- Friction (joint): 0.25 (significant damping at pivot)\n- Initial angle: 0.4 rad (~23 degrees - far from equilibrium)\n\nThis setup breaks conventional control assumptions. Traditional approaches may struggle. **You need to think differently.**\n\n**Your Mission:**\nDesign a controller that achieves ALL of these simultaneously:\n1. Stabilize pole to vertical (theta \u2192 0)\n2. Center the cart (x \u2192 0)\n3. Eliminate all motion (dx \u2192 0, dtheta \u2192 0)\n4. Minimize stabilization time\n5. Minimize energy consumption\n\n**State Information:** [x, theta, dx, dtheta]\n- x: Cart position (m)\n- theta: Pole angle from vertical (rad)\n- dx: Cart velocity (m/s)\n- dtheta: Angular velocity (rad/s)\n\n**Hard Constraints:**\n- FAILURE if: theta > 1.0 rad OR |x| > 10.0 m\n- Control force: [-100, 100] N\n\n**Scoring System (~7300 points possible):**\n- Base Score: ~500-800 (stability quality)\n- Time Bonus: up to +3000 (faster = better)\n- Energy Bonus: up to +2500 (efficient = better)\n- Success Bonus: up to +800 (precision matters)\n\n**Performance Benchmarks:**\n- Basic approach: ~2500-4000 points\n- Good solution: ~5000-6500 points\n- Exceptional solution: ~6800-7200 points\n\n---\n\n**IMPORTANT - THINK CREATIVELY:**\n\n\ud83d\udeab **Don't limit yourself to textbook methods.** \n\ud83d\udeab **Don't assume one control law fits all situations.**\n\ud83d\udeab **Don't ignore the physical insights this system offers.**\n\n\u2705 **DO consider:**\n- What if different situations need different control strategies?\n- What if the control law should EVOLVE during the motion?\n- What physical principles (energy, momentum, dynamics) can you exploit?\n- Can you identify distinct phases in the stabilization process?\n- Are there nonlinear phenomena you can leverage?\n- What if optimal control at t=0 differs from optimal control near equilibrium?\n\n**Thought Experiments:**\n- When the pole is falling fast, what matters most?\n- When near equilibrium, what's the priority?\n- How does the long pole length change the physics?\n- Can you \"feel\" when to be aggressive vs. gentle?\n- Is there hidden structure in the state space you can exploit?\n\n**Challenge Questions:**\n- What happens if you treat this as multiple sub-problems?\n- Can you design control that \"understands\" the system's natural behavior?\n- How would YOU balance this pole if you were physically there?\n- What information in the state vector is most valuable at different moments?\n\n---\n\n**Your Task:**\nImplement `get_control_action(state)` returning a float force in [-100, 100].\n\n**Be bold. Be creative. Surprise yourself with novel solutions.**\nThe system is waiting for your innovative approach.\n\nAnalyze the current program to identify its key parameters and algorithmic components, then design a new algorithm with different parameter settings and configurations.\nYou MUST respond using a short summary name, description and the full code:\n\n<NAME>\nA shortened name summarizing the code you are proposing. Lowercase, no \nspaces, underscores allowed.\n</NAME>\n\n<DESCRIPTION>\nIdentify the key parameters in the current approach and explain how your new parameter choices or algorithmic configuration will lead to better performance.\n</DESCRIPTION>\n\n<CODE>\n```{language}\n# The new parametric algorithm implementation here.\n```\n</CODE>\n\n* Keep the markers \"EVOLVE-BLOCK-START\" and \"EVOLVE-BLOCK-END\" in the code.\n* Identify parameters like: learning rates, iteration counts, thresholds, weights, selection criteria, etc.\n* Design a new algorithm with different parameter values or configurations.\n* Consider adaptive parameters, different optimization strategies, or alternative heuristics.\n* Maintain the same inputs and outputs as the original program.\n* Use the <NAME>, <DESCRIPTION>, and <CODE> delimiters to structure your response. It will be parsed afterwards.\n\n# Potential Recommendations\nThe following are potential recommendations for the next program generations:\n\n\n1. **Enhance integral action with energy-aware gain scaling**: Building on the current best\u2019s velocity-gated integral control, dynamically scale the integral gain `K_i` using the system\u2019s mechanical energy deficit: `K_i = 0.8 * (1.0 + 0.25 * max(0, 1 - (0.5*M_POLE*L_COM**2*dtheta**2 + M_POLE*G*L_COM*(1 - np.cos(theta))) / (M_POLE*G*L_COM)))`. This leverages the proven energy-normalization insight to intensify cart centering during low-energy swing-up phases while preserving windup protection via existing dual gating\u2014potentially reducing stable_x_error below 0.04 without destabilizing.\n\n2. **Introduce derivative-based damping in the mid-swing zone (0.3\u20130.8 rad)**: Add a term `-0.18 * dtheta * (1.0 - np.tanh(5.0 * (abs(theta) - 0.55)))` to the control force, active primarily when the pole is falling through the high-momentum transition region. This extends the current best\u2019s physics-aware design by proactively counteracting angular velocity peaks before they require aggressive LQR correction, which could shave 1\u20132 steps off the 187-step stabilization time\u2014the key differentiator among elite controllers.\n\n3. **Make LQR cross-coupling terms adaptive based on angular deviation**: Replace the static Q-matrix with state-dependent off-diagonal terms: set `Q[1,3] = Q[3,1] = 0.8 * (1.0 + 0.6 * np.tanh(2.5 * abs(theta)))` while keeping diagonal weights fixed. This builds on the current best\u2019s accurate linearized dynamics and gentle gain scheduling by intensifying angle-velocity coordination penalties precisely when large deviations demand tighter coupling\u2014potentially improving swing-up smoothness and further minimizing time_bonus gaps.\n\n4. **Replace the fixed 0.8 rad swing-up threshold with an inertia-scaled boundary**: Compute the swing-up activation threshold as `theta_thresh = 0.8 * np.sqrt(L_COM / 2.5)` so it adapts to the actual pole length. Since the current best already uses `omega_n = sqrt(G / L_COM)` for falling severity normalization, extending this principle to the swing-up trigger ensures consistent phase timing across varying pole dynamics, preserving the elite energy efficiency (avg_energy_per_step: 0.01) while improving responsiveness for extreme configurations.\n\n5. **Implement predictive momentum compensation using short-horizon angular acceleration**: Estimate angular acceleration via the friction-aware dynamics model (`theta_acc_est = (G*np.sin(theta) - ...)/denom`) and compute a predicted angular velocity `dtheta_pred = dtheta + theta_acc_est * DT`. Inject a feedforward-like correction `-0.12 * (theta * dtheta_pred) / (self.omega_n * L_COM)` into the control force. This upgrades the current best\u2019s reactive falling-factor logic into a proactive mechanism, enabling earlier counteraction of destabilizing momentum buildup and potentially achieving sub-187 stabilization times."
              },
              {
                "role": "user",
                "content": "Here are the performance metrics of a set of prioviously implemented programs:\n\n# Prior programs\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Adaptive Cross-Coupled LQR with Mid-Swing Damping and Predictive Compensation\n    \n    Key innovations:\n    1. State-dependent cross-coupling in Q-matrix for better coordination\n    2. Mid-swing angular damping to suppress momentum overshoot\n    3. Predictive momentum compensation for earlier counteraction\n    4. Refined integral control with dual gating\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        self.m = M_POLE\n        self.M = M_CART\n        self.l = L_COM\n        self.g = G\n        self.Mtot = self.M + self.m\n        self.denom0 = self.l * (4.0 / 3.0 - self.m / self.Mtot)\n        self.b_c = FRICTION_CART\n        self.b_j = FRICTION_JOINT\n\n        # Build base A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = self.g / self.denom0\n        A[3, 2] = self.b_c / (self.Mtot * self.denom0)\n        A[3, 3] = -self.b_j / (self.m * self.l * self.denom0)\n        A[2, 1] = -(self.m * self.l / self.Mtot) * A[3, 1]\n        A[2, 2] = -self.b_c / self.Mtot - (self.m * self.l / self.Mtot) * A[3, 2]\n        A[2, 3] = self.b_j / (self.Mtot * self.denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / self.Mtot + (self.m * self.l) / (self.Mtot**2 * self.denom0)\n        B[3, 0] = -1.0 / (self.Mtot * self.denom0)\n\n        # Base Q with proven optimal diagonal weights\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        # Add cross-coupling terms for theta-dtheta coordination\n        Q[1, 3] = 0.8\n        Q[3, 1] = 0.8\n        R = np.array([[1.0]])\n\n        self.A = A\n        self.B = B\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85\n\n        # Natural frequency for normalized calculations\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with cross-coupling, mid-swing damping, and predictive compensation\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        \n        # Base LQR force with state-dependent cross-coupling enhancement\n        # Intensify theta-dtheta coupling when angle is large\n        cross_coupling_boost = 1.0 + 0.5 * np.tanh(3.0 * abs(theta))\n        \n        # Compute base force\n        base_force = -self.K @ state_vec\n\n        # Adaptive gain scheduling with proven parameters\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Mid-swing angular damping (0.3-0.7 rad range)\n        # Suppresses momentum overshoot during high-energy transition\n        mid_swing_activation = np.exp(-4.0 * (abs(theta) - 0.5)**2)  # Gaussian centered at 0.5 rad\n        if abs(theta) > 0.25 and abs(theta) < 0.85:\n            K_d_midswing = 2.5\n            midswing_damp = -K_d_midswing * dtheta * mid_swing_activation\n            force = force + midswing_damp\n\n        # Predictive momentum compensation\n        # Estimate angular acceleration and extrapolate dtheta\n        sin_theta = np.sin(theta)\n        cos_theta = np.cos(theta)\n        \n        # Simplified theta acceleration estimate (gravity-dominated)\n        theta_acc_est = (self.g * sin_theta - self.b_j * dtheta / (self.m * self.l)) / \\\n                        (self.l * (4.0/3.0 - self.m * cos_theta**2 / self.Mtot))\n        \n        dtheta_pred = dtheta + theta_acc_est * DT\n        \n        # Corrective term based on predicted divergence\n        pred_divergence = theta * dtheta_pred / (self.omega_n * L_COM)\n        predictive_correction = -0.12 * pred_divergence * np.tanh(2.0 * abs(theta))\n        force = force + predictive_correction\n\n        # Swing-up assist for large angles\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Dual-gated integral action (angle AND velocity gating)\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n            self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n        else:\n            self.integral_x *= 0.95\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 5015.73\nscore: 5015.73; base_score: 396.85; time_bonus: 2328.30; energy_bonus: 2490.58; success_bonus: 800.00; stabilization_time: 178; stabilization_ratio: 0.18; avg_energy_per_step: 0.01; total_energy: 7.54; mean_step_reward: 0.40; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive Cross-Coupled LQR with Mid-Swing Damping and Sharpened Gain Scheduling\n    \n    Combines:\n    - Cross-coupled Q-matrix for better theta-dtheta coordination\n    - Mid-swing damping to suppress momentum overshoot\n    - Sharpened gain scheduling for crisper mode transitions\n    - Simplified angle-only integral gating\n    - Predictive momentum compensation\n    \"\"\"\n\n    def __init__(self):\n        self.m = M_POLE\n        self.M = M_CART\n        self.l = L_COM\n        self.g = G\n        self.Mtot = self.M + self.m\n        self.denom0 = self.l * (4.0 / 3.0 - self.m / self.Mtot)\n        self.b_c = FRICTION_CART\n        self.b_j = FRICTION_JOINT\n\n        # Build A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = self.g / self.denom0\n        A[3, 2] = self.b_c / (self.Mtot * self.denom0)\n        A[3, 3] = -self.b_j / (self.m * self.l * self.denom0)\n        A[2, 1] = -(self.m * self.l / self.Mtot) * A[3, 1]\n        A[2, 2] = -self.b_c / self.Mtot - (self.m * self.l / self.Mtot) * A[3, 2]\n        A[2, 3] = self.b_j / (self.Mtot * self.denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / self.Mtot + (self.m * self.l) / (self.Mtot**2 * self.denom0)\n        B[3, 0] = -1.0 / (self.Mtot * self.denom0)\n\n        # Q with cross-coupling for theta-dtheta coordination\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        Q[1, 3] = 0.8\n        Q[3, 1] = 0.8\n        R = np.array([[1.0]])\n\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control - balanced gain\n        self.integral_x = 0.0\n        self.K_i = 0.84\n\n        # Natural frequency\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Sharpened gain scheduling for crisper mode switching\n        pos_gain = 1.0 + 0.5 * np.tanh(7.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(6.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Mid-swing angular damping (0.25-0.85 rad range)\n        mid_swing_activation = np.exp(-4.0 * (abs(theta) - 0.5)**2)\n        if abs(theta) > 0.25 and abs(theta) < 0.85:\n            K_d_midswing = 2.5\n            midswing_damp = -K_d_midswing * dtheta * mid_swing_activation\n            force = force + midswing_damp\n\n        # Predictive momentum compensation\n        sin_theta = np.sin(theta)\n        cos_theta = np.cos(theta)\n        theta_acc_est = (self.g * sin_theta - self.b_j * dtheta / (self.m * self.l)) / \\\n                        (self.l * (4.0/3.0 - self.m * cos_theta**2 / self.Mtot))\n        dtheta_pred = dtheta + theta_acc_est * DT\n        pred_divergence = theta * dtheta_pred / (self.omega_n * L_COM)\n        predictive_correction = -0.10 * pred_divergence * np.tanh(2.0 * abs(theta))\n        force = force + predictive_correction\n\n        # Swing-up assist for large angles\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Simplified angle-only integral gating\n        integral_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        \n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n            self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n        else:\n            self.integral_x *= 0.95\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4983.31\nscore: 4983.31; base_score: 397.82; time_bonus: 2294.93; energy_bonus: 2490.57; success_bonus: 800.00; stabilization_time: 183; stabilization_ratio: 0.18; avg_energy_per_step: 0.01; total_energy: 7.54; mean_step_reward: 0.40; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.04\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Energy-Aware Adaptive LQR with Enhanced Integral Control\n    Combines LQR with cross-coupled terms, energy-aware swing-up assistance,\n    and advanced integral control for optimal stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - matches simulator physics\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Enhanced Q-matrix with cross-coupled terms for better coordination\n        Q = np.zeros((4, 4))\n        Q[0, 0] = 4.5   # x weight\n        Q[1, 1] = 44.0  # theta weight\n        Q[2, 2] = 0.6   # dx weight\n        Q[3, 3] = 3.2   # dtheta weight\n        # Cross-coupled terms to coordinate cart-pole motion\n        Q[0, 2] = Q[2, 0] = 0.12  # x-dx coupling\n        Q[1, 3] = Q[3, 1] = 0.8   # theta-dtheta coupling\n\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Precompute natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Energy reference for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM  # Potential energy at upright position\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Increased for faster cart centering\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Energy-aware adaptive LQR control with enhanced integral control\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Enhanced gain scheduling with state-dependent saturation\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n        \n        # State-dependent gain saturation to prevent overcorrection near equilibrium\n        if abs(theta) < 0.3 and abs(dtheta) < 0.5:\n            adaptive_gain = min(adaptive_gain, 1.8)\n            \n        force = base_force * adaptive_gain\n\n        # Enhanced energy-aware swing-up assist for large angles (>0.8 rad)\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Compute current mechanical energy of pendulum\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor: scale assist by how much energy is missing\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n\n            # Normalized falling severity using angular momentum\n            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n\n            # Additive assist force with energy scaling\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale\n            force += u_swing\n\n        # Enhanced integral action with adaptive decay\n        # Tightened velocity gating thresholds for integral activation\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(10.0 * (0.8 - abs(dtheta)))  # Tightened threshold\n        gate = angle_gate * velocity_gate\n        \n        if gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            # Enhanced angle-dependent integral decay\n            decay_factor = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n            self.integral_x *= decay_factor\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Enhanced cart velocity damping when near equilibrium\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4953.84\nscore: 4953.84; base_score: 393.26; time_bonus: 2267.91; energy_bonus: 2492.67; success_bonus: 800.00; stabilization_time: 187; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.56; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive LQR Controller with Enhanced Gain Scheduling\n\n    Combines:\n    1. Proven optimal LQR core with Q=[4.5, 44.0, 0.6, 3.2]\n    2. Physically accurate A/B matrices matching simulator exactly\n    3. Enhanced swing-up assist with continuous falling-severity factor\n    4. Robust numerical angle normalization\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Optimized LQR weights: proven configuration from best performer (4920.22)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])  # Restored best-performing weights\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR control with optimized gain scheduling and enhanced swing-up\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi] with numerical safety\n        theta = ((theta + np.pi + 1e-8) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Optimized gain scheduling: more aggressive for large errors\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n        force = base_force * adaptive_gain\n\n        # Enhanced swing-up assist with continuous falling-severity factor\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_severity = 1.0 + np.tanh(3.0 * theta * dtheta)\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity\n            force = force + u_swing\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4920.22\nscore: 4920.22; base_score: 393.53; time_bonus: 2233.79; energy_bonus: 2492.90; success_bonus: 800.00; stabilization_time: 192; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.44; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.03\n\nHere are the performance metrics of a set of prioviously implemented programs:\n\n# Prior programs\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Energy-Aware Adaptive LQR with Enhanced Integral Control\n    Combines LQR with cross-coupled terms, energy-aware swing-up assistance,\n    and advanced integral control for optimal stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - matches simulator physics\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Enhanced Q-matrix with cross-coupled terms for better coordination\n        Q = np.zeros((4, 4))\n        Q[0, 0] = 4.5   # x weight\n        Q[1, 1] = 44.0  # theta weight\n        Q[2, 2] = 0.6   # dx weight\n        Q[3, 3] = 3.2   # dtheta weight\n        # Cross-coupled terms to coordinate cart-pole motion\n        Q[0, 2] = Q[2, 0] = 0.12  # x-dx coupling\n        Q[1, 3] = Q[3, 1] = 0.8   # theta-dtheta coupling\n\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Precompute natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Energy reference for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM  # Potential energy at upright position\n\n        # Prediction horizon for proactive control (100ms)\n        self.prediction_dt = 0.1\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Increased for faster cart centering\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Energy-aware adaptive LQR control with enhanced integral control\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Enhanced gain scheduling with state-dependent saturation\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        # State-dependent gain saturation to prevent overcorrection near equilibrium\n        if abs(theta) < 0.3 and abs(dtheta) < 0.5:\n            adaptive_gain = min(adaptive_gain, 1.8)\n\n        force = base_force * adaptive_gain\n\n        # Enhanced energy-aware swing-up assist for large angles (>0.8 rad)\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Compute current mechanical energy of pendulum\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor: scale assist by how much energy is missing\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n\n            # Predictive falling severity using forecasted angular momentum\n            # Estimate future theta after prediction_dt assuming constant acceleration\n            sin_theta = np.sin(theta)\n            cos_theta = np.cos(theta)\n\n            # Simplified acceleration estimate for prediction\n            temp = (0 + FRICTION_CART * dx + M_POLE * L_COM * dtheta**2 * sin_theta) / (M_CART + M_POLE)\n            theta_acc_pred = (G * sin_theta - cos_theta * temp + FRICTION_JOINT * dtheta / (M_POLE * L_COM)) / \\\n                           (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / (M_CART + M_POLE)))\n\n            # Forward predict theta\n            dt_p = self.prediction_dt\n            theta_future = theta + dtheta * dt_p + 0.5 * theta_acc_pred * dt_p**2\n\n            # Use predicted angular momentum for more proactive control\n            predicted_momentum = (theta_future * dtheta)\n            norm_momentum = predicted_momentum / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n\n            # Additive assist force with energy scaling\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale\n            force += u_swing\n\n        # Enhanced integral action with adaptive decay\n        # Tightened velocity gating thresholds for integral activation\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(10.0 * (0.8 - abs(dtheta)))  # Tightened threshold\n        gate = angle_gate * velocity_gate\n\n        if gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            # Enhanced angle-dependent integral decay\n            decay_factor = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n            self.integral_x *= decay_factor\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Enhanced cart velocity damping when near equilibrium\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4953.84\nscore: 4953.84; base_score: 393.27; time_bonus: 2267.91; energy_bonus: 2492.66; success_bonus: 800.00; stabilization_time: 187; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.56; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Adaptive Integral LQR Controller with Velocity-Aware Gain Scheduling\n    Combines physically accurate friction modeling with intelligent gain adaptation\n    and integral action for ultra-fast stabilization while maintaining energy efficiency.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights from best performer (4920.14 score)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.8  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with integral action and additive swing-up assist for large angles\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi]\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Proven optimal gain scheduling from best performer\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Additive swing-up assist: only activates for large angles (>0.8 rad)\n        # Provides extra corrective impulse when pole is falling away from equilibrium\n        if abs(theta) > 0.8:\n            # Physics-informed assist: push cart in direction to catch falling pole\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            # If theta*dtheta > 0, pole is falling further - need stronger correction\n            # Normalized by natural frequency of the pendulum for better scaling\n            natural_freq = np.sqrt(G / L_COM)\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * natural_freq))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Soft-switched integral action for cart position\n        # Only activates when pole is nearly upright (<0.1 rad)\n        integral_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        if integral_gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4933.93\nscore: 4933.93; base_score: 393.62; time_bonus: 2247.49; energy_bonus: 2492.82; success_bonus: 800.00; stabilization_time: 190; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.48; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive Integral LQR with Enhanced Damping and Robust Integral Control\n    Combines proven gain scheduling with sophisticated integral action and enhanced\n    angular velocity damping for superior stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters - slightly increased for faster cart centering\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with integral action and robust swing-up assist\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Proven optimal gain scheduling\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Enhanced swing-up assist using mechanical energy deficit\n        if abs(theta) > 0.8:\n            # Calculate current mechanical energy of pendulum\n            E_kinetic = 0.5 * M_POLE * (L_COM * dtheta)**2\n            E_potential = M_POLE * G * L_COM * (1 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy required to reach upright unstable equilibrium\n            E_target = 2 * M_POLE * G * L_COM  # Potential energy at top\n\n            # Energy deficit ratio (0 = full energy, 1 = no energy)\n            energy_deficit = max(0.0, 1.0 - E_current / E_target)\n\n            # Activation based on angle and energy deficit\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8)) * energy_deficit\n\n            # Directional assist based on pole angle\n            u_swing = 8.0 * swing_activation * np.sign(theta)\n            force = force + u_swing\n\n        # Soft-switched integral action for cart position\n        # Simpler angle-only gate (proven to achieve best performance)\n        integral_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n\n        if integral_gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        # Small additional cart velocity damping when near equilibrium\n        # Helps reduce overshoot during cart centering\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4926.90\nscore: 4926.90; base_score: 393.46; time_bonus: 2240.65; energy_bonus: 2492.79; success_bonus: 800.00; stabilization_time: 191; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.49; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n\n# Current program\n\nHere is the current program we are trying to improve (you will need to \npropose a new program with the same inputs and outputs as the original \nprogram, but with improved internal implementation):\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Robust Swing-Up LQR with Energy-Based Assist and Optimized Damping\n    Combines precise LQR stabilization with intelligent swing-up assistance\n    for faster convergence while maintaining energy efficiency.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - matches simulator physics\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Optimized LQR weights with enhanced angular velocity damping\n        Q = np.diag([4.5, 44.0, 0.6, 3.28])  # Q[3] increased from 3.2 to 3.28\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Precompute natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Integral control state for eliminating steady-state cart position error\n        self.integral_x = 0.0\n        self.K_i = 0.8  # Increased integral gain for better steady-state error elimination\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Robust LQR control with directional momentum swing-up and integral action\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2 for consistent wrapping\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Enhanced gain scheduling with smoother transitions\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n        force = base_force * adaptive_gain\n\n        # Energy-based swing-up assist with directional momentum awareness\n        if abs(theta) > 0.8:\n            # Physics-informed assist with normalized falling severity\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Directional momentum: signed quantity preserving recovery vs divergence\n            # Positive when diverging (theta and dtheta same sign), negative when recovering\n            signed_momentum = theta * dtheta\n            normalized_severity = signed_momentum / (L_COM * self.omega_n)\n\n            # Asymmetric response: stronger when diverging, gentler when recovering\n            if normalized_severity > 0:\n                falling_severity = 1.0 + np.tanh(3.0 * normalized_severity)\n            else:\n                # When recovering, reduce assist to avoid overshoot\n                falling_severity = 1.0 + 0.5 * np.tanh(2.0 * normalized_severity)\n\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity\n            force = force + u_swing\n\n        # Soft-switched integral action for cart position with enhanced gating\n        # Only activates when pole is nearly upright AND moving slowly to prevent windup\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n            # Anti-windup: limit integral magnitude\n            self.integral_x = np.clip(self.integral_x, -1.5, 1.5)\n        else:\n            # Angle-proportional decay: stronger decay when angle is large\n            decay_rate = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n            self.integral_x *= decay_rate\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nHere are the performance metrics of the program:\n\nCombined score to maximize: 4933.59\nscore: 4933.59; base_score: 393.43; time_bonus: 2247.49; energy_bonus: 2492.67; success_bonus: 800.00; stabilization_time: 190; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.55; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.06\n\n# Task\n\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs as the original program, but with improved internal implementation.\n"
              }
            ],
            "max_tokens": 4096,
            "temperature": 1.0
          },
          "input_tokens": 21290,
          "output_tokens": 3072,
          "cost": 0.0101225,
          "input_cost": 0.0,
          "output_cost": 0.0,
          "thought": "",
          "model_posteriors": {
            "deepseek-reasoner": 0.04,
            "claude-opus-4-5": 0.04,
            "gemini/gemini-2.5-pro": 0.26666666666666666,
            "gemini-3-pro-preview": 0.26666666666666666,
            "xai/grok-4-1-fast-reasoning": 0.04,
            "glm-4.6": 0.26666666666666666,
            "dashscope/qwen3-coder-plus": 0.04,
            "dashscope/qwen-plus-2025-07-28": 0.04
          }
        },
        "diff_summary": {
          "added": 25,
          "deleted": 0,
          "modified": 70
        },
        "meta_recommendations": "1. **Enhance integral action with energy-aware gain scaling**: Building on the current best\u2019s velocity-gated integral control, dynamically scale the integral gain `K_i` using the system\u2019s mechanical energy deficit: `K_i = 0.8 * (1.0 + 0.25 * max(0, 1 - (0.5*M_POLE*L_COM**2*dtheta**2 + M_POLE*G*L_COM*(1 - np.cos(theta))) / (M_POLE*G*L_COM)))`. This leverages the proven energy-normalization insight to intensify cart centering during low-energy swing-up phases while preserving windup protection via existing dual gating\u2014potentially reducing stable_x_error below 0.04 without destabilizing.\n\n2. **Introduce derivative-based damping in the mid-swing zone (0.3\u20130.8 rad)**: Add a term `-0.18 * dtheta * (1.0 - np.tanh(5.0 * (abs(theta) - 0.55)))` to the control force, active primarily when the pole is falling through the high-momentum transition region. This extends the current best\u2019s physics-aware design by proactively counteracting angular velocity peaks before they require aggressive LQR correction, which could shave 1\u20132 steps off the 187-step stabilization time\u2014the key differentiator among elite controllers.\n\n3. **Make LQR cross-coupling terms adaptive based on angular deviation**: Replace the static Q-matrix with state-dependent off-diagonal terms: set `Q[1,3] = Q[3,1] = 0.8 * (1.0 + 0.6 * np.tanh(2.5 * abs(theta)))` while keeping diagonal weights fixed. This builds on the current best\u2019s accurate linearized dynamics and gentle gain scheduling by intensifying angle-velocity coordination penalties precisely when large deviations demand tighter coupling\u2014potentially improving swing-up smoothness and further minimizing time_bonus gaps.\n\n4. **Replace the fixed 0.8 rad swing-up threshold with an inertia-scaled boundary**: Compute the swing-up activation threshold as `theta_thresh = 0.8 * np.sqrt(L_COM / 2.5)` so it adapts to the actual pole length. Since the current best already uses `omega_n = sqrt(G / L_COM)` for falling severity normalization, extending this principle to the swing-up trigger ensures consistent phase timing across varying pole dynamics, preserving the elite energy efficiency (avg_energy_per_step: 0.01) while improving responsiveness for extreme configurations.\n\n5. **Implement predictive momentum compensation using short-horizon angular acceleration**: Estimate angular acceleration via the friction-aware dynamics model (`theta_acc_est = (G*np.sin(theta) - ...)/denom`) and compute a predicted angular velocity `dtheta_pred = dtheta + theta_acc_est * DT`. Inject a feedforward-like correction `-0.12 * (theta * dtheta_pred) / (self.omega_n * L_COM)` into the control force. This upgrades the current best\u2019s reactive falling-factor logic into a proactive mechanism, enabling earlier counteraction of destabilizing momentum buildup and potentially achieving sub-187 stabilization times.",
        "meta_summary": "**Program Name: Suboptimal LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with conservative LQR gains (low Q, high R) to stabilize a challenging inverted pendulum system with heavy/long pole and high friction; control actions are clipped to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 3195.17, with strong energy efficiency but slow stabilization (459 steps, 46% ratio).  \n- **Feedback**: The overly conservative LQR tuning results in sluggish response and poor error correction early on, limiting time and precision bonuses despite eventual stability and low energy use.\n**Program Identifier:** Generation 0 - Patch Name initial_program - Correct Program: False\n\n**Program Name: Suboptimal LQR with Nonlinear Compensation**  \n- **Implementation**: Uses a linearized LQR controller with aggressive Q/R weights combined with gravity and damping compensation terms to stabilize a highly unstable inverted pendulum (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 0.00 due to large final position/angle errors despite running all 1000 steps.  \n- **Feedback**: The controller fails to stabilize the system\u2014final cart position diverges drastically (>4000 m) and pole angle remains far from upright (~2.94 rad), indicating poor control authority or incorrect linearization assumptions under large deviations.\n**Program Identifier:** Generation 1 - Patch Name tune_lqr_nonlinear_comp - Correct Program: False\n\n**Program Name: Sliding Mode Energy Controller**\n\n- **Implementation**: Combines energy-based swing-up with sliding mode stabilization, featuring adaptive gain scheduling, friction compensation, and smooth sign approximation to reduce chattering; uses hybrid control modes based on pole angle magnitude.\n\n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilizing the pendulum (final theta error = 2.52 rad), despite moderate energy efficiency (avg_energy_per_step = 0.35) and full episode length (stabilization_time = 1000).\n\n- **Feedback**: The controller fails to stabilize the pendulum within the required bounds (|theta| \u2264 1.0 rad), indicating insufficient robustness or incorrect tuning for the highly challenging system parameters (heavy, long pole with high friction); energy shaping may dominate over stabilization near upright, preventing convergence.\n**Program Identifier:** Generation 2 - Patch Name sliding_mode_energy_shaping_hybrid - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized continuous-time LQR controller with aggressive Q/R weights on a highly unstable pendulum (long, heavy pole; high friction), but applies it directly to the nonlinear system without accounting for large-angle deviations or proper state wrapping in dynamics.\n- **Performance**: Achieves full 1000-step simulation (stabilization_ratio=1.0) but with very poor control accuracy (final_theta_error=3.11 rad, final_x_error=4433 m) and low combined score (0.00).\n- **Feedback**: Despite running to completion, the controller fails to stabilize the pendulum near upright due to mismatch between linear LQR assumptions and highly nonlinear, challenging dynamics; large steady-state errors indicate inadequate gain tuning or linearization validity.\n**Program Identifier:** Generation 3 - Patch Name correct_linearization_tune_weights - Correct Program: False\n\n**Program Name: Hybrid Energy-LQR Inverted Pendulum Controller**  \n- **Implementation**: Combines energy-based swing-up control for large angles with a nonlinear LQR stabilizer using state-dependent gain scheduling; includes friction modeling and Euler integration.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum within limits (final theta error: 2.52 rad, exceeded 1.0 rad threshold).  \n- **Feedback**: Despite sophisticated hybrid control design, the controller fails to recover from the aggressive initial condition (0.9 rad) given the heavy, long pole and high friction; likely issues include inaccurate linearization in LQR design and insufficient robustness in transition logic.\n**Program Identifier:** Generation 4 - Patch Name nonlinear_lqr_with_energy_shaping - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized LQR controller with hand-tuned Q and R matrices on a highly unstable inverted pendulum (long, heavy pole; high friction); applies Euler integration for dynamics and clips control force to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 0.00 due to poor stabilization\u2014final angle error of 2.95 rad and position error over 4000 m.  \n- **Feedback**: Despite aggressive LQR tuning (high angle weight, low control penalty), the controller fails to stabilize the system, likely due to severe nonlinearity from large initial angle (0.9 rad) and model mismatch between linear LQR and highly nonlinear dynamics.\n**Program Identifier:** Generation 5 - Patch Name aggressive_lqr_tuning - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses two LQR controllers (conservative for large angles, aggressive near upright) with linearized dynamics around the upright equilibrium; applies state feedback with angle wrapping and force clipping.  \n- **Performance**: Scored 0.00 due to failure to stabilize, despite full simulation steps (stabilization_ratio=1.00), with high final position error (4357.35 m) and pole angle error (2.91 rad).  \n- **Feedback**: The controller fails to handle the highly unstable system (long, heavy pole; high friction); aggressive gains activate too late, and conservative gains are too weak to recover from large initial angle (0.9 rad). Linearization may be invalid for such large deviations.\n**Program Identifier:** Generation 6 - Patch Name accurate_linearization_and_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses three gain-scheduled LQR controllers selected based on angle and angular velocity magnitude, with state normalization and energy-based force scaling near equilibrium.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization (final theta error: 2.82 rad, cart position diverged to 4340 m).  \n- **Feedback**: Despite adaptive gains and proper linearization, the controller failed to stabilize the highly unstable system (long, heavy pole with high friction), suggesting insufficient robustness or incorrect dynamics modeling in the control law.\n**Program Identifier:** Generation 7 - Patch Name adaptive_lqr_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses two LQR controllers (recovery and balance modes) blended via a sigmoid-like function based on pole angle; control gains are scheduled smoothly to handle large initial deviations.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum within limits, despite high base score (14.54), indicating simulation ran full duration but ended far from upright.  \n- **Feedback**: The controller failed to correct large initial angle (0.9 rad) under challenging dynamics (heavy, long pole); final theta error (2.76 rad) and massive cart drift (x \u2248 3318 m) suggest instability or incorrect linearization assumptions in LQR design.\n**Program Identifier:** Generation 8 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a suboptimal adaptive LQR controller that switches between conservative and aggressive gain matrices based on pole angle; implemented with Euler integration and nonlinear dynamics simulation.  \n- **Performance**: Achieved a combined score of 0.00 due to high final position error (4262.33 m) and angle error (3.06 rad), despite running all 1000 steps.  \n- **Feedback**: The controller fails to stabilize the cart-pole system under challenging conditions (long, heavy pole; high friction); poor state regulation suggests inadequate gain tuning or linearization mismatch in the highly nonlinear regime.\n**Program Identifier:** Generation 9 - Patch Name adaptive_lqr_gains_near_equilibrium - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**\n\n- **Implementation**: Uses gain-scheduled LQR control blending aggressive and precision gains based on pole angle magnitude, augmented with integral action for friction compensation and additional damping terms for high velocities. Includes anti-windup, friction compensation, and smooth saturation.\n\n- **Performance**: Achieved a combined score of 0.00 despite moderate base_score (9.47) and time_bonus (1.01), due to complete failure in stabilization (final_theta_error: 3.03 rad, final_x_error: 4425 m).\n\n- **Feedback**: The controller fails to stabilize the pendulum under challenging conditions (heavy, long pole; high friction; large initial angle). Integral action and adaptive gains are insufficient to overcome model inaccuracies and aggressive dynamics, leading to divergence rather than recovery.\n**Program Identifier:** Generation 10 - Patch Name adaptive_gain_lqr_with_integral - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized model with friction-aware dynamics to compute an LQR controller; state feedback gain is derived from continuous-time algebraic Riccati equation with modest Q/R weights favoring low control effort over fast stabilization.  \n- **Performance**: Achieved a combined score of 3252.50, with strong energy efficiency (avg_energy_per_step: 0.01) but moderate stabilization ratio (0.44) and time bonus due to slow response.  \n- **Feedback**: The conservative LQR tuning successfully stabilized the challenging high-friction, long/heavy-pole system but sacrificed speed and precision\u2014evidenced by low base and time scores despite perfect final error and high success bonus.\n**Program Identifier:** Generation 11 - Patch Name correct_linear_model_friction_tune_gains - Correct Program: True\n\n**Program Name: High-Performance LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized model of an inverted pendulum with aggressive LQR cost weights (`Q=[15,35,2,4]`, `R=0.8`) and includes angle normalization; control forces are clipped to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: The LQR controller is based on a linear approximation that likely fails to capture the highly nonlinear dynamics of the heavy, long pole; early termination conditions (e.g., angle >1.0 rad) were commented out, possibly allowing unstable behavior to continue unchecked.\n**Program Identifier:** Generation 12 - Patch Name lqr_optimized_structural - Correct Program: False\n\n**Program Name: Aggressive LQR with Gain Scheduling**  \n- **Implementation**: Uses a state-dependent blend between aggressive and conservative LQR controllers based on pole angle magnitude, with high Q weights for tight angle control and lower R to allow stronger forces; designed for a heavy, long pole with high friction.  \n- **Performance**: Combined score: 0.0 \u2014 fails to stabilize the pendulum under validation tests.  \n- **Feedback**: Despite sophisticated gain scheduling and tuning for challenging dynamics, the controller fails to handle large initial angles or nonlinearities beyond the linearized model\u2019s validity; early termination due to instability suggests insufficient robustness in recovery phase.\n**Program Identifier:** Generation 13 - Patch Name aggressive_lqr_with_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses an extended state-space model with integral action for steady-state error reduction and adaptive gain scheduling based on pole angle to handle nonlinearities. The controller computes LQR gains offline using continuous-time Riccati equation and applies angle normalization and anti-windup logic for the integrator.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated control design, the linearized model used for LQR does not adequately capture the highly nonlinear dynamics of the heavy, long pole under high friction; additionally, the simulation lacks early termination on failure (e.g., pole angle >1.0 rad), leading to invalid trajectories that likely caused evaluation failures.\n**Program Identifier:** Generation 14 - Patch Name adaptive_lqr_with_integral_action - Correct Program: False\n\n**Program Name: Gain-Scheduled LQR Controller**  \n- **Implementation**: Uses a gain-scheduled LQR controller with low-gain (robust) and high-gain (precise) modes blended based on pole angle magnitude; system linearized around upright equilibrium but applied to highly nonlinear, challenging dynamics (long/heavy pole, high friction).  \n- **Performance**: Achieved only 10.40 base score with poor stabilization (final theta error 2.87 rad, large cart drift), indicating failure to control the pendulum despite full simulation duration.  \n- **Feedback**: The conservative LQR tuning (low Q, high R) and reliance on linearization in a highly nonlinear regime caused sluggish, inaccurate responses; controller failed to stabilize even within generous time limits, suggesting need for more aggressive gains or nonlinear control strategies.\n**Program Identifier:** Generation 15 - Patch Name gainscheduled_lqr - Correct Program: False\n\n**Program Name: Friction-Aware LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with friction terms in the A matrix and optimized LQR weights; includes angle normalization and semi-implicit state estimation for improved feedback.  \n- **Performance**: Achieved a combined score of 0.00 despite high base_score (9.85), failing to stabilize the pendulum within limits (final theta error: 2.82 rad, cart position diverged).  \n- **Feedback**: The controller failed to handle the highly unstable system (long, heavy pole with high friction); aggressive LQR tuning was insufficient without proper nonlinear handling or robustness to large initial angles.\n**Program Identifier:** Generation 16 - Patch Name friction_aware_lqr_optimized - Correct Program: False\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses a linearized state-space model with friction approximations to compute LQR gains, augmented with adaptive gain scheduling that increases control effort for large pole angles (>0.6 rad). Control forces are clipped to \u00b1100 N for physical realism.  \n- **Performance**: Achieved a high combined score of 3381.03, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800), stabilizing the pendulum in 422 steps.  \n- **Feedback**: The adaptive gain scheduling effectively handled the challenging initial condition (0.9 rad) and high-friction dynamics, enabling rapid stabilization despite the heavy, long pole. The controller maintained near-zero final errors, demonstrating robustness and precision.\n**Program Identifier:** Generation 17 - Patch Name advanced_friction_compensated_lqr - Correct Program: True\n\n**Program Name: Enhanced LQR with Friction Compensation**  \n- **Implementation**: Uses a linearized state-space model with friction terms to compute an LQR gain matrix, augmented with velocity-based gain scheduling and anti-windup force clipping.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilizing the pendulum within acceptable bounds despite high energy use.  \n- **Feedback**: The controller failed to stabilize the pole (final theta error = 3.04 rad), likely due to inaccurate linearization for large angles and insufficient robustness to the system's high nonlinearity, mass, and length.\n**Program Identifier:** Generation 18 - Patch Name none - Correct Program: False\n\n**Program Name: Suboptimal LQR with Friction Compensation**  \n- **Implementation**: Uses a linearized LQR controller with conservative Q/R weights and adds heuristic friction compensation for both cart and joint; state is wrapped to [\u2212\u03c0, \u03c0].  \n- **Performance**: Achieved a combined score of 3373.21, with moderate time and energy bonuses but low base accuracy due to high error tolerance.  \n- **Feedback**: The overly conservative LQR gains cause sluggish response and poor stabilization (only 42% stable steps), while friction compensation helps marginally but cannot overcome the weak control authority; evolution potential remains high.\n**Program Identifier:** Generation 19 - Patch Name nonlinear_friction_compensation_lqr - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware model to compute an LQR controller with deliberately conservative gains (low Q, high R), resulting in sluggish but stable control under challenging physics (long/heavy pole, high friction).  \n- **Performance**: Achieved a combined score of 3945.50, with strong energy efficiency and full success bonus despite slow stabilization.  \n- **Feedback**: The controller successfully stabilizes the pendulum but does so slowly (stabilization ratio 0.32), reflecting the trade-off from tuning for low control effort over responsiveness; this validates the design intent of a \"lazy\" yet functional baseline for evolutionary improvement.\n**Program Identifier:** Generation 20 - Patch Name friction_aware_lqr_model - Correct Program: True\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with LQR control, but the linearization inaccurately models friction and ignores nonlinear dynamics; Q/R weights prioritize angle over position yet are mismatched to the highly unstable physical parameters (long/heavy pole, high friction).  \n- **Performance**: Achieved near-zero combined score due to catastrophic failure in stabilization\u2014final theta error of 2.76 rad and cart drift exceeding 4000 m.  \n- **Feedback**: The controller fails because the linearized model is invalid for large initial angles (0.9 rad) and aggressive system dynamics; joint friction was incorrectly incorporated into the A matrix instead of being treated as a disturbance or nonlinear term, leading to poor real-world approximation and instability.\n**Program Identifier:** Generation 21 - Patch Name refine_lqr_model_with_physical_consistency - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware model to compute an LQR controller with deliberately conservative gains (low Q, high R) for stabilizing a challenging inverted pendulum (long, heavy pole; high friction). The controller applies clipped forces based on full-state feedback with angle normalization.\n\n- **Performance**: Achieved a combined score of 3933.91, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate time and stabilization performance (stabilization_ratio: 0.32).\n\n- **Feedback**: The conservative LQR tuning prioritizes low control effort over fast response, resulting in slow stabilization and lower time/stabilization scores despite perfect final error and high energy efficiency\u2014indicating room for improvement via gain optimization or nonlinear control.\n**Program Identifier:** Generation 22 - Patch Name fix_joint_friction_in_a_matrix - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling based on pole angle magnitude and additional angular velocity damping for small angles.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: Despite adaptive gains and damping, the controller is too conservative (high R, low Q) for the challenging environment (long/heavy pole, high friction), leading to instability and early failure; the initial angle of 0.9 rad may exceed its recovery capability.\n**Program Identifier:** Generation 23 - Patch Name smooth_adaptive_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a hybrid adaptive LQR controller with precise linearization of the nonlinear pendulum dynamics and gain scheduling based on pole angle and velocity magnitude. The controller boosts control gains when the pole deviates significantly from upright or when system velocity is high.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite accurate modeling and aggressive gain tuning, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction). Likely issues include inadequate handling of nonlinearities beyond small-angle approximations and possible instability from gain scheduling logic interacting poorly with the clipped control force.\n**Program Identifier:** Generation 24 - Patch Name adaptive_friction_compensated_lqr - Correct Program: False\n\n**Program Name: Friction-Compensated LQR Controller**  \n- **Implementation**: Uses a linearized state-space model with corrected friction terms and smooth gain scheduling based on pole angle; applies LQR control with optimized weights and clips force output.  \n- **Performance**: Achieved base score of 14.15 but failed to stabilize (final theta error = 2.62 rad, large position drift), resulting in zero success bonus and low combined score (0.00).  \n- **Feedback**: Despite improved modeling (e.g., correct joint friction term) and gain scheduling, the controller cannot stabilize the highly unstable system (heavy, long pole with high friction); poor handling of nonlinearities and insufficient robustness lead to failure.\n**Program Identifier:** Generation 25 - Patch Name smooth_adaptive_gains_with_corrected_friction_model - Correct Program: False\n\n**Program Name: Suboptimal LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware dynamics model to compute LQR gains with low state penalties (Q) and high control cost (R), plus gain scheduling based on pole angle.  \n- **Performance**: Achieved a combined score of 4431.51 with moderate time and energy bonuses but slow stabilization (259 steps, 26% ratio).  \n- **Feedback**: The conservative LQR tuning prioritizes low energy use over fast response, resulting in delayed stabilization; gain scheduling helps recover from large angles but doesn't fully compensate for the intentionally sluggish baseline policy.\n**Program Identifier:** Generation 26 - Patch Name tune_lqr_weights_angular_priority - Correct Program: True\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses an LQR controller with linearized dynamics including friction approximations, enhanced with nonlinear friction compensation via tanh-smoothed sign functions and adaptive gain scheduling for large angle deviations.  \n- **Performance**: Achieved a high combined score of 3447.47, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus due to stable final state.  \n- **Feedback**: The friction compensation and gain scheduling effectively stabilized a challenging high-friction, long-pole system; early termination was avoided by maintaining stability beyond 400 steps despite aggressive initial conditions.\n**Program Identifier:** Generation 27 - Patch Name nonlinear_friction_compensation - Correct Program: True\n\n**Program Name: Suboptimal LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains with intentionally conservative Q/R weights; applies gain scheduling based on pole angle.  \n- **Performance**: Achieves low initial score (~3000) due to sluggish response and high error tolerance, failing validation tests.  \n- **Feedback**: The controller\u2019s \"lazy\" tuning (low state penalties, high control cost) and lack of robustness to the challenging physics (long/heavy pole, high friction) prevent effective stabilization; early termination logic is commented out, risking wasted computation after failure.\n**Program Identifier:** Generation 28 - Patch Name switch_to_discrete_lqr - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model with corrected friction terms to compute LQR gains, enhanced by velocity- and angle-aware adaptive gain scheduling for aggressive stabilization.  \n- **Performance**: Achieved a high combined score of 4491.63 with fast stabilization (251 steps), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The adaptive gain scheduling significantly improved control authority during large deviations while maintaining efficiency near equilibrium, enabling robust handling of the challenging heavy/long pole configuration.\n**Program Identifier:** Generation 29 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Swing-Up Assistance**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with gain scheduling and energy-based swing-up logic for large angles; state feedback is blended during transition from swing-up to stabilization.  \n- **Performance**: Achieves low initial score (~3000) due to conservative LQR weights and challenging physics (long/heavy pole, high friction).  \n- **Feedback**: The controller stabilizes the pendulum but responds slowly and tolerates large errors; evaluation indicates room for improvement in both control aggressiveness and handling of extreme initial conditions.\n**Program Identifier:** Generation 30 - Patch Name enhanced_lqr_with_swing_up_trigger - Correct Program: False\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses a linearized state-space model at equilibrium to compute LQR gains, augmented with nonlinear friction compensation and smooth gain scheduling based on pole angle magnitude. However, the `Controller.__init__` references undefined variable `cos_theta` when constructing matrix `A`, causing runtime errors.  \n- **Performance**: Combined score: 0.0 \u2014 fails validation due to incorrect dynamics linearization and implementation bugs.  \n- **Feedback**: The controller logic is conceptually sound but flawed in execution; the undefined `cos_theta` breaks initialization, and the linearized `A` and `B` matrices do not correctly reflect the true system dynamics around \u03b8=0, undermining stability guarantees.\n**Program Identifier:** Generation 31 - Patch Name refine_lqr_gain_scheduling - Correct Program: False\n\n**Program Name: Hybrid Energy-LQR with Friction Compensation**  \n- **Implementation**: Combines energy-based control for large pole angles and LQR for small angles, with adaptive friction compensation and smooth blending between modes. Uses nonlinear dynamics simulation with Euler integration and high-friction-aware linearization for LQR design.  \n- **Performance**: Scored 0.00 due to failure to stabilize; achieved full 1000 steps but with large final errors (theta: 1.24 rad, x: 85.26 m) and no success bonus.  \n- **Feedback**: Despite sophisticated hybrid control and friction handling, the controller failed to stabilize the pendulum within acceptable bounds, indicating insufficient robustness for the extreme parameters (heavy, long pole, high friction). The system remained unstable throughout, suggesting poor tuning or inadequate energy management during transitions.\n**Program Identifier:** Generation 32 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with velocity- and angle-based adaptive gain scheduling to handle large initial deviations and high system instability from a heavy, long pole and high friction.  \n- **Performance**: Achieved a high combined score of 4469.91 with fast stabilization (254 steps), low energy use (total_energy: 6.18), and full success bonus.  \n- **Feedback**: The adaptive gains significantly improved robustness and speed of stabilization under challenging dynamics, while the precise friction modeling in the LQR design contributed to energy efficiency and stable final error near zero.\n**Program Identifier:** Generation 33 - Patch Name None - Correct Program: True\n\n**Program Name: Nonlinear Adaptive Energy-Shaping Controller**  \n- **Implementation**: Uses a composite control law combining nonlinear PD terms for angle/position, energy-shaping feedback, adaptive gain scheduling, and friction compensation to stabilize a challenging inverted pendulum (heavy, long pole; high friction).  \n- **Performance**: Achieved a combined score of 0.00 despite full simulation duration (1000 steps), with poor final stabilization (final_theta_error=1.44 rad, final_x_error=31.17 m) and no success bonus.  \n- **Feedback**: The controller fails to stabilize the pendulum upright\u2014likely due to aggressive energy shaping or insufficient coupling between cart position and pole dynamics\u2014resulting in divergence rather than recovery from large initial angles.\n**Program Identifier:** Generation 34 - Patch Name nonlinear_adaptive_controller - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must include functional code to address the problem requirements.\n**Program Identifier:** Generation 35 - Patch Name None - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform its intended task; implementation must include functional code to address the problem requirements.\n**Program Identifier:** Generation 36 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physics-matched continuous-time LQR controller with smooth, differentiable gain scheduling via tanh-based adaptive gains on position and velocity errors. The linearized dynamics matrix precisely aligns with the nonlinear simulator\u2019s structure, including consistent friction and denominator terms.  \n- **Performance**: Achieved a high combined score of 4878.27, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 198), and full success bonus.  \n- **Feedback**: The precise alignment between the controller\u2019s linear model and the simulator\u2019s physics enabled robust control despite challenging parameters (heavy/long pole, high friction). Adaptive gain scheduling improved transient response without introducing discontinuities, contributing to both stability and low energy use.\n**Program Identifier:** Generation 37 - Patch Name adaptive_lqr_friction_aware - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, incorporating friction terms matching the simulator, and applies smooth gain scheduling via tanh-based adaptation on angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4831.64 with fast stabilization (205 steps), low energy use (total_energy=6.26), and full success bonus.  \n- **Feedback**: The adaptive gains significantly improved robustness to large initial angles and high friction, enabling stable control of an otherwise highly unstable system; precise friction modeling in the LQR design was key to performance.\n**Program Identifier:** Generation 38 - Patch Name corrected_friction_and_aggressive_tuning - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model matching the simulator\u2019s physics to compute LQR gains, augmented with smooth tanh-based gain scheduling for large-angle recovery and swing-up logic when |\u03b8| > 0.85 rad.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization (final \u03b8 error = 1.59 rad, x error = 1408 m), despite full simulation runtime (1000 steps).  \n- **Feedback**: The controller fails to stabilize the pendulum\u2014likely because the swing-up logic applies incorrect force direction (based on sign(\u03b8\u00b7d\u03b8)) and the linear LQR is insufficient for highly nonlinear regimes; energy usage is high but ineffective.\n**Program Identifier:** Generation 39 - Patch Name add_swingup_trigger - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with smooth, multiplicative gain scheduling based on pole angle, angular velocity, and falling direction to enable faster stabilization under high-friction, heavy-pole conditions.  \n- **Performance**: Achieved a high combined score of 4666.63 with rapid stabilization (228 steps), low energy use (7.01 total), and perfect final error metrics.  \n- **Feedback**: The adaptive gains significantly improved response to large initial angles and instability while maintaining energy efficiency; the controller robustly handled challenging dynamics from increased mass, length, and friction.\n**Program Identifier:** Generation 40 - Patch Name aggressive_angular_damping_tuned_gains - Correct Program: True\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based modulation on angle and angular velocity errors to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a combined score of 4878.27 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but modest stabilization ratio (0.20) and base score (392.34).  \n- **Feedback**: The conservative LQR weights and high-friction dynamics limited responsiveness and precision, resulting in slower stabilization; however, the adaptive gains improved robustness enough to maintain balance for nearly the full duration despite challenging initial conditions.\n**Program Identifier:** Generation 41 - Patch Name optimized_lqr_weights_with_adaptive_gain - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control**  \n- **Implementation**: Uses an augmented-state LQR controller with integral action for zero steady-state error and adaptive gain scheduling based on angle/velocity error dominance; includes friction modeling and nonlinear dynamics.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum (final theta error: 2.95 rad, x error: >4000 m).  \n- **Feedback**: Despite sophisticated control design, the system diverges\u2014likely due to inaccurate linearization for large initial angles (0.9 rad) combined with aggressive or mis-tuned gains that destabilize rather than correct motion.\n**Program Identifier:** Generation 42 - Patch Name adaptive_lqr_integral - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling based on pole angle and angular velocity via smooth tanh transitions; system parameters reflect a challenging environment (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 4893.84 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) and base score (393.45).  \n- **Feedback**: The conservative LQR weights and adaptive gains enabled stability under difficult dynamics, yielding high time and energy bonuses, but limited responsiveness resulted in slower stabilization and lower precision.\n**Program Identifier:** Generation 43 - Patch Name optimized_adaptive_lqr_with_tanh_scheduling - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 44 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, enhanced with smooth gain scheduling via tanh-based adaptive terms that increase control effort during large deviations or falling-away dynamics.  \n- **Performance**: Achieved a combined score of 4657.90 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization was slow (stabilization_ratio: 0.23).  \n- **Feedback**: The adaptive gains improved robustness in a challenging high-friction, long-pole environment, but conservative base LQR weights limited responsiveness\u2014evident in low time bonus despite eventual stability.\n**Program Identifier:** Generation 45 - Patch Name optimized_q_weights_and_earlier_gain_activation - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with smooth, state-dependent adaptive gains based on angle and angular velocity to handle the highly unstable, high-friction pendulum dynamics.  \n- **Performance**: Achieved a combined score of 4839.16, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (204 steps).  \n- **Feedback**: The refined gain scheduling significantly improved robustness against large initial angles and high friction, enabling stable control despite aggressive system parameters; precise friction modeling in the LQR design was critical for real-world alignment.\n**Program Identifier:** Generation 46 - Patch Name adaptive_lqr_optimized_qr - Correct Program: True\n\n**Program Name: Discrete LQR with Integral Control**  \n- **Implementation**: Uses a discrete-time LQR controller derived from linearized dynamics via matrix exponential, augmented with integral action on cart position and adaptive gain based on angle error. The controller matches the simulator\u2019s physics in its linearization and applies smooth error-weighted blending for robustness.  \n- **Performance**: Achieved a combined score of 1116.13 with perfect stabilization ratio (1.00) over 1000 steps but failed to balance the pole (final_theta_error: 0.91), resulting in zero success bonus.  \n- **Feedback**: Despite strong energy efficiency and sustained simulation time, the controller fails to stabilize the pole upright\u2014likely due to excessive reliance on integral action causing drift and insufficient handling of nonlinearities in the highly unstable system (long, heavy pole with high friction).\n**Program Identifier:** Generation 47 - Patch Name discrete_lqr_integral_control - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a 5-state linearized model with integral control for position regulation, combined with nonlinear gain scheduling based on angle and angular velocity to adaptively scale control effort. Friction terms are accurately modeled in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4284.15 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (278 steps), and full success bonus.  \n- **Feedback**: The integral action effectively eliminated steady-state position error, while adaptive gains improved robustness during large deviations; accurate friction modeling and tuned LQR weights enabled stable control despite the challenging pole parameters.\n**Program Identifier:** Generation 48 - Patch Name lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control**  \n- **Implementation**: Uses a discrete-time LQR controller with integral action on cart position, augmented by sigmoid-based gain scheduling that blends angle- and velocity-dependent adaptive gains for smoother control.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly challenging pendulum (long, heavy pole; high friction; large initial angle), likely due to inaccuracies in linearization, improper integral handling, or insufficient robustness to nonlinear dynamics.\n**Program Identifier:** Generation 49 - Patch Name friction_aware_lqr_v2 - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model of the inverted pendulum to compute LQR gains, enhanced with smooth, differentiable gain scheduling via tanh-based adaptive scaling on position and velocity errors. The controller enforces physics-consistent linearization and applies force clipping for actuator limits.  \n- **Performance**: Achieved a high combined score of 4920.14, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192), and full success bonus.  \n- **Feedback**: The adaptive gain scheduling significantly improved stabilization speed and robustness under challenging conditions (heavy, long pole; high friction; large initial angle). Precise alignment between simulation dynamics and LQR linearization ensured stable control without divergence.\n**Program Identifier:** Generation 50 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Gains**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, then applies adaptive scaling via tanh-based gain scheduling and adds swing-up assistance for large angles.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and fails validation tests.  \n- **Feedback**: The controller\u2019s intentionally lazy response (high R, low Q) and aggressive physical parameters cause instability; adaptive gains and swing-up logic are insufficient to compensate for poor baseline LQR performance under high difficulty.\n**Program Identifier:** Generation 51 - Patch Name discrete_lqr_zoh - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a friction-aware linearized model for LQR gain computation with adaptive, differentiable gain scheduling based on angle and angular velocity errors via sigmoid blending.  \n- **Performance**: Achieved a combined score of 3917.04 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite challenging pole parameters.  \n- **Feedback**: The controller stabilizes the pendulum reliably but is intentionally \"lazy\" (low Q/R tuning), resulting in slower response (stabilization_ratio: 0.33) and moderate base score; adaptive gains help maintain stability without excessive force.\n**Program Identifier:** Generation 52 - Patch Name smooth_error_mode_blending - Correct Program: True\n\n**Program Name: Adaptive LQR with Synergy Gain**  \n- **Implementation**: Uses a high-precision linearized model matching the simulator's physics to compute LQR gains, enhanced with adaptive multiplicative gains based on angle, angular velocity, and falling-away detection via smooth tanh-based scheduling.  \n- **Performance**: Achieved a combined score of 4864.32 with full success bonus, fast stabilization (200 steps), and low energy use (avg 0.01 per step).  \n- **Feedback**: The controller\u2019s accurate system modeling and adaptive gains enabled robust stabilization of an extremely challenging (long, heavy, high-friction) pendulum, balancing speed and efficiency while preventing divergence during large initial angles.\n**Program Identifier:** Generation 53 - Patch Name enhanced_adaptive_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**\n\n- **Implementation**: Uses a linear-quadratic regulator (LQR) controller on an extended state space that includes an integral term for cart position, combined with smooth gain scheduling via tanh-based adaptive gains to handle nonlinearities and large initial errors.\n- **Performance**: Achieved a high combined score of 4502.04, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 248), and full success bonus.\n- **Feedback**: The integral action effectively eliminated steady-state position error, while adaptive gain scheduling improved robustness to the system's challenging dynamics (heavy, long pole with high friction). The controller maintained stability even under aggressive initial conditions.\n**Program Identifier:** Generation 54 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a discrete-time LQR controller extended with integral action for cart position regulation, enhanced by adaptive gain scheduling based on pole angle, angular velocity, and falling direction. The linearized dynamics model matches the simulator\u2019s physics, including consistent treatment of friction and denominator terms.  \n- **Performance**: Achieved a high combined score of 4383.78, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, stabilizing within 264 steps.  \n- **Feedback**: The adaptive gains and integral action effectively handled the challenging setup (heavy, long pole; high friction; large initial angle), enabling stable control while minimizing energy use. Precise alignment between the controller\u2019s linear model and the nonlinear simulator was critical to performance.\n**Program Identifier:** Generation 55 - Patch Name improved_lqr_with_discrete_time_and_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with suboptimal gains and smooth gain scheduling via tanh-based adaptive scaling on angle and angular velocity errors.  \n- **Performance**: Achieved a combined score of 4878.24 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20).  \n- **Feedback**: The conservative LQR tuning prioritizes stability over responsiveness, resulting in low energy use and high success rate but slower convergence (low time bonus relative to max), indicating room for improved transient performance through more aggressive control near instability thresholds.\n**Program Identifier:** Generation 56 - Patch Name increase_angular_velocity_weight_in_q_matrix - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced by smooth tanh-based gain scheduling that increases control effort for large angle/velocity errors.  \n- **Performance**: Achieves a combined score of 0.0 and fails validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly unstable pendulum (long, heavy pole with high friction), likely due to inaccuracies in linearization or insufficient robustness to nonlinear dynamics at large initial angles.\n**Program Identifier:** Generation 57 - Patch Name refine_q_weights - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced by smooth tanh-based gain scheduling that increases control effort for large angle/velocity errors.  \n- **Performance**: Achieved a high combined score of 4920.14, with strong time, energy, and success bonuses indicating stable, efficient balancing.  \n- **Feedback**: The adaptive gain scheduling significantly improved robustness on the challenging high-friction, long/heavy-pole setup, enabling fast stabilization (192 steps) and near-zero final errors while using minimal energy.\n**Program Identifier:** Generation 58 - Patch Name optimized_q_and_multiplicative_gains - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physics-consistent linearized model for LQR control with optimized state weights and smooth tanh-based gain scheduling that adapts to angle and angular velocity errors.  \n- **Performance**: Achieved a high combined score of 4920.13, with strong stabilization (success bonus = 800), low energy use (avg 0.01/step), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains improved transient response without introducing discontinuities, while precise alignment between the controller\u2019s A matrix and simulator dynamics was critical for stability under challenging conditions (heavy, long pole; high friction).\n**Program Identifier:** Generation 59 - Patch Name optimized_lqr_smooth_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a friction-aware linearized model with discrete-time LQR and smooth tanh-based gain scheduling to adaptively adjust control effort based on angle and angular velocity errors.  \n- **Performance**: Achieved a combined score of 4872.34 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20).  \n- **Feedback**: The adaptive gains improved responsiveness to large initial angles while maintaining stability; however, the conservative Q/R tuning limited time efficiency despite low energy use.\n**Program Identifier:** Generation 60 - Patch Name discrete_lqr_with_refined_gains - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based modulation on angle and angular velocity errors to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a combined score of 4878.25 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) indicating slow convergence.  \n- **Feedback**: The conservative LQR weights and high friction tolerance enabled stability under challenging dynamics, yet the \"lazy\" control response limited time-based rewards; adaptive gains improved large-error handling without introducing discontinuities.\n**Program Identifier:** Generation 61 - Patch Name increase_angular_velocity_penalty - Correct Program: True\n\n**Program Name: Phase-Based Adaptive Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-detection mechanism to switch between four specialized control strategies (Recovery, Swing-Back, Capture, Centering), combining LQR near equilibrium with nonlinear adaptive gains and integral cart-position control. The controller dynamically blends base LQR actions with phase-specific forces based on real-time state analysis.  \n- **Performance**: Achieved a high combined score of 4739.13, with strong time, energy, and success bonuses, stabilizing the pole in 218 steps and maintaining low final errors.  \n- **Feedback**: The phase-based adaptation effectively handles the challenging dynamics of a heavy, long pole with high friction, enabling both aggressive recovery and precise stabilization. Energy efficiency and smooth phase transitions contributed significantly to the high performance.\n**Program Identifier:** Generation 62 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a discretized LQR controller with friction-aware linearization, integral action activated near upright via tanh scheduling, and adaptive gain blending based on angle/velocity error dominance.  \n- **Performance**: Achieved a combined score of 3219.32 with moderate time and energy bonuses but low stabilization ratio (0.33).  \n- **Feedback**: The conservative LQR weights and high friction tolerance led to sluggish response and poor stabilization; integral action helped reduce steady-state position error but wasn't sufficient to maintain balance under challenging initial conditions (0.9 rad).\n**Program Identifier:** Generation 63 - Patch Name add_integral_action_with_tanh_activation - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, then applies smooth tanh-based gain scheduling on angle and angular velocity to adaptively scale control effort.  \n- **Performance**: Achieves a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction); likely due to inaccurate linearization or insufficient robustness to nonlinearities and large initial angles.\n**Program Identifier:** Generation 64 - Patch Name fine_tune_q_matrix - Correct Program: False\n\n**Program Name: Adaptive Discrete-Time LQR Controller**  \n- **Implementation**: Uses a discrete-time LQR controller derived from linearized continuous dynamics with Zero-Order Hold discretization, augmented with adaptive gain scheduling based on pole angle and angular velocity. The physics simulation includes high friction, a long/heavy pole, and large initial angle for increased difficulty.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite accurate modeling and adaptive gains, the controller fails to stabilize the highly unstable system\u2014likely due to excessive nonlinearity beyond the LQR\u2019s linear approximation range and insufficient robustness to large initial deviations.\n**Program Identifier:** Generation 65 - Patch Name adaptive_lqr_with_discrete_tuning - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linearized model for LQR control with optimized Q weights and smooth tanh-based gain scheduling that adapts to pole angle and angular velocity errors.  \n- **Performance**: Achieves a combined score of 4920.14 with full success bonus, low energy use (6.45 total), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains significantly improve robustness under challenging conditions (heavy/long pole, high friction), while the accurate linearization and tuned LQR weights enable precise stabilization with minimal control effort.\n**Program Identifier:** Generation 66 - Patch Name adaptive_lqr_tuned_q - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, incorporating friction terms matching the simulator, and applies adaptive gain scheduling based on angle and angular velocity errors to boost responsiveness during large deviations.  \n- **Performance**: Achieved a high combined score of 4920.14 with fast stabilization (192 steps), low energy use (avg 0.01 per step), and perfect final error metrics.  \n- **Feedback**: The controller\u2019s precise modeling of friction and intelligent gain adaptation enabled robust handling of a challenging, heavy, and long pole while maintaining energy efficiency and rapid stabilization.\n**Program Identifier:** Generation 67 - Patch Name optimized_q_matrix_and_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linear quadratic regulator (LQR) with physically accurate system matrices and proven optimal state weights; enhances robustness via multiplicative gain scheduling using tanh-based adaptation on angle and angular velocity.  \n- **Performance**: Achieved a combined score of 4920.13, including full success bonus, low energy usage (total_energy=6.44), and fast stabilization (192 steps).  \n- **Feedback**: The exact LQR tuning and smooth gain scheduling enabled stable control despite challenging parameters (heavy/long pole, high friction); minor Q-matrix adjustment (Q[3]=3.28) improved damping without sacrificing performance.\n**Program Identifier:** Generation 68 - Patch Name energy_based_sliding_mode_control - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based blending of position and velocity gains, tailored to a challenging high-friction, long/heavy-pole environment.  \n- **Performance**: Achieved a combined score of 4893.84 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) indicating slow convergence.  \n- **Feedback**: The conservative LQR weights and adaptive gains enable stability under difficult dynamics but result in sluggish response (low time bonus relative to max), suggesting room for tuning aggressiveness without sacrificing robustness.\n**Program Identifier:** Generation 69 - Patch Name optimize_lqr_weights_and_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive Swing-Up LQR with Integral Control**  \n- **Implementation**: Combines discrete-time LQR on an extended state (including integral of position), physics-informed swing-up assistance using hyperbolic tangent switching, and soft-switched integral action that deactivates far from equilibrium. Uses accurate linearized dynamics and zero-order hold discretization.  \n- **Performance**: Achieved a combined score of 0.00 due to failure to stabilize the pendulum within limits (final theta error: 2.81 rad, final x error: 3498 m).  \n- **Feedback**: Despite aggressive swing-up and integral terms, the controller failed to stabilize the highly challenging system (heavy, long pole with high friction); large steady-state errors suggest insufficient control authority or poor tuning for this extreme configuration.\n**Program Identifier:** Generation 70 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model to compute LQR gains with optimized state weights, augmented by smooth gain scheduling via tanh and physics-informed swing-up feedforward based on angle and angular velocity.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests despite design efforts for high difficulty conditions.  \n- **Feedback**: The controller\u2019s linearization may be inaccurate for large angles (e.g., 0.9 rad initial condition), and the swing-up logic might introduce destabilizing forces; friction modeling in the linearized A matrix appears inconsistent with the nonlinear simulator dynamics.\n**Program Identifier:** Generation 71 - Patch Name additive_swing_assist_feedforward - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with gain scheduling based on pole angle and angular velocity, plus an additive swing-up term that activates for large angles to counteract falling dynamics.  \n- **Performance**: Achieved a high combined score of 4920.15 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The adaptive gains and physics-informed swing-up assist effectively handle the challenging heavy/long pole configuration, enabling robust stabilization while minimizing energy use; precise friction modeling in both simulation and controller contributes to real-world fidelity and performance.\n**Program Identifier:** Generation 72 - Patch Name additive_swing_assist_with_proven_base - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized model to compute LQR gains with suboptimal Q/R weights, enhanced by smooth tanh-based gain scheduling that increases control effort only when angle or angular velocity errors exceed thresholds.  \n- **Performance**: Achieved a combined score of 4920.14 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The conservative LQR tuning ensures stability under challenging physics (long/heavy pole, high friction), while adaptive gains improve transient response without sacrificing smoothness; however, delayed aggressive correction limits time-based rewards.\n**Program Identifier:** Generation 73 - Patch Name none - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR Controller**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation combined with hybrid gain scheduling that blends linear and nonlinear responses based on angle and angular velocity magnitude; includes robust angle normalization and force clipping.  \n- **Performance**: Achieved a high combined score of 4865.75 with strong energy efficiency, full success bonus, and excellent stabilization.  \n- **Feedback**: The adaptive gain scheduling significantly improved control responsiveness for large deviations while maintaining precision near equilibrium, enabling stable performance even under challenging conditions (heavy, long pole with high friction and large initial angle).\n**Program Identifier:** Generation 74 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**\n\n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based smooth transitions and additive physics-informed swing-up assistance for large angles. The system parameters reflect a highly unstable pendulum (long, heavy pole; high friction).\n\n- **Performance**: Achieves an initial score near 3000 but fails to stabilize consistently under validation tests, resulting in a combined score of 0.0.\n\n- **Feedback**: Despite sophisticated control logic, the controller is too \"lazy\" due to conservative LQR weights (low Q, high R), leading to insufficient correction for the challenging dynamics. Validation likely fails because the pendulum exceeds the 1.0 rad failure threshold before stabilization.\n**Program Identifier:** Generation 75 - Patch Name optimized_q_with_swing_assist - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with suboptimal gains, enhanced by physics-informed swing-up logic and adaptive gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 4724.67, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite slow stabilization (stabilization_ratio: 0.22).  \n- **Feedback**: The conservative LQR tuning prioritizes low energy use over fast response, resulting in high energy and success bonuses but modest time and base scores; the swing-up assist effectively handles large initial angles in a challenging high-friction, long-pole environment.\n**Program Identifier:** Generation 76 - Patch Name enhanced_swing_up_assist_with_adaptive_gain - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with tanh-based gain scheduling and additive swing-up control for large angles (>0.8 rad) to handle highly unstable configurations (long, heavy pole).  \n- **Performance**: Achieved a high combined score of 4920.15, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (192 steps).  \n- **Feedback**: The hybrid approach effectively balances optimal linear control near equilibrium with non-linear corrective actions during large deviations, enabling robust performance under extreme physical parameters; angle normalization and friction-aware dynamics modeling contributed to numerical stability and realism.\n**Program Identifier:** Generation 77 - Patch Name enhanced_swingup_assist_and_q_optimization - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in no functional logic or algorithm implementation.  \n- **Performance**: Combined score is 0.0, indicating complete failure to meet validation requirements.  \n- **Feedback**: The absence of any implementation means the program cannot process inputs, produce outputs, or satisfy basic correctness criteria. A valid solution must include core logic aligned with the problem specification.\n**Program Identifier:** Generation 78 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assistance**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based activation and continuous swing-up assistance for large angles, tailored to a highly unstable pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a combined score of 4920.19, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, despite slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The \"lazy\" LQR weights (low Q, high R) prioritize energy over speed, limiting time bonus but enabling robust stability; the smooth, differentiable swing-up logic effectively handles large initial deviations without abrupt control switches.\n**Program Identifier:** Generation 79 - Patch Name continuous_falling_factor_swing_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based smooth transitions and an additive swing-up term for large angles (>0.8 rad), tailored to a challenging pendulum (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 4920.22, with strong energy efficiency (avg_energy_per_step=0.01) and full success bonus, but slow stabilization (stabilization_ratio=0.19).  \n- **Feedback**: The conservative LQR weights (low Q, high R) prioritize low energy use over fast response, resulting in delayed stabilization; however, the swing-up assist effectively recovers from large initial deviations, enabling consistent success despite difficult dynamics.\n**Program Identifier:** Generation 80 - Patch Name falling_severity_swing_assist - Correct Program: True\n\n**Program Name: Adaptive Momentum Compensation Controller**  \n- **Implementation**: Uses phase-aware switching between swing-up, transition, and fine-tuning control modes with momentum compensation, energy shaping, nonlinear damping, and friction compensation; includes integral action near equilibrium and smooth force limiting.  \n- **Performance**: Achieved a combined score of 0.00 due to failure to stabilize the pendulum (final theta error: 1.81 rad, x error: 20.07 m), despite high energy usage (total_energy: 123.20).  \n- **Feedback**: The controller fails to balance the highly unstable system (long, heavy pole with high friction); aggressive swing-up terms may destabilize early dynamics, and LQR-like gains appear insufficient for stabilization under these extreme parameters.\n**Program Identifier:** Generation 81 - Patch Name adaptive_momentum_compensation_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-consistent linearized LQR controller with adaptive gain scheduling and a momentum-based swing-up assist triggered for large angles (>0.8 rad), using normalized angular momentum and smooth tanh activations for differentiable control adjustments.  \n- **Performance**: Achieved a high combined score of 4920.12, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00), stabilizing in 192 steps.  \n- **Feedback**: The swing-up assist effectively handles the challenging initial condition (0.9 rad) with a heavy, long pole, while precise friction modeling and consistent linearization between simulation and LQR design ensure robust stabilization and minimal steady-state error.\n**Program Identifier:** Generation 82 - Patch Name none - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Action**  \n- **Implementation**: Combines a physically accurate linearized LQR controller with conditional integral action (activated near equilibrium), robust angle normalization, smooth gain scheduling via tanh, and physics-informed swing-up assist for large angles.  \n- **Performance**: Achieved a high combined score of 4691.82 with full success bonus, low energy use (avg 0.01/step), and rapid stabilization (224 steps).  \n- **Feedback**: The hybrid approach effectively balances optimal linear control near equilibrium with non-linear corrections during large deviations, enabling stable control despite challenging parameters (heavy/long pole, high friction). Conditional integration prevents windup while improving steady-state cart position accuracy.\n**Program Identifier:** Generation 83 - Patch Name hybrid_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Gated Integral Control**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, augmented with a gated integral term on cart position that activates only near upright equilibrium, and applies smooth gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 1278.20, with perfect stabilization ratio (1.00) over 1000 steps but zero success bonus due to failure to meet terminal position/angle thresholds.  \n- **Feedback**: Despite excellent energy efficiency and stability maintenance, the controller fails to center the cart (final_x_error = 25.87), indicating the integral action is insufficiently aggressive or misaligned with the true control objective; the lack of success bonus suggests the final state does not satisfy evaluation criteria for \"solved\" episodes.\n**Program Identifier:** Generation 84 - Patch Name adaptive_lqr_with_integrated_correction - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model to compute LQR gains, augmented with tanh-based gain scheduling, soft integral control, and swing-up assistance for large angles. The controller adapts force output based on pole angle and angular velocity deviations.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests despite sophisticated design.  \n- **Feedback**: Overly aggressive gain scheduling and incorrect swing-up logic likely destabilize the system under high-difficulty conditions (heavy/long pole, large initial angle). The integral term and friction modeling may also introduce instability rather than robustness.\n**Program Identifier:** Generation 85 - Patch Name soft_integral_cart_centering - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with adaptive gain scheduling based on angle and angular velocity, plus swing-up assistance for large deviations. Friction terms in the dynamics are carefully matched between simulation and controller.  \n- **Performance**: Achieved a high combined score of 4920.22 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The controller excels in balancing aggressive correction for large errors with minimal energy use near equilibrium; precise modeling of friction and well-tuned LQR weights were critical to success.\n**Program Identifier:** Generation 86 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-informed linearized LQR controller with adaptive gain scheduling and phase-aware swing-up assistance based on normalized angular momentum and pole dynamics. Uses precise friction modeling, consistent linearization around upright equilibrium, and smooth exponential transitions for gain adaptation.  \n- **Performance**: Achieved a high combined score of 4635.10 with full success bonus, low energy usage (avg 0.01/step), and fast stabilization (232 steps).  \n- **Feedback**: The swing-up assist effectively handles large initial deviations (0.9 rad) despite challenging pole parameters (heavy, long, high friction), while adaptive gains improve robustness without destabilizing the LQR core. Energy efficiency and stability were maintained through careful physics-consistent design.\n**Program Identifier:** Generation 87 - Patch Name phase_aware_swing_assist - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Action**  \n- **Implementation**: Combines a physically accurate linearized LQR controller with adaptive gain scheduling, swing-up assist for large angles, and soft-switched integral action to eliminate steady-state cart position error. Uses robust angle normalization and Euler integration matching the simulator dynamics exactly.  \n- **Performance**: Achieved a combined score of 1308.26 with perfect stabilization (1000 steps), low average energy per step (0.01), and high energy efficiency (energy_bonus: 2492.28).  \n- **Feedback**: The hybrid approach effectively stabilizes an extremely challenging system (heavy, long pole with high friction and large initial angle). Integral action and swing-up logic prevent drift and aid recovery, while adaptive gains enhance responsiveness without destabilizing the system near equilibrium.\n**Program Identifier:** Generation 88 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Combines energy-shaping swing-up control with LQR-based stabilization, using phase-based switching and friction compensation; includes adaptive gains and predictive terms for improved response.  \n- **Performance**: Achieved high energy usage (2499.93) but failed to stabilize the pendulum (final theta error: 2.93 rad), resulting in zero success bonus and low combined score (0.00).  \n- **Feedback**: Despite sophisticated control logic, the controller could not stabilize the highly challenging system (heavy, long pole with high friction); likely due to inaccurate dynamics modeling or insufficient robustness in transition between control phases.\n**Program Identifier:** Generation 89 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a suboptimal LQR controller with friction-aware linearization, adaptive gain scheduling via smooth tanh-based modulation, and soft-switched integral action for cart centering.  \n- **Performance**: Achieved a combined score of 1196.58 with perfect stabilization ratio (1.00) over 1000 steps but low base accuracy due to high position errors.  \n- **Feedback**: The conservative LQR tuning (low Q, high R) prioritizes stability over precision, resulting in high energy efficiency but poor final/state error metrics; adaptive gains help maintain balance under challenging dynamics (long/heavy pole, high friction).\n**Program Identifier:** Generation 90 - Patch Name add_soft_cart_integral - Correct Program: True\n\n**Program Name: Adaptive LQR with Synergy Gain**  \n- **Implementation**: Uses a high-precision discrete-time LQR controller derived from physically consistent linearized dynamics, enhanced with smooth adaptive gains based on pole angle, angular velocity, and falling-away detection via tanh-based scheduling.  \n- **Performance**: Achieved a combined score of 4842.54 with full success bonus, low energy use (5.86 total), and fast stabilization (203 steps).  \n- **Feedback**: The synergy gain effectively prevents divergence during large-angle excursions, while accurate friction-aware linearization ensures robustness; the adaptive gains significantly improve damping without destabilizing the system.\n**Program Identifier:** Generation 91 - Patch Name discrete_time_lqr_with_tuned_gains - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linearized LQR controller derived from consistent physics equations, enhanced with smooth tanh-based gain scheduling that adaptively increases control effort for large angle or angular velocity errors.  \n- **Performance**: Achieved a combined score of 4920.13 with full success bonus, fast stabilization (192 steps), and very low energy use (total_energy=6.44).  \n- **Feedback**: The adaptive gains significantly improved stabilization speed without sacrificing energy efficiency, while precise alignment between the controller\u2019s linear model and the nonlinear simulator ensured robustness under challenging conditions (heavy, long pole; high friction; large initial angle).\n**Program Identifier:** Generation 92 - Patch Name increase_angular_velocity_weight - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with gain scheduling based on pole angle and angular velocity, plus an additive swing-up assist for large deviations (>0.8 rad) to recover from near-failure states.  \n- **Performance**: Achieved a combined score of 4920.15, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization was slow (stabilization_ratio: 0.19).  \n- **Feedback**: The adaptive gains and swing-up logic effectively prevented failure and enabled eventual stabilization despite aggressive initial conditions and challenging physics; however, conservative base LQR weights limited responsiveness, resulting in low time bonus and delayed convergence.\n**Program Identifier:** Generation 93 - Patch Name multiplicative_gain_with_swing_assist - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Energy Assist**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance and adaptive gain scheduling, using physically accurate system linearization and friction-aware dynamics.  \n- **Performance**: Achieved a high combined score of 4920.21 with strong stabilization (success_bonus: 800), low energy use (avg_energy_per_step: 0.01), and fast stabilization (stabilization_time: 192).  \n- **Feedback**: The controller effectively handles large initial angles and high system instability due to a heavy, long pole; energy-efficient swing-up logic and precise LQR tuning enable both rapid recovery and stable balance.\n**Program Identifier:** Generation 94 - Patch Name robust_swingup_lqr - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a high-precision linearized LQR controller with optimized Q weights, adaptive gain scheduling via tanh-based activation, swing-up assistance for large angles, and soft integral action on cart position with anti-windup.  \n- **Performance**: Achieved a combined score of 4913.35 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (193 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging heavy/long pole and high friction, while integral control improved steady-state positioning without causing windup during transients.\n**Program Identifier:** Generation 95 - Patch Name swing_up_assist_with_integral - Correct Program: True\n\n**Program Name: Phase-Based Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-based control architecture with smooth transitions between recovery, transition, and precision phases; integrates LQR gains computed from friction-aware linearization, adaptive swing-up assistance for large deviations, and gated integral action for cart centering.  \n- **Performance**: Achieved a high combined score of 4845.21 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (203 steps), and perfect final pole angle (0.00 rad).  \n- **Feedback**: The phase-blending strategy effectively handles the challenging dynamics of a heavy, long pole with high friction, while anti-windup and upright-gated integral action prevent instability and ensure precise steady-state positioning.\n**Program Identifier:** Generation 96 - Patch Name phase_based_controller - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control derived from physically accurate linearized dynamics with adaptive gain scheduling and a physics-informed swing-up assist for large angles, using smooth tanh-based activation and falling severity estimation.  \n- **Performance**: Achieved a high combined score of 4920.19 with full success bonus, low energy usage (total_energy=6.48), and rapid stabilization (stabilization_time=192).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the highly unstable configuration (heavy, long pole; high friction); precise angle normalization and divergence-aware assistance enabled robust recovery and stable balancing.\n**Program Identifier:** Generation 97 - Patch Name adaptive_lqr_with_swing_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model with physically accurate friction terms to compute LQR gains, augmented with tanh-based adaptive gain scheduling and soft-switched integral action on cart position.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated control design, the controller fails to stabilize the highly unstable pendulum (long, heavy pole with high friction); likely due to inaccurate linearization, poor handling of nonlinear dynamics, or inadequate robustness to large initial angles.\n**Program Identifier:** Generation 98 - Patch Name robust_arctan2_angle_norm - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a suboptimal LQR controller with gain scheduling, integral action gated near upright, and physics-informed swing-up assistance for large angles. The linearized model accounts for friction, and control gains are adaptively scaled based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 1313.02 with perfect stabilization ratio (1.0) over 1000 steps but low base accuracy due to conservative tuning.  \n- **Feedback**: High energy efficiency (avg 0.01 per step) and full time utilization indicate stable balancing, yet poor position/angle precision (final_x_error=7.98, stable_x_error=2.79) reveals intentionally lazy control from low Q/high R weights, limiting base score despite robustness.\n**Program Identifier:** Generation 99 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a suboptimal LQR controller with friction-aware linearization and smooth gain scheduling via tanh-based adaptive gains to handle large pole angles and high friction.  \n- **Performance**: Achieves a combined score of 4920.14 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite challenging physics (long, heavy pole; high friction).  \n- **Feedback**: The conservative LQR weights result in slower response (low time bonus relative to max), but adaptive gains improve stability at large errors without discontinuities, enabling consistent stabilization within 192 steps.\n**Program Identifier:** Generation 100 - Patch Name update_to_best_lqr_weights - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a friction-aware linearized model for LQR gain computation, enhanced with smooth gain scheduling via tanh-based adaptive gains, swing-up assistance for large angles, and soft-switched integral action on cart position to improve steady-state accuracy.  \n- **Performance**: Achieved a combined score of 4920.10 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization ratio was low (0.19).  \n- **Feedback**: The controller effectively balances the challenging pendulum (long, heavy pole; high friction) and achieves stability, but slow response due to conservative LQR weights limits time bonus; integral action and adaptive gains help maintain precision once stabilized.\n**Program Identifier:** Generation 101 - Patch Name integral_action_for_zero_drift - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with adaptive gain scheduling via tanh-based blending, swing-up assist for large angles, and gated integral action for position error correction.  \n- **Performance**: Achieved a combined score of 1308.25 with perfect stabilization (1000 steps, stabilization ratio 1.0) and very low energy usage (avg 0.01 per step).  \n- **Feedback**: The controller successfully stabilizes an extremely challenging inverted pendulum (long, heavy pole; high friction; large initial angle), demonstrating robustness through hybrid nonlinear adaptations while maintaining energy efficiency\u2014though it incurs a positional drift (final x_error = 7.01), indicating room for improved steady-state accuracy.\n**Program Identifier:** Generation 102 - Patch Name increase_angular_velocity_damping - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**  \n- **Implementation**: Uses a friction-aware linearized model for LQR control with adaptive gain scheduling, integral action gated by pole angle, and physics-informed swing-up assistance for large deviations.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and environmental difficulty, failing validation tests with a combined score of 0.0.  \n- **Feedback**: The controller\u2019s deliberate \"laziness\" (low Q/high R weights) causes slow response and poor precision; despite advanced features like gain scheduling and swing-up logic, it cannot stabilize the highly unstable system within required limits.\n**Program Identifier:** Generation 103 - Patch Name robust_angle_norm - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute discrete-time LQR gains, enhanced by adaptive gain scheduling based on pole angle and angular velocity, plus swing-up assistance for large deviations.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive control logic, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction), likely due to inaccuracies in linearization or insufficient robustness in the gain adaptation under extreme initial conditions.\n**Program Identifier:** Generation 104 - Patch Name discrete_lqr_zoh - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Uses a high-precision linearized LQR controller with optimized Q weights, adaptive gain scheduling via tanh-based activation, swing-up assistance for large angles, and soft integral action on cart position with anti-windup and gating near equilibrium.  \n- **Performance**: Achieved a combined score of 4912.79 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (193 steps), and full success bonus.  \n- **Feedback**: The combination of physically consistent linearization, adaptive gains, and targeted swing-up logic effectively handles the challenging heavy/long pole and high-friction dynamics; integral action improves steady-state positioning without destabilizing transients.\n**Program Identifier:** Generation 105 - Patch Name integral_action_refinement - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 106 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a friction-aware linearized model for LQR control, enhanced with smooth gain scheduling via tanh, swing-up assist for large angles, and anti-windup integral action on cart position.  \n- **Performance**: Achieved a combined score of 4920.09 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle large initial angles and prevent failure, while conservative LQR weights and integral action improve steady-state accuracy without destabilizing the system. However, the controller\u2019s sluggish response limits time-based rewards.\n**Program Identifier:** Generation 107 - Patch Name integral_anti_windup - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with friction-aware dynamics, combined with gain scheduling based on pole angle/velocity and an additive swing-up term for large deviations.  \n- **Performance**: Achieved a high combined score of 4920.20 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The adaptive gains and physics-informed swing-up assist effectively handle the challenging heavy/long pole configuration, enabling robust recovery from large initial angles while minimizing energy use.\n**Program Identifier:** Generation 108 - Patch Name none - Correct Program: True\n\n**Program Name: Phase-Predictive LQR Controller**  \n- **Implementation**: Uses a discretized LQR controller with phase-aware switching between recovery, transition, and precision modes; includes predictive correction, swing-up assistance, and integral action for steady-state accuracy.  \n- **Performance**: Achieved a high combined score of 4782.29 with strong energy efficiency, fast stabilization (212 steps), and perfect final error metrics.  \n- **Feedback**: The adaptive phase logic and predictive corrections effectively handled the challenging high-friction, long-pole dynamics, enabling robust swing-up and precise stabilization while minimizing control effort.\n**Program Identifier:** Generation 109 - Patch Name phase_predictive_lqr - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a linearized friction-aware model for LQR control with adaptive gain scheduling via tanh-based smooth switching, integral action for position regulation (with anti-windup), and physics-informed swing-up assistance for large angles.  \n- **Performance**: Achieved a combined score of 4920.10 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The controller effectively balances the challenging high-friction, long/heavy-pole system and avoids failure, but its conservative gains and delayed response limit time-based rewards despite excellent steady-state accuracy.\n**Program Identifier:** Generation 110 - Patch Name integral_action_with_soft_switching - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Gains**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, augmented with smooth tanh-based gain scheduling and soft-switched integral action for cart position.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and fails validation tests under challenging physics (long/heavy pole, high friction).  \n- **Feedback**: The controller\u2019s \"lazy\" Q/R weights and insufficient robustness to large initial angles cause poor stabilization; adaptive gains help but don\u2019t compensate for fundamental underperformance in hard conditions.\n**Program Identifier:** Generation 111 - Patch Name increase_dtheta_weight - Correct Program: False\n\n**Program Name: Energy-Phase Adaptive LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based adaptive gains, phase-specific strategies (recovery, precision, predictive damping), and integral action with anti-windup for steady-state accuracy. The controller dynamically adjusts force output based on normalized mechanical energy, angular velocity, and state magnitude to handle a highly unstable, heavy, and long inverted pendulum.  \n- **Performance**: Achieved a high combined score of 4755.01 with strong bonuses for success, time, and energy efficiency, stabilizing the system in 215 steps with near-zero final errors.  \n- **Feedback**: The adaptive gain scheduling and phase-aware logic effectively managed the challenging dynamics (high mass, length, friction), while predictive damping and integral control ensured precise stabilization without overshoot or windup.\n**Program Identifier:** Generation 112 - Patch Name energy_phase_controller - Correct Program: True\n\n**Program Name: Adaptive Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-based hybrid controller with energy-based swing-up for large angles, blended transition control, and LQR with integral action for stabilization; includes nonlinear modifications like velocity damping and control authority scaling.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization despite high energy usage (total_energy=8.22) and full simulation duration (1000 steps).  \n- **Feedback**: The controller failed to stabilize the pendulum (final_theta_error=2.77 rad), indicating ineffective mode transitions or insufficient robustness for the highly unstable system (long, heavy pole with high friction); integral windup or aggressive swing-up may have hindered convergence.\n**Program Identifier:** Generation 113 - Patch Name none - Correct Program: False\n\n**Program Name: Feedback Linearization with Adaptive LQR**  \n- **Implementation**: Combines discrete LQR control on a linearized model with exact nonlinear inverse dynamics to track a desired angular acceleration, augmented by adaptive gain scheduling and gated integral action for position regulation.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: The controller struggles with the highly unstable, high-friction, and heavy/long pole configuration; likely due to singularities in inverse dynamics near \u00b1\u03c0/2 and insufficient robustness in the feedback linearization under large initial angles (0.9 rad). The adaptive gains and integral term may not compensate adequately for model inaccuracies or aggressive nonlinearities.\n**Program Identifier:** Generation 114 - Patch Name fbl_discretelqr_integral - Correct Program: False\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with optimized Q-matrix weights, enhanced by adaptive gain scheduling and a continuous swing-up assist that activates based on angle magnitude and falling severity.  \n- **Performance**: Achieved a combined score of 4920.22 with perfect stabilization (final theta error = 0.00) and high energy efficiency (avg_energy_per_step = 0.01).  \n- **Feedback**: The controller effectively balances aggressive correction for large deviations with fine-tuned stabilization near upright, while the swing-up logic ensures recovery from highly unstable initial conditions; the exact match between simulation dynamics and LQR model was critical to success.\n**Program Identifier:** Generation 115 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, then applies adaptive gain scheduling via tanh-based scaling and adds swing-up assistance for large angles (>0.8 rad).  \n- **Performance**: Achieves low initial score (~3000) due to conservative Q/R tuning and insufficient robustness in highly unstable conditions.  \n- **Feedback**: The controller fails validation tests because the linearized LQR is inadequate for the highly nonlinear, high-friction, long-pole regime; swing-up logic lacks smooth integration and doesn\u2019t prevent early failure beyond 1.0 rad.\n**Program Identifier:** Generation 116 - Patch Name fix_duplicate_init_remove_integral - Correct Program: False\n\n**Program Name: Phase-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Uses a three-phase control strategy (Recovery, Transition, Precision) with physics-informed LQR gains, adaptive swing-up logic, and anti-windup integral action for steady-state error correction.  \n- **Performance**: Achieved a high combined score of 4760.24 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps.  \n- **Feedback**: The aggressive recovery phase effectively handles large initial angles, while the gated integral term ensures precise stabilization without windup; high friction and pole mass make this a robust solution under challenging dynamics.\n**Program Identifier:** Generation 117 - Patch Name adaptive_lqr_with_integral_correction - Correct Program: True\n\n**Program Name: Adaptive Energy-Phase Inverted Pendulum Controller**  \n- **Implementation**: Combines LQR-based linear control with energy-aware swing-up logic, phase-specific gains, integral action for precision stabilization, and predictive damping to handle a highly unstable pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a high combined score of 4755.01 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps with minimal final errors.  \n- **Feedback**: The adaptive gain scheduling and phase-aware switching effectively managed the challenging dynamics, while integral control and predictive damping significantly improved steady-state accuracy and reduced overshoot near equilibrium.\n**Program Identifier:** Generation 118 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Uses an enhanced adaptive LQR controller with integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, and a swing-up assist activated during large deviations. The dynamics include high-friction, heavy/long pole parameters for increased difficulty.  \n- **Performance**: Achieved a combined score of 4927.10 with full success bonus, low energy usage (6.46 total), and stabilization after 191 steps.  \n- **Feedback**: The integral control effectively eliminated steady-state cart error when upright, while the swing-up logic and adaptive gains enabled recovery from highly unstable initial conditions (0.9 rad). Energy efficiency and stability were maintained despite aggressive physical challenges.\n**Program Identifier:** Generation 119 - Patch Name adaptive_lqr_integral_swingup - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-informed linear quadratic regulator (LQR) with adaptive gain scheduling and a smooth, momentum-based swing-up assist for large angles; includes soft-switched integral action to eliminate steady-state cart position error.  \n- **Performance**: Achieved a high combined score of 4920.25, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and perfect upright maintenance (final_theta_error: 0.00).  \n- **Feedback**: The swing-up assist effectively handles the challenging initial condition (0.9 rad) with a heavy, long pole, while adaptive gains and integral action improve robustness and precision without destabilizing the system.\n**Program Identifier:** Generation 120 - Patch Name increase_dtheta_q_weight - Correct Program: True\n\n**Program Name: Phase-Adaptive Discrete LQR Controller**  \n- **Implementation**: Uses discrete-time LQR with zero-order hold discretization, enhanced by phase-based swing-up assistance, adaptive gain scheduling, filtered state estimation, and integral control activated only during fine-tuning.  \n- **Performance**: Achieved a combined score of 0.0 and failed validation tests.  \n- **Feedback**: The controller\u2019s complex phase logic and energy-based swing-up may be misaligned with the highly unstable system dynamics (heavy, long pole); discretization via an incorrect matrix exponential approximation likely introduced instability, and aggressive clipping or filtering may degrade responsiveness.\n**Program Identifier:** Generation 121 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physics-informed linearized model for LQR control with adaptive gain scheduling based on pole angle and velocity, plus swing-up assistance for large deviations and leaky integral action for cart position regulation.  \n- **Performance**: Achieved a high combined score of 4920.25 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The controller effectively balances aggressive stabilization for large angles with precise upright maintenance, while friction-aware modeling and smooth gain transitions prevent instability and energy waste.\n**Program Identifier:** Generation 122 - Patch Name add_swing_up_assist_and_integral_control - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state error and a physics-informed swing-up assist that activates for large pole angles, using smooth gain scheduling via tanh-based activation functions.  \n- **Performance**: Achieved a high combined score of 4920.25, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging dynamics (heavy, long pole; high friction), while integral control ensured precise cart positioning once upright\u2014demonstrating robustness and efficiency in both transient and steady-state phases.\n**Program Identifier:** Generation 123 - Patch Name Add integral control for cart position and swing-up assist for large angles - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with adaptive gain scheduling based on angle and angular velocity, plus physics-informed swing-up assistance for large deviations.  \n- **Performance**: Achieved a high combined score of 4898.88 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 195 steps).  \n- **Feedback**: The controller effectively balances aggressive correction for large errors with energy-efficient fine control near equilibrium; the swing-up assist and multiplicative gain adaptation were key to handling the challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 124 - Patch Name discrete_lqr_design - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance using predictive angle estimation, adaptive gain scheduling via tanh activation, and soft-switched integral action for steady-state error correction. The controller uses accurate linearized dynamics matching the nonlinear simulator and includes friction-aware modeling.  \n- **Performance**: Achieved a high combined score of 4872.08 with strong time, energy, and success bonuses, stabilizing the pendulum in 199 steps with minimal final errors.  \n- **Feedback**: The swing-up assist effectively handles large initial angles (0.9 rad), while adaptive gains and integral action improve robustness and precision; low average energy per step (0.01) indicates efficient control despite challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 125 - Patch Name predictive_feedforward_swing - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Energy Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart positioning; uses physically accurate linearization and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4913.06 with full success bonus, low energy use (avg 0.01/step), and fast stabilization (193 steps).  \n- **Feedback**: The controller effectively handles the challenging pole parameters (heavy, long) by intelligently switching between swing-up and stabilization modes; integral control and adaptive gains ensure precise cart centering and robustness without excessive energy expenditure.\n**Program Identifier:** Generation 126 - Patch Name add_cart_vel_cross_coupling - Correct Program: True\n\n**Program Name: Discrete LQR with Energy-Aware Swing-Up**\n\n- **Implementation**: Uses a discrete-time LQR controller derived via zero-order hold to match Euler integration dynamics, enhanced with energy-based swing-up logic and adaptive integral control that decays aggressively when the pole is far from upright. The controller applies gain scheduling based on angle and angular velocity, and only activates swing-up assistance when energy falls below 85% of the upright potential energy.\n\n- **Performance**: Achieved a high combined score of 4892.07, with strong contributions from time bonus (2206.23), energy efficiency (2493.47), and full success bonus (800.00), stabilizing in 196 steps with near-zero final errors.\n\n- **Feedback**: The energy-aware swing-up prevents unnecessary force application while effectively recovering from large initial angles; adaptive integral decay and gain scheduling significantly improve robustness under challenging conditions (heavy, long pole with high friction).\n**Program Identifier:** Generation 127 - Patch Name discrete_lqr_energy_adaptive - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action for cart position, and physics-informed swing-up assistance triggered at large angles (>0.8 rad). Friction is accurately modeled in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4933.93 with excellent energy efficiency (avg_energy_per_step: 0.01) and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up assist effectively stabilized the highly challenging pendulum (long, heavy pole; large initial angle), while integral action ensured precise long-term positioning without compromising stability.\n**Program Identifier:** Generation 128 - Patch Name none - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) stabilization around the upright position with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart position error elimination; uses physically accurate linearization and directional momentum awareness to enhance swing-up efficiency.  \n- **Performance**: Achieved a high combined score of 4927.07 with full success bonus, low energy usage (total_energy=6.50), and fast stabilization (191 steps).  \n- **Feedback**: The controller effectively balances aggressive swing-up and precise stabilization, with integral action and adaptive gains contributing to zero final errors and robustness under challenging conditions (heavy/long pole, high friction).\n**Program Identifier:** Generation 129 - Patch Name directional_momentum_integral_control - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed gain scheduling, swing-up assistance for large angles, and soft-switched integral action to eliminate steady-state cart position error. The controller uses accurate linearized dynamics matching the simulator and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4920.25 with full success bonus, low energy use (total_energy=6.47), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging pole parameters (heavy, long, high friction), while integral control ensured precise upright stabilization without windup due to leaky integration and gating.\n**Program Identifier:** Generation 130 - Patch Name enhanced_swing_up_with_integral_control - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action gated by angle/velocity thresholds, and physics-informed swing-up assistance for large deviations. Friction-aware system modeling ensures accurate controller synthesis.  \n- **Performance**: Achieved a high combined score of 4926.67 with excellent energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up assist effectively handle the challenging heavy/long pole configuration, while gated integral action prevents destabilization during transients\u2014key to balancing speed, stability, and energy use.\n**Program Identifier:** Generation 131 - Patch Name joint_integral_gate_and_tuned_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed adaptive gain scheduling, swing-up assistance for large angles, and gated integral action for zero steady-state cart position error. The controller uses robust angle normalization, smooth activation functions (tanh), and leaky integration with angle-dependent decay.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and near-perfect stabilization.  \n- **Feedback**: The enhanced swing-up logic and adaptive gains effectively handled the challenging pole parameters (longer, heavier), while the gated integral term improved long-term positioning without destabilizing the system during large swings.\n**Program Identifier:** Generation 132 - Patch Name integral_windup_prevention_with_angle_velocity_gate - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines gain-scheduled LQR with robust swing-up assist, enhanced angular damping, and gated integral action that activates only near upright equilibrium to minimize drift.  \n- **Performance**: Achieved a high combined score of 4926.67 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00).  \n- **Feedback**: The controller effectively stabilizes a highly challenging pendulum (long, heavy, high friction) by blending aggressive swing-up logic with precise integral correction, though stabilization ratio remains low (0.19), indicating delayed convergence.\n**Program Identifier:** Generation 133 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model to compute LQR gains, enhanced with adaptive gain scheduling, robust angle normalization via `arctan2`, soft-switched integral control gated by pole angle and velocity, and a swing-up assist that activates based on normalized falling severity.  \n- **Performance**: Achieved a high combined score of 4926.64, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (stabilization_time: 191).  \n- **Feedback**: The integral action effectively eliminated steady-state cart position error without destabilizing transients, while the swing-up logic and adaptive gains enabled reliable recovery from large initial angles (0.9 rad) despite challenging pole parameters (longer and heavier than standard).\n**Program Identifier:** Generation 134 - Patch Name refined_integral_gating_with_velocity - Correct Program: True\n\n**Program Name: Phase-Based Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Uses a hybrid control strategy with energy-based swing-up for large angles, smooth transition blending, and LQR with integral action near equilibrium; includes friction modeling and Euler integration.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization despite high energy usage (total_energy: 101.95) and full simulation duration (stabilization_time: 1000).  \n- **Feedback**: The controller failed to stabilize the pendulum (final_theta_error: 2.95 rad), indicating ineffective phase transitions or insufficient robustness for the challenging pole parameters (longer, heavier, high friction).\n**Program Identifier:** Generation 135 - Patch Name none - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments and whitespace.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass any validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 136 - Patch Name None - Correct Program: False\n\n**Program Name: Energy-Shaping Hybrid Controller**  \n- **Implementation**: Combines energy-based swing-up control with LQR stabilization and smooth phase transitions based on angle, velocity, and energy thresholds; includes integral action with anti-windup and centrifugal compensation.  \n- **Performance**: Achieved a combined score of 34.79 but failed to stabilize the pendulum (success_bonus: 0.00), despite maintaining full simulation duration (stabilization_ratio: 1.00).  \n- **Feedback**: The controller could not balance the pole (final_theta_error: 3.12 rad), indicating ineffective transition or stabilization logic; high friction and aggressive pole parameters likely exposed weaknesses in the LQR design or phase-switching criteria.\n**Program Identifier:** Generation 137 - Patch Name energy_shaping_hybrid_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and soft-switched integral action to eliminate steady-state cart position error, using gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and perfect final pole alignment (final_theta_error: 0.00).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging high-mass, long-pole configuration, while integral control with angle/velocity gating prevented windup and improved precision without destabilizing the system.\n**Program Identifier:** Generation 138 - Patch Name enhanced_integral_suppression - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up**\n\n- **Implementation**: Uses an enhanced adaptive LQR controller featuring integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, and a swing-up assist activated during large deviations. Friction and nonlinear dynamics are explicitly modeled in simulation.\n\n- **Performance**: Achieved a high combined score of 4926.69 with full success bonus, low energy usage (total_energy=6.50), and perfect final stabilization (final_theta_error=0.00).\n\n- **Feedback**: The soft-switched integral control effectively eliminated steady-state cart drift without destabilizing transients, while the normalized swing-up logic provided reliable recovery from extreme angles. Optimized angular velocity damping (Q[3]=3.28) reduced oscillations and contributed to early stabilization (stabilization_time=191).\n**Program Identifier:** Generation 139 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up Assist**  \n- **Implementation**: Uses an enhanced adaptive Linear Quadratic Regulator (LQR) controller featuring integral action for steady-state cart position correction, robust angle normalization via `arctan2`, gain scheduling based on pole angle and angular velocity, and energy-aware swing-up assistance triggered when the pole is far from upright.  \n- **Performance**: Achieved a high combined score of 4906.68, with strong contributions from time bonus (2220.04), energy efficiency (2492.62), and full success bonus (800.00), stabilizing in 194 steps with near-zero final errors.  \n- **Feedback**: The integral control effectively eliminated steady-state cart drift without destabilizing the system due to intelligent gating by angle and velocity; the swing-up logic efficiently managed the challenging initial condition (0.9 rad) with a heavy, long pole, while adaptive gains improved phase-specific responsiveness.\n**Program Identifier:** Generation 140 - Patch Name energy_aware_swingup_assist - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in a complete lack of functionality for any intended algorithm or task.  \n- **Performance**: Combined score is 0.0, indicating failure to pass any validation tests.  \n- **Feedback**: An empty implementation cannot process inputs or produce outputs, leading to total failure in evaluation; actual logic must be implemented to address the problem requirements.\n**Program Identifier:** Generation 141 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced with adaptive gain scheduling, predictive correction using angular acceleration, physics-informed swing-up assistance for large angles, and gated integral control for cart position.  \n- **Performance**: Achieved a high combined score of 4787.89 with strong time, energy, and success bonuses, stabilizing in 211 steps with near-zero final errors.  \n- **Feedback**: The controller effectively balances aggressive early-phase control with energy-efficient stabilization; the swing-up assist and adaptive gains are critical for handling the challenging pole parameters (heavy, long, high friction), while integral action with gating prevents windup and improves steady-state accuracy.\n**Program Identifier:** Generation 142 - Patch Name aggressive_early_phase_control - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines gain-scheduled LQR with robust integral action, enhanced angular damping, and physics-informed swing-up assistance; uses precise linearized dynamics matching the simulator\u2019s friction model and adaptive gains based on pole angle/velocity.  \n- **Performance**: Achieved a high combined score of 4926.94 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large angles with fine stabilization near upright, while leaky integral action and soft-switched damping reduce overshoot and improve cart centering without destabilizing the pole.\n**Program Identifier:** Generation 143 - Patch Name optimized_integral_and_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and integral action for cart position regulation, using gain scheduling and energy-based scaling to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a high combined score of 4907.13 with full success bonus, low energy usage, and stable final state.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large deviations with precise LQR stabilization near upright, while integral action eliminates steady-state position error without windup due to soft gating and leaky integration.\n**Program Identifier:** Generation 144 - Patch Name energy_aware_swing_up_assist - Correct Program: True\n\n**Program Name: Phase-Based Adaptive Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-switching strategy with four LQR controllers (swing-up, transition, balancing, fine-tune), energy shaping for swing-up, integral action with windup protection, predictive compensation, and nonlinear damping.  \n- **Performance**: Combined score of 0.0; fails to stabilize the pendulum under challenging initial conditions and system parameters.  \n- **Feedback**: Despite sophisticated control logic, the controller struggles with the highly unstable configuration (long, heavy pole; high friction; large initial angle), likely due to inadequate swing-up energy injection or poor phase transition timing.\n**Program Identifier:** Generation 145 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with adaptive gain scheduling based on angle and angular velocity, enhanced by soft-switched integral action and energy-aware swing-up assistance.  \n- **Performance**: Achieved a high combined score of 4907.12 with strong time, energy, and success bonuses, stabilizing in 194 steps with minimal final errors.  \n- **Feedback**: The controller effectively balances aggressive stabilization for large deviations with energy efficiency near equilibrium; the inclusion of friction in the linearized model and intelligent gain adaptation were key to handling the challenging heavy/long pole configuration.\n**Program Identifier:** Generation 146 - Patch Name sharpen_gain_transitions - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, robust swing-up assistance based on mechanical energy deficit, and gated integral action for cart position stabilization. The controller uses a physically accurate linearized model and includes enhanced damping near equilibrium.  \n- **Performance**: Achieved a high combined score of 4926.90, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 191), and full success bonus.  \n- **Feedback**: The hybrid approach effectively handles the challenging dynamics of a heavy, long pole with high friction, demonstrating that adaptive gains and energy-based swing-up significantly improve robustness and stabilization speed, while leaky integral action prevents windup without degrading performance.\n**Program Identifier:** Generation 147 - Patch Name energy_based_swing_up - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, robust swing-up assistance based on falling severity, and soft-switched integral action for cart position regulation. The controller uses a physically accurate linearized model and dynamically adjusts gains based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4933.93, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (190 steps), and full success bonus.  \n- **Feedback**: The hybrid approach effectively stabilizes a highly challenging inverted pendulum (long, heavy pole; high friction; large initial angle). Integral action and swing-up logic are key to maintaining stability and achieving near-zero final errors, while adaptive gains prevent overreaction during large deviations.\n**Program Identifier:** Generation 148 - Patch Name simplified_integral_gating_with_optimal_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control derived from physically accurate linearized dynamics with adaptive gain scheduling, swing-up assistance based on normalized angular momentum, and dual-gated integral action that activates only near upright and low-velocity conditions.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192 steps), and full success bonus.  \n- **Feedback**: The hybrid approach effectively handles the challenging pole parameters (heavy, long, high friction), with adaptive gains and swing-up logic enabling robust recovery from large initial angles, while gated integral control prevents windup and improves steady-state accuracy.\n**Program Identifier:** Generation 149 - Patch Name adaptive_lqr_integral_swingup_enhanced - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and gated integral action to eliminate steady-state cart position error; uses adaptive gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and full success bonus.  \n- **Feedback**: The enhanced swing-up logic effectively handles the challenging initial conditions (heavy, long pole with large initial angle), while the soft-gated integral term prevents windup and improves final positioning without destabilizing the system.\n**Program Identifier:** Generation 150 - Patch Name adaptive_energy_aware_swingup - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Uses a refined LQR controller with gain scheduling, angle-only integral gating, and swing-up assistance for large deviations; includes physically accurate linearization and friction-aware dynamics.  \n- **Performance**: Achieved a high combined score of 4920.84 with full success bonus, low energy use (6.58 total), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains and simplified integral control effectively balanced stability and efficiency, while the swing-up logic enabled recovery from large initial angles (0.9 rad); tight anti-windup and decay prevented integral windup without sacrificing responsiveness.\n**Program Identifier:** Generation 151 - Patch Name refined_adaptive_lqr_optimized - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control and Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with gain scheduling, integral action for cart position regulation, and physics-informed swing-up assistance triggered by pole angle and falling dynamics. Uses accurate friction modeling and Euler integration for simulation.  \n- **Performance**: Achieved a high combined score of 4934.54 with full success bonus, low energy use (6.61 total), and fast stabilization (190 steps).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle large initial angles and system instability from heavy/long poles, while integral control ensures precise long-term positioning without windup due to velocity-gated activation.\n**Program Identifier:** Generation 152 - Patch Name sharper_gain_transitions_and_velocity_gated_integral - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Integral Action**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart position stabilization; uses physically accurate dynamics and directional momentum awareness to enhance recovery during large-angle excursions.  \n- **Performance**: Achieved a high combined score of 4933.59 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 190 steps), and perfect final state accuracy.  \n- **Feedback**: The controller effectively balances aggressive swing-up and precise stabilization, with integral action eliminating steady-state cart error without windup; directional momentum logic prevents overshoot during recovery, contributing to robustness under challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 153 - Patch Name enhanced_integral_control_with_dynamic_gating - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state cart position error, adaptive gain scheduling via tanh-based angle/velocity thresholds, and physics-informed swing-up assist using energy deficit and angular momentum cues.  \n- **Performance**: Achieved a high combined score of 4926.95 with strong stability (final theta error \u2248 0), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The integration of swing-up logic enabled recovery from large initial angles (0.9 rad), while adaptive gains and integral control ensured precise stabilization; energy-efficient actuation and leaky integration prevented windup and maintained robustness under high-friction, heavy-pole conditions.\n**Program Identifier:** Generation 154 - Patch Name energy_scaled_swing_up_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state error and a physics-informed swing-up assist that activates for large pole angles (>0.8 rad), using smooth tanh-based gain scheduling and dual-gated integral feedback.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00).  \n- **Feedback**: The adaptive gains and swing-up logic enabled rapid recovery from large initial angles (0.9 rad), while integral control ensured precise stabilization; the design effectively balanced aggressive correction with energy conservation under high-friction, unstable dynamics.\n**Program Identifier:** Generation 155 - Patch Name enhanced_integral_control_with_velocity_gating_and_adaptive_decay - Correct Program: True\n\n**Program Name: Phase-Aware Adaptive LQR Controller**  \n- **Implementation**: Uses three phase-specific LQR controllers (swing-up, transition, stabilize) combined with energy-based swing-up logic and adaptive integral action with anti-windup. Angle normalization and momentum-aware control enhance robustness under high-friction, heavy-pole conditions.  \n- **Performance**: Achieved a combined score of 4478.10 with full success bonus, low final errors, and efficient energy use (avg 0.01 per step).  \n- **Feedback**: The phase-adaptive strategy effectively handles the challenging dynamics (long, heavy pole; high friction), enabling rapid stabilization (253 steps) and maintaining balance with minimal energy expenditure. Integral action and smooth phase transitions prevent windup and ensure precision.\n**Program Identifier:** Generation 156 - Patch Name none - Correct Program: True\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with cross-coupled state weighting, adaptive gain scheduling, energy-based swing-up assistance for large angles, and enhanced integral action with angle/velocity gating and decay.  \n- **Performance**: Achieved a high combined score of 4953.84, with strong stabilization (final theta error \u2248 0), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The energy-aware swing-up and adaptive gains effectively handled the challenging heavy/long pole, while integral control with tight gating improved cart centering without destabilizing the system near equilibrium.\n**Program Identifier:** Generation 157 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Energy-Based Swing-Up**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, dual-gated integral action for cart position, and energy-based swing-up assistance triggered by large pole angles. The controller uses physically accurate linearization and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4947.50, with strong time, energy, and success bonuses, stabilizing the pendulum in 188 steps despite extreme parameters (heavy, long pole; high friction).  \n- **Feedback**: The energy-based swing-up and adaptive gains effectively handled large initial deviations, while dual-gated integral control improved steady-state cart positioning without destabilizing the pole. Evaluation confirms robustness and efficiency under challenging dynamics.\n**Program Identifier:** Generation 158 - Patch Name adaptive_lqr_integral_v2 - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in a complete lack of functionality for any intended algorithm or task.  \n- **Performance**: Combined score is 0.0, indicating failure to pass any validation tests.  \n- **Feedback**: An empty implementation cannot perform computation or meet problem requirements; adding core logic is essential for correctness and scoring.\n**Program Identifier:** Generation 159 - Patch Name energy_coupled_lqr - Correct Program: False\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, predictive falling severity estimation, and enhanced integral action with state-dependent gating and decay. The controller uses a physically accurate linearized model and cross-coupled Q-matrix terms for coordinated cart-pole dynamics.  \n- **Performance**: Achieved a high combined score of 4953.84, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 187 steps), and full success bonus.  \n- **Feedback**: The energy-aware swing-up logic and adaptive gains effectively handled large initial angles and system instability from the heavy, long pole. Integral control with tight velocity/angle gating prevented windup while improving steady-state cart positioning, contributing to low final errors and high time/energy bonuses.\n**Program Identifier:** Generation 160 - Patch Name enhanced_swingup_with_momentum_prediction - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines gain-scheduled LQR control with adaptive integral action, enhanced angular damping, and physics-informed swing-up assistance to stabilize a challenging inverted pendulum (heavy, long pole with high friction).  \n- **Performance**: Achieved a high combined score of 4933.93, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00), stabilizing in 190 steps.  \n- **Feedback**: The adaptive gains and integral clamping effectively handled large initial angles and high system instability, while the swing-up logic provided critical recovery during aggressive falls\u2014key to robust performance under extreme parameters.\n**Program Identifier:** Generation 161 - Patch Name adaptive_integral_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines physically accurate linearized dynamics with gain-scheduled LQR, swing-up assistance for large angles, and gated integral action with anti-windup to handle a challenging high-friction, long/heavy-pole inverted pendulum.  \n- **Performance**: Achieved a high combined score of 4920.84 with full success bonus, low energy use (6.58 total), and fast stabilization (192 steps).  \n- **Feedback**: The sharpened gain scheduling and angle-gated integral control effectively balanced aggressive swing-up and precise stabilization, while the tuned LQR weights (notably Q[3]=3.2) and friction-aware modeling were critical to robustness under difficult physical parameters.\n**Program Identifier:** Generation 162 - Patch Name adaptive_integral_lqr_hybrid - Correct Program: True\n\n**Program Name: Adaptive Energy-Aware LQR with Swing-Up Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, directional momentum-aware swing-up logic, and soft-switched integral action for cart position, all tuned for extreme pendulum parameters (heavy, long pole; high friction).  \n- **Performance**: Achieved a high combined score of 4927.07, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and stable final state.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large deviations with precise LQR stabilization near upright, while integral action eliminates steady-state position error without causing overshoot\u2014key to handling the challenging system dynamics.\n**Program Identifier:** Generation 163 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Energy-Based Swing-Up**\n\n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, dual-gated integral action for cart position, predictive momentum compensation, and energy-based swing-up assistance tailored to extreme pendulum parameters (heavy, long pole with high friction).\n\n- **Performance**: Achieved a high combined score of 4947.47, with strong bonuses for success (800), time (2261), and energy efficiency (2492), stabilizing in 188 steps with near-zero final errors.\n\n- **Feedback**: The controller effectively handles challenging initial conditions and system dynamics through intelligent nonlinear enhancements; the energy-based swing-up and adaptive gains were critical for rapid stabilization and robustness under high friction and instability.\n**Program Identifier:** Generation 164 - Patch Name predictive_momentum_compensation - Correct Program: True\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance, predictive falling severity estimation, and adaptive integral action that activates only near equilibrium; includes cross-coupled terms in the Q-matrix and state-dependent gain scheduling for robustness.  \n- **Performance**: Achieved a high combined score of 4953.83 with strong time, energy, and success bonuses, stabilizing the pendulum in 187 steps with minimal final errors.  \n- **Feedback**: The controller effectively handles the challenging pole parameters (longer, heavier) by blending aggressive swing-up logic with precise stabilization; integral control and predictive momentum compensation significantly improve centering and robustness without excessive energy use.\n**Program Identifier:** Generation 165 - Patch Name predictive_momentum_compensation - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action gated by pole angle/velocity, and energy-based swing-up assistance for large deviations. Uses physically accurate friction modeling in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4926.68 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle the challenging heavy/long pole configuration, while gated integral action prevents destabilization during transients\u2014key to balancing speed and stability.\n**Program Identifier:** Generation 166 - Patch Name enhanced_swing_up_with_energy_awareness - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines physically accurate linearized dynamics with LQR control (Q[3]=3.2), gain scheduling based on angle/velocity thresholds, energy-aware swing-up logic, and gated integral action with anti-windup for cart centering.  \n- **Performance**: Achieved a high combined score of 4920.85 with full success bonus, low energy use (avg 0.01/step), and rapid stabilization (192 steps).  \n- **Feedback**: The sharpened gain scheduling and energy-aware swing-up effectively handled the challenging heavy/long pole, while the simplified integral gating proved robust for steady-state accuracy without windup issues.\n**Program Identifier:** Generation 167 - Patch Name enhanced_swing_up_with_energy_awareness - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control and Swing-Up**\n\n- **Implementation**: Uses an enhanced adaptive LQR controller with integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, swing-up assistance triggered at large angles, and mid-swing angular damping. The dynamics include high-friction terms and a heavy, long pole to increase difficulty.\n\n- **Performance**: Achieved a combined score of 0.00 despite a base score of 7.16, failing to stabilize the pendulum (final theta error: 2.94 rad, final x error: 4432 m) over 1000 steps.\n\n- **Feedback**: The controller fails to stabilize the system due to excessive drift in cart position and inability to recover from large initial angles; integral action may be too weak or improperly gated, and swing-up logic might interfere with stabilization rather than assist it.\n**Program Identifier:** Generation 168 - Patch Name sharper_gain_scheduling - Correct Program: False\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines LQR with velocity-gated integral action, gentle gain scheduling, and swing-up assistance for large angles; uses physically accurate linearized dynamics and proven Q/R weights.  \n- **Performance**: Achieved a high combined score of 4954.47 with full success bonus, low energy use (6.51 total), and fast stabilization (187 steps).  \n- **Feedback**: The hybrid approach effectively balances aggressive swing-up for large deviations with precise LQR control near upright, while velocity-gated integration prevents windup and improves stability.\n**Program Identifier:** Generation 169 - Patch Name adaptive_integral_lqr_hybrid - Correct Program: True",
        "meta_scratch_pad": "## Successful Algorithmic Patterns\n\n- **Velocity-gated integral action with dual angle/velocity thresholds enables precise cart centering without destabilization**: The current best program (*Hybrid Adaptive Integral LQR Controller*, **4954.47**) implements a refined gating mechanism using `angle_gate = tanh(12.0 * (0.1 - abs(theta)))` and `velocity_gate = tanh(8.0 * (1.0 - abs(dtheta)))`, multiplying them to form an integral gate. This approach\u2014also seen in Generation 165 (**4953.83**) with even tighter velocity thresholds (`tanh(10.0 * (0.8 - abs(dtheta)))`)\u2014ensures integral action activates only near equilibrium and low velocity, yielding exceptional steady-state accuracy: **final_x_error: 0.00** and **stable_x_error: 0.04**, surpassing earlier variants like Generation 166 (**4926.68**) which used simpler gating.\n\n- **Energy-aware swing-up with normalized falling severity enhances recovery from large angles while minimizing energy waste**: All top-performing programs (Generations 165\u2013167, 169) incorporate physics-informed swing-up logic that scales assistance based on angular momentum normalized by natural frequency (`theta * dtheta / (L_COM * omega_n)`). The current best uses this to compute a `falling_factor = 1.0 + tanh(2.0 * ...)`, allowing directional compensation during mid-swing. This contributes to its fastest **stabilization_time: 187 steps**\u2014matching Generation 165\u2014and supports elite **energy_bonus: 2492.75**, confirming that momentum-aware feedforward is critical for heavy/long poles.\n\n- **Gentle, continuous gain scheduling via tanh-based activation preserves stability while maintaining responsiveness**: Unlike aggressive or discrete switching, the current best employs smooth adaptive gains: `pos_gain = 1.0 + 0.5 * tanh(5.0 * max(0, abs(theta) - 0.6))` and `vel_gain = 1.0 + 0.3 * tanh(4.0 * max(0, abs(dtheta) - 1.0))`. This \"gentler\" scheduling\u2014explicitly cited as crossover-inspired\u2014avoids overcorrection near upright while still amplifying control effort during large deviations, directly supporting its record **time_bonus: 2267.91**, tied with Generation 165 but achieved with simpler tuning than cross-coupled Q-matrices.\n\n- **Proven LQR weights (Q[3]=3.2) combined with physically accurate linearized dynamics ensure zero final angle error under high friction**: The current best reuses the validated Q-matrix `diag([4.5, 44.0, 0.6, 3.2])` and constructs an A-matrix that explicitly models joint friction (`A[3,3] = -b_j / (m*l*denom0)`), matching the structure used in all correct programs. This fidelity enables perfect stabilization (**final_theta_error: 0.00**) despite **FRICTION_JOINT = 0.25**, demonstrating that model accuracy\u2014not just feedback gains\u2014is essential for handling extreme physical parameters.\n\n## Ineffective Approaches\n\n- **Improperly gated or weak integral action leads to catastrophic drift and total failure**: Generation 168 (*Adaptive LQR with Integral Control and Swing-Up*) scored **0.00** due to excessive cart drift (**final_x_error: 4432 m**) despite having integral components. Evaluation feedback suggests its integral action was either too weak or poorly gated, failing to activate effectively during stabilization. This contrasts sharply with the tightly controlled gating in successful programs and reaffirms that unrefined integral implementation is worse than none in high-inertia regimes.\n\n- **Swing-up logic that interferes with stabilization rather than assisting it causes instability**: In Generation 168, the swing-up mechanism may have remained active or misfired during the transition to upright control, preventing convergence. Unlike the clean separation in top performers\u2014where swing-up deactivates smoothly via `tanh(6.0 * (abs(theta) - 0.8))`\u2014this program\u2019s failure indicates that non-adaptive or poorly phased swing-up can actively disrupt LQR stabilization, especially when combined with inadequate integral correction.\n\n- **Absence of state-dependent gain modulation reduces robustness under challenging dynamics**: Although not directly tested here, the consistent use of adaptive gains across all correct programs (vs. fixed gains in historical failures like Generation 156) implies that static LQR controllers cannot handle the combined stress of long/heavy poles and high friction. Generation 168\u2019s collapse further supports this: even with nominal adaptive elements, insufficient gain scheduling granularity leads to loss of control.\n\n## Implementation Insights\n\n- **Dual force clipping ensures actuator constraint compliance and simulation robustness**: The current best clips force both inside `get_control_action` and in the simulation loop (`np.clip(force, -100.0, 100.0)`). This redundancy safeguards against numerical overflow during aggressive swing-up (e.g., `u_swing = 8.0 * ...`) and guarantees adherence to physical limits, directly enabling its full **success_bonus: 800.00**\u2014a pattern mirrored in all correct programs but absent in failed implementations.\n\n- **Angle normalization via modulo arithmetic prevents discontinuities in control logic**: The current best uses `theta = ((theta + np.pi) % (2 * np.pi)) - np.pi` to wrap pole angle into [-\u03c0, \u03c0], ensuring consistent interpretation of `abs(theta)` in swing-up and gating conditions. This simple but critical step avoids erroneous triggering of swing-up at \u00b1\u03c0 boundaries, contributing to reliable behavior across all initial conditions\u2014a detail consistently present in high-scoring controllers.\n\n- **Natural frequency normalization (`omega_n = sqrt(G / L_COM)`) makes swing-up intensity scalable to actual pendulum dynamics**: By normalizing angular momentum with the system\u2019s intrinsic timescale, the current best adapts swing-up response to the true physical regime of the longer, heavier pole. This explains its ability to match the energy efficiency (**avg_energy_per_step: 0.01**) of predecessors while stabilizing faster, demonstrating that dimensionless quantities improve generalization across parameter variations.\n\n- **Integral decay during inactive phases prevents windup without losing readiness**: The current best decays the integral term by `*= 0.95` when the gate is closed (`integral_gate <= 0.1`), a technique also used in Generation 167. This balances anti-windup protection with retention of accumulated error history, enabling rapid reactivation when returning to equilibrium\u2014critical for achieving **stable_x_error: 0.04** under persistent disturbances like high cart friction.\n\n## Performance Analysis\n\n- **The current best program (**4954.47**) achieves parity with Generation 165 (**4953.83**) in time bonus (**2267.91**) but surpasses it slightly in total score**, despite lacking cross-coupled Q-matrix terms. This indicates that **velocity-gated integral control and gentle gain scheduling can compensate for the absence of off-diagonal cost coupling**, offering a simpler yet equally effective path to top performance\u2014challenging the prior assumption that cross-coupling was indispensable.\n\n- **Stabilization time remains the dominant differentiator among elite controllers**: The current best and Generation 165 both stabilize in **187 steps**, the fastest recorded, earning them **+6 to +20 points** in time bonus over peers like Generation 166 (**191 steps**, **4926.68**) and Generation 167 (**192 steps**, **4920.85**). This 3\u20135 step advantage accounts for nearly all score separation above **4920**, confirming that swing-up efficiency\u2014not energy use\u2014is now the key competitive frontier.\n\n- **Energy efficiency has fully saturated across all correct implementations**: All four successful programs (Generations 165\u2013167, 169) achieve **total_energy \u2248 6.5\u20136.6** and **avg_energy_per_step: 0.01**, indicating that under these physical constraints (heavy pole, high friction), further energy reductions are impossible without sacrificing stability. Consequently, the scoring hierarchy is now determined entirely by **time_bonus** and **x-error metrics**, where the current best excels through synergistic control blending.\n\n- **Full success bonus (**800.00**) is universal among correct programs but meaningless for ranking**: Every program scoring above **4920** earns the maximum success bonus, proving robustness against the test case (0.9 rad initial angle, heavy/long pole). However, scores vary by **>33 points** solely due to transient speed and steady-state precision, reinforcing that post-stabilization performance\u2014not mere success\u2014is what defines elite controllers in this evaluation regime.",
        "embed_cost": 0.0,
        "novelty_cost": 0.0,
        "stdout_log": "Evaluating program: results_pendulum_liter/gen_175/main.py\nSaving results to: results_pendulum_liter/gen_175/results\nRun 1/1 completed in 0.04 seconds\nCorrectness and error status saved to results_pendulum_liter/gen_175/results/correct.json\nMetrics saved to results_pendulum_liter/gen_175/results/metrics.json\nEvaluation or Validation failed: Validation failed: Cart moved out of bounds (>5.0m)\nMetrics:\n  combined_score: 0.0\n  public: {'score': 0.0, 'base_score': 10.939392272031206, 'time_bonus': 1.0063878837075355, 'energy_bonus': 3.759735647256262e-07, 'success_bonus': 0.0, 'stabilization_time': 1000, 'stabilization_ratio': 1.0, 'avg_energy_per_step': 0.9458864610649634, 'total_energy': 945.8864610649634, 'mean_step_reward': 0.010939392272031206, 'final_theta_error': 3.112949117141774, 'final_x_error': 4179.886374061643, 'stable_theta_error': 2.079842915719763, 'stable_x_error': 1626.7272611727374}\n  private: {'oscillation_penalty': -4.0, 'angle_oscillation_penalty': -464.0, 'boundary_penalty': -2000.0, 'unstable_penalty': -500.0, 'max_theta': 3.14093187729042, 'max_x': 4179.886374061643, 'max_force': 100.0, 'force_std': 30.696044483138973}\n  execution_time_mean: 0.043036176000896376\n  execution_time_std: 0.0\n  num_valid_runs: 0\n  num_invalid_runs: 1\n  all_validation_errors: ['Cart moved out of bounds (>5.0m)']\n",
        "stderr_log": "",
        "code_analysis_metrics": {
          "cyclomatic_complexity": 15,
          "average_cyclomatic_complexity": 2.142857142857143,
          "halstead_volume": 4403.2671565494575,
          "halstead_difficulty": 8.537549407114625,
          "halstead_effort": 37593.11090176612,
          "lines_of_code": 252,
          "logical_lines_of_code": 138,
          "comments": 49,
          "maintainability_index": -24.62459980978271,
          "max_nesting_depth": 2,
          "complexity_score": 1.0
        }
      },
      "in_archive": false
    },
    {
      "id": "a596f505-6e55-4ef9-b014-d1595f72d5b5",
      "code": "",
      "language": "python",
      "parent_id": "ce1336dc-62df-48df-8593-ee6b652da3bf",
      "archive_inspiration_ids": [
        "cf208ebc-01a7-4a3b-8051-e3fbe4c0dccf",
        "c1c49a4d-70c9-4bed-8939-4d8b31aacd11",
        "0d6b42f5-ea5f-4650-b793-c760cafd0ce7",
        "460e902f-154e-48a4-8eb5-ec6c7a20e2ff"
      ],
      "top_k_inspiration_ids": [
        "738b86a0-a93c-489b-af98-690c689ef5a2",
        "d37fa0dd-a945-435f-9512-1e41054e7d5b",
        "fba93c49-b8aa-4513-b1f1-1f72fc863346"
      ],
      "island_idx": 1,
      "generation": 176,
      "timestamp": 1764096589.6290276,
      "code_diff": null,
      "combined_score": 0.0,
      "public_metrics": {},
      "private_metrics": {},
      "text_feedback": "",
      "correct": false,
      "children_count": 0,
      "complexity": 0.0,
      "embedding": [],
      "embedding_pca_2d": [],
      "embedding_pca_3d": [],
      "embedding_cluster_id": null,
      "migration_history": [],
      "metadata": {
        "compute_time": 127.91508746147156,
        "patch_type": "full",
        "api_costs": 0,
        "num_applied": 0,
        "patch_name": null,
        "patch_description": null,
        "error_attempt": "LLM response content was None.",
        "novelty_attempt": 1,
        "resample_attempt": 3,
        "patch_attempt": 2,
        "model_name": "glm-4.6",
        "temperature": 0.5,
        "max_output_tokens": 4096,
        "llm_result": null,
        "diff_summary": {},
        "meta_recommendations": "1. **Enhance predictive momentum compensation with joint-friction-aware angular acceleration estimation**: Replace the current simplified `theta_acc_est` with a full friction-inclusive model that accounts for both cart and joint friction terms in the denominator, mirroring the precision of the A-matrix used in LQR design. This would align predictive correction with the high-fidelity dynamics already proven essential for zero final error under `FRICTION_JOINT=0.25`, potentially enabling even earlier divergence counteraction and shaving 2\u20133 steps off the 178-step stabilization time.\n\n2. **Introduce state-dependent mid-swing damping gain modulated by predicted energy deficit**: Scale the `K_d_midswing = 2.5` coefficient using the normalized mechanical energy: `K_d = 2.5 * (1.0 + 0.4 * max(0, 1 - E_current / E_max))`, where `E_max = M_POLE*G*L_COM`. This leverages the Gaussian damping\u2019s phase specificity while making its intensity responsive to swing-up progress\u2014applying stronger damping when the pole is under-energized and prone to stall, which could reduce overshoot variability across extreme initial conditions without compromising balancing precision.\n\n3. **Make cross-coupling boost adaptive to both angle and angular velocity magnitude**: Extend the current `cross_coupling_boost = 1.0 + 0.5 * tanh(3.0 * abs(theta))` to include velocity dependence: `boost = 1.0 + 0.5 * tanh(3.0 * abs(theta)) * (1.0 + 0.3 * tanh(2.0 * abs(dtheta)))`. Since high `dtheta` during mid-swing exacerbates coordination challenges, this dual-state adaptation would intensify theta-dtheta coupling precisely when momentum is highest, improving transient response during the critical 0.3\u20130.8 rad transition zone identified as pivotal for record time_bonus.\n\n4. **Implement exponential decay on the integral state only when outside both gating thresholds**, rather than applying decay whenever the gate product is low. Specifically, decay `self.integral_x *= 0.95` only if `abs(theta) > 0.12 OR abs(dtheta) > 1.1`, preserving accumulated correction during brief excursions beyond equilibrium. This refines the current dual-gating strategy to better handle high-inertia recovery transients, potentially reducing `stable_x_error` below 0.05 while maintaining windup protection\u2014addressing the last remaining precision gap among elite controllers.\n\n5. **Replace the fixed Gaussian center (0.5 rad) in mid-swing damping with an inertia-scaled peak location**: Set the activation center as `theta_peak = 0.5 * np.sqrt(L_COM / 2.5)` so the damping maximum adapts to the actual pole length, consistent with the `omega_n` normalization already used in predictive logic. Given that longer poles exhibit delayed momentum peaks, this ensures damping aligns with the true high-risk region of the swing-up trajectory, preventing premature or late intervention that could otherwise waste energy or permit overshoot under extreme `L_POLE=2.5` conditions.",
        "meta_summary": "**Program Name: Suboptimal LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with conservative LQR gains (low Q, high R) to stabilize a challenging inverted pendulum system with heavy/long pole and high friction; control actions are clipped to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 3195.17, with strong energy efficiency but slow stabilization (459 steps, 46% ratio).  \n- **Feedback**: The overly conservative LQR tuning results in sluggish response and poor error correction early on, limiting time and precision bonuses despite eventual stability and low energy use.\n**Program Identifier:** Generation 0 - Patch Name initial_program - Correct Program: False\n\n**Program Name: Suboptimal LQR with Nonlinear Compensation**  \n- **Implementation**: Uses a linearized LQR controller with aggressive Q/R weights combined with gravity and damping compensation terms to stabilize a highly unstable inverted pendulum (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 0.00 due to large final position/angle errors despite running all 1000 steps.  \n- **Feedback**: The controller fails to stabilize the system\u2014final cart position diverges drastically (>4000 m) and pole angle remains far from upright (~2.94 rad), indicating poor control authority or incorrect linearization assumptions under large deviations.\n**Program Identifier:** Generation 1 - Patch Name tune_lqr_nonlinear_comp - Correct Program: False\n\n**Program Name: Sliding Mode Energy Controller**\n\n- **Implementation**: Combines energy-based swing-up with sliding mode stabilization, featuring adaptive gain scheduling, friction compensation, and smooth sign approximation to reduce chattering; uses hybrid control modes based on pole angle magnitude.\n\n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilizing the pendulum (final theta error = 2.52 rad), despite moderate energy efficiency (avg_energy_per_step = 0.35) and full episode length (stabilization_time = 1000).\n\n- **Feedback**: The controller fails to stabilize the pendulum within the required bounds (|theta| \u2264 1.0 rad), indicating insufficient robustness or incorrect tuning for the highly challenging system parameters (heavy, long pole with high friction); energy shaping may dominate over stabilization near upright, preventing convergence.\n**Program Identifier:** Generation 2 - Patch Name sliding_mode_energy_shaping_hybrid - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized continuous-time LQR controller with aggressive Q/R weights on a highly unstable pendulum (long, heavy pole; high friction), but applies it directly to the nonlinear system without accounting for large-angle deviations or proper state wrapping in dynamics.\n- **Performance**: Achieves full 1000-step simulation (stabilization_ratio=1.0) but with very poor control accuracy (final_theta_error=3.11 rad, final_x_error=4433 m) and low combined score (0.00).\n- **Feedback**: Despite running to completion, the controller fails to stabilize the pendulum near upright due to mismatch between linear LQR assumptions and highly nonlinear, challenging dynamics; large steady-state errors indicate inadequate gain tuning or linearization validity.\n**Program Identifier:** Generation 3 - Patch Name correct_linearization_tune_weights - Correct Program: False\n\n**Program Name: Hybrid Energy-LQR Inverted Pendulum Controller**  \n- **Implementation**: Combines energy-based swing-up control for large angles with a nonlinear LQR stabilizer using state-dependent gain scheduling; includes friction modeling and Euler integration.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum within limits (final theta error: 2.52 rad, exceeded 1.0 rad threshold).  \n- **Feedback**: Despite sophisticated hybrid control design, the controller fails to recover from the aggressive initial condition (0.9 rad) given the heavy, long pole and high friction; likely issues include inaccurate linearization in LQR design and insufficient robustness in transition logic.\n**Program Identifier:** Generation 4 - Patch Name nonlinear_lqr_with_energy_shaping - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized LQR controller with hand-tuned Q and R matrices on a highly unstable inverted pendulum (long, heavy pole; high friction); applies Euler integration for dynamics and clips control force to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 0.00 due to poor stabilization\u2014final angle error of 2.95 rad and position error over 4000 m.  \n- **Feedback**: Despite aggressive LQR tuning (high angle weight, low control penalty), the controller fails to stabilize the system, likely due to severe nonlinearity from large initial angle (0.9 rad) and model mismatch between linear LQR and highly nonlinear dynamics.\n**Program Identifier:** Generation 5 - Patch Name aggressive_lqr_tuning - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses two LQR controllers (conservative for large angles, aggressive near upright) with linearized dynamics around the upright equilibrium; applies state feedback with angle wrapping and force clipping.  \n- **Performance**: Scored 0.00 due to failure to stabilize, despite full simulation steps (stabilization_ratio=1.00), with high final position error (4357.35 m) and pole angle error (2.91 rad).  \n- **Feedback**: The controller fails to handle the highly unstable system (long, heavy pole; high friction); aggressive gains activate too late, and conservative gains are too weak to recover from large initial angle (0.9 rad). Linearization may be invalid for such large deviations.\n**Program Identifier:** Generation 6 - Patch Name accurate_linearization_and_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses three gain-scheduled LQR controllers selected based on angle and angular velocity magnitude, with state normalization and energy-based force scaling near equilibrium.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization (final theta error: 2.82 rad, cart position diverged to 4340 m).  \n- **Feedback**: Despite adaptive gains and proper linearization, the controller failed to stabilize the highly unstable system (long, heavy pole with high friction), suggesting insufficient robustness or incorrect dynamics modeling in the control law.\n**Program Identifier:** Generation 7 - Patch Name adaptive_lqr_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses two LQR controllers (recovery and balance modes) blended via a sigmoid-like function based on pole angle; control gains are scheduled smoothly to handle large initial deviations.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum within limits, despite high base score (14.54), indicating simulation ran full duration but ended far from upright.  \n- **Feedback**: The controller failed to correct large initial angle (0.9 rad) under challenging dynamics (heavy, long pole); final theta error (2.76 rad) and massive cart drift (x \u2248 3318 m) suggest instability or incorrect linearization assumptions in LQR design.\n**Program Identifier:** Generation 8 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a suboptimal adaptive LQR controller that switches between conservative and aggressive gain matrices based on pole angle; implemented with Euler integration and nonlinear dynamics simulation.  \n- **Performance**: Achieved a combined score of 0.00 due to high final position error (4262.33 m) and angle error (3.06 rad), despite running all 1000 steps.  \n- **Feedback**: The controller fails to stabilize the cart-pole system under challenging conditions (long, heavy pole; high friction); poor state regulation suggests inadequate gain tuning or linearization mismatch in the highly nonlinear regime.\n**Program Identifier:** Generation 9 - Patch Name adaptive_lqr_gains_near_equilibrium - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**\n\n- **Implementation**: Uses gain-scheduled LQR control blending aggressive and precision gains based on pole angle magnitude, augmented with integral action for friction compensation and additional damping terms for high velocities. Includes anti-windup, friction compensation, and smooth saturation.\n\n- **Performance**: Achieved a combined score of 0.00 despite moderate base_score (9.47) and time_bonus (1.01), due to complete failure in stabilization (final_theta_error: 3.03 rad, final_x_error: 4425 m).\n\n- **Feedback**: The controller fails to stabilize the pendulum under challenging conditions (heavy, long pole; high friction; large initial angle). Integral action and adaptive gains are insufficient to overcome model inaccuracies and aggressive dynamics, leading to divergence rather than recovery.\n**Program Identifier:** Generation 10 - Patch Name adaptive_gain_lqr_with_integral - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized model with friction-aware dynamics to compute an LQR controller; state feedback gain is derived from continuous-time algebraic Riccati equation with modest Q/R weights favoring low control effort over fast stabilization.  \n- **Performance**: Achieved a combined score of 3252.50, with strong energy efficiency (avg_energy_per_step: 0.01) but moderate stabilization ratio (0.44) and time bonus due to slow response.  \n- **Feedback**: The conservative LQR tuning successfully stabilized the challenging high-friction, long/heavy-pole system but sacrificed speed and precision\u2014evidenced by low base and time scores despite perfect final error and high success bonus.\n**Program Identifier:** Generation 11 - Patch Name correct_linear_model_friction_tune_gains - Correct Program: True\n\n**Program Name: High-Performance LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized model of an inverted pendulum with aggressive LQR cost weights (`Q=[15,35,2,4]`, `R=0.8`) and includes angle normalization; control forces are clipped to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: The LQR controller is based on a linear approximation that likely fails to capture the highly nonlinear dynamics of the heavy, long pole; early termination conditions (e.g., angle >1.0 rad) were commented out, possibly allowing unstable behavior to continue unchecked.\n**Program Identifier:** Generation 12 - Patch Name lqr_optimized_structural - Correct Program: False\n\n**Program Name: Aggressive LQR with Gain Scheduling**  \n- **Implementation**: Uses a state-dependent blend between aggressive and conservative LQR controllers based on pole angle magnitude, with high Q weights for tight angle control and lower R to allow stronger forces; designed for a heavy, long pole with high friction.  \n- **Performance**: Combined score: 0.0 \u2014 fails to stabilize the pendulum under validation tests.  \n- **Feedback**: Despite sophisticated gain scheduling and tuning for challenging dynamics, the controller fails to handle large initial angles or nonlinearities beyond the linearized model\u2019s validity; early termination due to instability suggests insufficient robustness in recovery phase.\n**Program Identifier:** Generation 13 - Patch Name aggressive_lqr_with_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses an extended state-space model with integral action for steady-state error reduction and adaptive gain scheduling based on pole angle to handle nonlinearities. The controller computes LQR gains offline using continuous-time Riccati equation and applies angle normalization and anti-windup logic for the integrator.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated control design, the linearized model used for LQR does not adequately capture the highly nonlinear dynamics of the heavy, long pole under high friction; additionally, the simulation lacks early termination on failure (e.g., pole angle >1.0 rad), leading to invalid trajectories that likely caused evaluation failures.\n**Program Identifier:** Generation 14 - Patch Name adaptive_lqr_with_integral_action - Correct Program: False\n\n**Program Name: Gain-Scheduled LQR Controller**  \n- **Implementation**: Uses a gain-scheduled LQR controller with low-gain (robust) and high-gain (precise) modes blended based on pole angle magnitude; system linearized around upright equilibrium but applied to highly nonlinear, challenging dynamics (long/heavy pole, high friction).  \n- **Performance**: Achieved only 10.40 base score with poor stabilization (final theta error 2.87 rad, large cart drift), indicating failure to control the pendulum despite full simulation duration.  \n- **Feedback**: The conservative LQR tuning (low Q, high R) and reliance on linearization in a highly nonlinear regime caused sluggish, inaccurate responses; controller failed to stabilize even within generous time limits, suggesting need for more aggressive gains or nonlinear control strategies.\n**Program Identifier:** Generation 15 - Patch Name gainscheduled_lqr - Correct Program: False\n\n**Program Name: Friction-Aware LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with friction terms in the A matrix and optimized LQR weights; includes angle normalization and semi-implicit state estimation for improved feedback.  \n- **Performance**: Achieved a combined score of 0.00 despite high base_score (9.85), failing to stabilize the pendulum within limits (final theta error: 2.82 rad, cart position diverged).  \n- **Feedback**: The controller failed to handle the highly unstable system (long, heavy pole with high friction); aggressive LQR tuning was insufficient without proper nonlinear handling or robustness to large initial angles.\n**Program Identifier:** Generation 16 - Patch Name friction_aware_lqr_optimized - Correct Program: False\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses a linearized state-space model with friction approximations to compute LQR gains, augmented with adaptive gain scheduling that increases control effort for large pole angles (>0.6 rad). Control forces are clipped to \u00b1100 N for physical realism.  \n- **Performance**: Achieved a high combined score of 3381.03, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800), stabilizing the pendulum in 422 steps.  \n- **Feedback**: The adaptive gain scheduling effectively handled the challenging initial condition (0.9 rad) and high-friction dynamics, enabling rapid stabilization despite the heavy, long pole. The controller maintained near-zero final errors, demonstrating robustness and precision.\n**Program Identifier:** Generation 17 - Patch Name advanced_friction_compensated_lqr - Correct Program: True\n\n**Program Name: Enhanced LQR with Friction Compensation**  \n- **Implementation**: Uses a linearized state-space model with friction terms to compute an LQR gain matrix, augmented with velocity-based gain scheduling and anti-windup force clipping.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilizing the pendulum within acceptable bounds despite high energy use.  \n- **Feedback**: The controller failed to stabilize the pole (final theta error = 3.04 rad), likely due to inaccurate linearization for large angles and insufficient robustness to the system's high nonlinearity, mass, and length.\n**Program Identifier:** Generation 18 - Patch Name none - Correct Program: False\n\n**Program Name: Suboptimal LQR with Friction Compensation**  \n- **Implementation**: Uses a linearized LQR controller with conservative Q/R weights and adds heuristic friction compensation for both cart and joint; state is wrapped to [\u2212\u03c0, \u03c0].  \n- **Performance**: Achieved a combined score of 3373.21, with moderate time and energy bonuses but low base accuracy due to high error tolerance.  \n- **Feedback**: The overly conservative LQR gains cause sluggish response and poor stabilization (only 42% stable steps), while friction compensation helps marginally but cannot overcome the weak control authority; evolution potential remains high.\n**Program Identifier:** Generation 19 - Patch Name nonlinear_friction_compensation_lqr - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware model to compute an LQR controller with deliberately conservative gains (low Q, high R), resulting in sluggish but stable control under challenging physics (long/heavy pole, high friction).  \n- **Performance**: Achieved a combined score of 3945.50, with strong energy efficiency and full success bonus despite slow stabilization.  \n- **Feedback**: The controller successfully stabilizes the pendulum but does so slowly (stabilization ratio 0.32), reflecting the trade-off from tuning for low control effort over responsiveness; this validates the design intent of a \"lazy\" yet functional baseline for evolutionary improvement.\n**Program Identifier:** Generation 20 - Patch Name friction_aware_lqr_model - Correct Program: True\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with LQR control, but the linearization inaccurately models friction and ignores nonlinear dynamics; Q/R weights prioritize angle over position yet are mismatched to the highly unstable physical parameters (long/heavy pole, high friction).  \n- **Performance**: Achieved near-zero combined score due to catastrophic failure in stabilization\u2014final theta error of 2.76 rad and cart drift exceeding 4000 m.  \n- **Feedback**: The controller fails because the linearized model is invalid for large initial angles (0.9 rad) and aggressive system dynamics; joint friction was incorrectly incorporated into the A matrix instead of being treated as a disturbance or nonlinear term, leading to poor real-world approximation and instability.\n**Program Identifier:** Generation 21 - Patch Name refine_lqr_model_with_physical_consistency - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware model to compute an LQR controller with deliberately conservative gains (low Q, high R) for stabilizing a challenging inverted pendulum (long, heavy pole; high friction). The controller applies clipped forces based on full-state feedback with angle normalization.\n\n- **Performance**: Achieved a combined score of 3933.91, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate time and stabilization performance (stabilization_ratio: 0.32).\n\n- **Feedback**: The conservative LQR tuning prioritizes low control effort over fast response, resulting in slow stabilization and lower time/stabilization scores despite perfect final error and high energy efficiency\u2014indicating room for improvement via gain optimization or nonlinear control.\n**Program Identifier:** Generation 22 - Patch Name fix_joint_friction_in_a_matrix - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling based on pole angle magnitude and additional angular velocity damping for small angles.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: Despite adaptive gains and damping, the controller is too conservative (high R, low Q) for the challenging environment (long/heavy pole, high friction), leading to instability and early failure; the initial angle of 0.9 rad may exceed its recovery capability.\n**Program Identifier:** Generation 23 - Patch Name smooth_adaptive_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a hybrid adaptive LQR controller with precise linearization of the nonlinear pendulum dynamics and gain scheduling based on pole angle and velocity magnitude. The controller boosts control gains when the pole deviates significantly from upright or when system velocity is high.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite accurate modeling and aggressive gain tuning, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction). Likely issues include inadequate handling of nonlinearities beyond small-angle approximations and possible instability from gain scheduling logic interacting poorly with the clipped control force.\n**Program Identifier:** Generation 24 - Patch Name adaptive_friction_compensated_lqr - Correct Program: False\n\n**Program Name: Friction-Compensated LQR Controller**  \n- **Implementation**: Uses a linearized state-space model with corrected friction terms and smooth gain scheduling based on pole angle; applies LQR control with optimized weights and clips force output.  \n- **Performance**: Achieved base score of 14.15 but failed to stabilize (final theta error = 2.62 rad, large position drift), resulting in zero success bonus and low combined score (0.00).  \n- **Feedback**: Despite improved modeling (e.g., correct joint friction term) and gain scheduling, the controller cannot stabilize the highly unstable system (heavy, long pole with high friction); poor handling of nonlinearities and insufficient robustness lead to failure.\n**Program Identifier:** Generation 25 - Patch Name smooth_adaptive_gains_with_corrected_friction_model - Correct Program: False\n\n**Program Name: Suboptimal LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware dynamics model to compute LQR gains with low state penalties (Q) and high control cost (R), plus gain scheduling based on pole angle.  \n- **Performance**: Achieved a combined score of 4431.51 with moderate time and energy bonuses but slow stabilization (259 steps, 26% ratio).  \n- **Feedback**: The conservative LQR tuning prioritizes low energy use over fast response, resulting in delayed stabilization; gain scheduling helps recover from large angles but doesn't fully compensate for the intentionally sluggish baseline policy.\n**Program Identifier:** Generation 26 - Patch Name tune_lqr_weights_angular_priority - Correct Program: True\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses an LQR controller with linearized dynamics including friction approximations, enhanced with nonlinear friction compensation via tanh-smoothed sign functions and adaptive gain scheduling for large angle deviations.  \n- **Performance**: Achieved a high combined score of 3447.47, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus due to stable final state.  \n- **Feedback**: The friction compensation and gain scheduling effectively stabilized a challenging high-friction, long-pole system; early termination was avoided by maintaining stability beyond 400 steps despite aggressive initial conditions.\n**Program Identifier:** Generation 27 - Patch Name nonlinear_friction_compensation - Correct Program: True\n\n**Program Name: Suboptimal LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains with intentionally conservative Q/R weights; applies gain scheduling based on pole angle.  \n- **Performance**: Achieves low initial score (~3000) due to sluggish response and high error tolerance, failing validation tests.  \n- **Feedback**: The controller\u2019s \"lazy\" tuning (low state penalties, high control cost) and lack of robustness to the challenging physics (long/heavy pole, high friction) prevent effective stabilization; early termination logic is commented out, risking wasted computation after failure.\n**Program Identifier:** Generation 28 - Patch Name switch_to_discrete_lqr - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model with corrected friction terms to compute LQR gains, enhanced by velocity- and angle-aware adaptive gain scheduling for aggressive stabilization.  \n- **Performance**: Achieved a high combined score of 4491.63 with fast stabilization (251 steps), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The adaptive gain scheduling significantly improved control authority during large deviations while maintaining efficiency near equilibrium, enabling robust handling of the challenging heavy/long pole configuration.\n**Program Identifier:** Generation 29 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Swing-Up Assistance**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with gain scheduling and energy-based swing-up logic for large angles; state feedback is blended during transition from swing-up to stabilization.  \n- **Performance**: Achieves low initial score (~3000) due to conservative LQR weights and challenging physics (long/heavy pole, high friction).  \n- **Feedback**: The controller stabilizes the pendulum but responds slowly and tolerates large errors; evaluation indicates room for improvement in both control aggressiveness and handling of extreme initial conditions.\n**Program Identifier:** Generation 30 - Patch Name enhanced_lqr_with_swing_up_trigger - Correct Program: False\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses a linearized state-space model at equilibrium to compute LQR gains, augmented with nonlinear friction compensation and smooth gain scheduling based on pole angle magnitude. However, the `Controller.__init__` references undefined variable `cos_theta` when constructing matrix `A`, causing runtime errors.  \n- **Performance**: Combined score: 0.0 \u2014 fails validation due to incorrect dynamics linearization and implementation bugs.  \n- **Feedback**: The controller logic is conceptually sound but flawed in execution; the undefined `cos_theta` breaks initialization, and the linearized `A` and `B` matrices do not correctly reflect the true system dynamics around \u03b8=0, undermining stability guarantees.\n**Program Identifier:** Generation 31 - Patch Name refine_lqr_gain_scheduling - Correct Program: False\n\n**Program Name: Hybrid Energy-LQR with Friction Compensation**  \n- **Implementation**: Combines energy-based control for large pole angles and LQR for small angles, with adaptive friction compensation and smooth blending between modes. Uses nonlinear dynamics simulation with Euler integration and high-friction-aware linearization for LQR design.  \n- **Performance**: Scored 0.00 due to failure to stabilize; achieved full 1000 steps but with large final errors (theta: 1.24 rad, x: 85.26 m) and no success bonus.  \n- **Feedback**: Despite sophisticated hybrid control and friction handling, the controller failed to stabilize the pendulum within acceptable bounds, indicating insufficient robustness for the extreme parameters (heavy, long pole, high friction). The system remained unstable throughout, suggesting poor tuning or inadequate energy management during transitions.\n**Program Identifier:** Generation 32 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with velocity- and angle-based adaptive gain scheduling to handle large initial deviations and high system instability from a heavy, long pole and high friction.  \n- **Performance**: Achieved a high combined score of 4469.91 with fast stabilization (254 steps), low energy use (total_energy: 6.18), and full success bonus.  \n- **Feedback**: The adaptive gains significantly improved robustness and speed of stabilization under challenging dynamics, while the precise friction modeling in the LQR design contributed to energy efficiency and stable final error near zero.\n**Program Identifier:** Generation 33 - Patch Name None - Correct Program: True\n\n**Program Name: Nonlinear Adaptive Energy-Shaping Controller**  \n- **Implementation**: Uses a composite control law combining nonlinear PD terms for angle/position, energy-shaping feedback, adaptive gain scheduling, and friction compensation to stabilize a challenging inverted pendulum (heavy, long pole; high friction).  \n- **Performance**: Achieved a combined score of 0.00 despite full simulation duration (1000 steps), with poor final stabilization (final_theta_error=1.44 rad, final_x_error=31.17 m) and no success bonus.  \n- **Feedback**: The controller fails to stabilize the pendulum upright\u2014likely due to aggressive energy shaping or insufficient coupling between cart position and pole dynamics\u2014resulting in divergence rather than recovery from large initial angles.\n**Program Identifier:** Generation 34 - Patch Name nonlinear_adaptive_controller - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must include functional code to address the problem requirements.\n**Program Identifier:** Generation 35 - Patch Name None - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform its intended task; implementation must include functional code to address the problem requirements.\n**Program Identifier:** Generation 36 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physics-matched continuous-time LQR controller with smooth, differentiable gain scheduling via tanh-based adaptive gains on position and velocity errors. The linearized dynamics matrix precisely aligns with the nonlinear simulator\u2019s structure, including consistent friction and denominator terms.  \n- **Performance**: Achieved a high combined score of 4878.27, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 198), and full success bonus.  \n- **Feedback**: The precise alignment between the controller\u2019s linear model and the simulator\u2019s physics enabled robust control despite challenging parameters (heavy/long pole, high friction). Adaptive gain scheduling improved transient response without introducing discontinuities, contributing to both stability and low energy use.\n**Program Identifier:** Generation 37 - Patch Name adaptive_lqr_friction_aware - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, incorporating friction terms matching the simulator, and applies smooth gain scheduling via tanh-based adaptation on angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4831.64 with fast stabilization (205 steps), low energy use (total_energy=6.26), and full success bonus.  \n- **Feedback**: The adaptive gains significantly improved robustness to large initial angles and high friction, enabling stable control of an otherwise highly unstable system; precise friction modeling in the LQR design was key to performance.\n**Program Identifier:** Generation 38 - Patch Name corrected_friction_and_aggressive_tuning - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model matching the simulator\u2019s physics to compute LQR gains, augmented with smooth tanh-based gain scheduling for large-angle recovery and swing-up logic when |\u03b8| > 0.85 rad.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization (final \u03b8 error = 1.59 rad, x error = 1408 m), despite full simulation runtime (1000 steps).  \n- **Feedback**: The controller fails to stabilize the pendulum\u2014likely because the swing-up logic applies incorrect force direction (based on sign(\u03b8\u00b7d\u03b8)) and the linear LQR is insufficient for highly nonlinear regimes; energy usage is high but ineffective.\n**Program Identifier:** Generation 39 - Patch Name add_swingup_trigger - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with smooth, multiplicative gain scheduling based on pole angle, angular velocity, and falling direction to enable faster stabilization under high-friction, heavy-pole conditions.  \n- **Performance**: Achieved a high combined score of 4666.63 with rapid stabilization (228 steps), low energy use (7.01 total), and perfect final error metrics.  \n- **Feedback**: The adaptive gains significantly improved response to large initial angles and instability while maintaining energy efficiency; the controller robustly handled challenging dynamics from increased mass, length, and friction.\n**Program Identifier:** Generation 40 - Patch Name aggressive_angular_damping_tuned_gains - Correct Program: True\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based modulation on angle and angular velocity errors to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a combined score of 4878.27 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but modest stabilization ratio (0.20) and base score (392.34).  \n- **Feedback**: The conservative LQR weights and high-friction dynamics limited responsiveness and precision, resulting in slower stabilization; however, the adaptive gains improved robustness enough to maintain balance for nearly the full duration despite challenging initial conditions.\n**Program Identifier:** Generation 41 - Patch Name optimized_lqr_weights_with_adaptive_gain - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control**  \n- **Implementation**: Uses an augmented-state LQR controller with integral action for zero steady-state error and adaptive gain scheduling based on angle/velocity error dominance; includes friction modeling and nonlinear dynamics.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum (final theta error: 2.95 rad, x error: >4000 m).  \n- **Feedback**: Despite sophisticated control design, the system diverges\u2014likely due to inaccurate linearization for large initial angles (0.9 rad) combined with aggressive or mis-tuned gains that destabilize rather than correct motion.\n**Program Identifier:** Generation 42 - Patch Name adaptive_lqr_integral - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling based on pole angle and angular velocity via smooth tanh transitions; system parameters reflect a challenging environment (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 4893.84 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) and base score (393.45).  \n- **Feedback**: The conservative LQR weights and adaptive gains enabled stability under difficult dynamics, yielding high time and energy bonuses, but limited responsiveness resulted in slower stabilization and lower precision.\n**Program Identifier:** Generation 43 - Patch Name optimized_adaptive_lqr_with_tanh_scheduling - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 44 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, enhanced with smooth gain scheduling via tanh-based adaptive terms that increase control effort during large deviations or falling-away dynamics.  \n- **Performance**: Achieved a combined score of 4657.90 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization was slow (stabilization_ratio: 0.23).  \n- **Feedback**: The adaptive gains improved robustness in a challenging high-friction, long-pole environment, but conservative base LQR weights limited responsiveness\u2014evident in low time bonus despite eventual stability.\n**Program Identifier:** Generation 45 - Patch Name optimized_q_weights_and_earlier_gain_activation - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with smooth, state-dependent adaptive gains based on angle and angular velocity to handle the highly unstable, high-friction pendulum dynamics.  \n- **Performance**: Achieved a combined score of 4839.16, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (204 steps).  \n- **Feedback**: The refined gain scheduling significantly improved robustness against large initial angles and high friction, enabling stable control despite aggressive system parameters; precise friction modeling in the LQR design was critical for real-world alignment.\n**Program Identifier:** Generation 46 - Patch Name adaptive_lqr_optimized_qr - Correct Program: True\n\n**Program Name: Discrete LQR with Integral Control**  \n- **Implementation**: Uses a discrete-time LQR controller derived from linearized dynamics via matrix exponential, augmented with integral action on cart position and adaptive gain based on angle error. The controller matches the simulator\u2019s physics in its linearization and applies smooth error-weighted blending for robustness.  \n- **Performance**: Achieved a combined score of 1116.13 with perfect stabilization ratio (1.00) over 1000 steps but failed to balance the pole (final_theta_error: 0.91), resulting in zero success bonus.  \n- **Feedback**: Despite strong energy efficiency and sustained simulation time, the controller fails to stabilize the pole upright\u2014likely due to excessive reliance on integral action causing drift and insufficient handling of nonlinearities in the highly unstable system (long, heavy pole with high friction).\n**Program Identifier:** Generation 47 - Patch Name discrete_lqr_integral_control - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a 5-state linearized model with integral control for position regulation, combined with nonlinear gain scheduling based on angle and angular velocity to adaptively scale control effort. Friction terms are accurately modeled in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4284.15 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (278 steps), and full success bonus.  \n- **Feedback**: The integral action effectively eliminated steady-state position error, while adaptive gains improved robustness during large deviations; accurate friction modeling and tuned LQR weights enabled stable control despite the challenging pole parameters.\n**Program Identifier:** Generation 48 - Patch Name lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control**  \n- **Implementation**: Uses a discrete-time LQR controller with integral action on cart position, augmented by sigmoid-based gain scheduling that blends angle- and velocity-dependent adaptive gains for smoother control.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly challenging pendulum (long, heavy pole; high friction; large initial angle), likely due to inaccuracies in linearization, improper integral handling, or insufficient robustness to nonlinear dynamics.\n**Program Identifier:** Generation 49 - Patch Name friction_aware_lqr_v2 - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model of the inverted pendulum to compute LQR gains, enhanced with smooth, differentiable gain scheduling via tanh-based adaptive scaling on position and velocity errors. The controller enforces physics-consistent linearization and applies force clipping for actuator limits.  \n- **Performance**: Achieved a high combined score of 4920.14, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192), and full success bonus.  \n- **Feedback**: The adaptive gain scheduling significantly improved stabilization speed and robustness under challenging conditions (heavy, long pole; high friction; large initial angle). Precise alignment between simulation dynamics and LQR linearization ensured stable control without divergence.\n**Program Identifier:** Generation 50 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Gains**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, then applies adaptive scaling via tanh-based gain scheduling and adds swing-up assistance for large angles.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and fails validation tests.  \n- **Feedback**: The controller\u2019s intentionally lazy response (high R, low Q) and aggressive physical parameters cause instability; adaptive gains and swing-up logic are insufficient to compensate for poor baseline LQR performance under high difficulty.\n**Program Identifier:** Generation 51 - Patch Name discrete_lqr_zoh - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a friction-aware linearized model for LQR gain computation with adaptive, differentiable gain scheduling based on angle and angular velocity errors via sigmoid blending.  \n- **Performance**: Achieved a combined score of 3917.04 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite challenging pole parameters.  \n- **Feedback**: The controller stabilizes the pendulum reliably but is intentionally \"lazy\" (low Q/R tuning), resulting in slower response (stabilization_ratio: 0.33) and moderate base score; adaptive gains help maintain stability without excessive force.\n**Program Identifier:** Generation 52 - Patch Name smooth_error_mode_blending - Correct Program: True\n\n**Program Name: Adaptive LQR with Synergy Gain**  \n- **Implementation**: Uses a high-precision linearized model matching the simulator's physics to compute LQR gains, enhanced with adaptive multiplicative gains based on angle, angular velocity, and falling-away detection via smooth tanh-based scheduling.  \n- **Performance**: Achieved a combined score of 4864.32 with full success bonus, fast stabilization (200 steps), and low energy use (avg 0.01 per step).  \n- **Feedback**: The controller\u2019s accurate system modeling and adaptive gains enabled robust stabilization of an extremely challenging (long, heavy, high-friction) pendulum, balancing speed and efficiency while preventing divergence during large initial angles.\n**Program Identifier:** Generation 53 - Patch Name enhanced_adaptive_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**\n\n- **Implementation**: Uses a linear-quadratic regulator (LQR) controller on an extended state space that includes an integral term for cart position, combined with smooth gain scheduling via tanh-based adaptive gains to handle nonlinearities and large initial errors.\n- **Performance**: Achieved a high combined score of 4502.04, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 248), and full success bonus.\n- **Feedback**: The integral action effectively eliminated steady-state position error, while adaptive gain scheduling improved robustness to the system's challenging dynamics (heavy, long pole with high friction). The controller maintained stability even under aggressive initial conditions.\n**Program Identifier:** Generation 54 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a discrete-time LQR controller extended with integral action for cart position regulation, enhanced by adaptive gain scheduling based on pole angle, angular velocity, and falling direction. The linearized dynamics model matches the simulator\u2019s physics, including consistent treatment of friction and denominator terms.  \n- **Performance**: Achieved a high combined score of 4383.78, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, stabilizing within 264 steps.  \n- **Feedback**: The adaptive gains and integral action effectively handled the challenging setup (heavy, long pole; high friction; large initial angle), enabling stable control while minimizing energy use. Precise alignment between the controller\u2019s linear model and the nonlinear simulator was critical to performance.\n**Program Identifier:** Generation 55 - Patch Name improved_lqr_with_discrete_time_and_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with suboptimal gains and smooth gain scheduling via tanh-based adaptive scaling on angle and angular velocity errors.  \n- **Performance**: Achieved a combined score of 4878.24 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20).  \n- **Feedback**: The conservative LQR tuning prioritizes stability over responsiveness, resulting in low energy use and high success rate but slower convergence (low time bonus relative to max), indicating room for improved transient performance through more aggressive control near instability thresholds.\n**Program Identifier:** Generation 56 - Patch Name increase_angular_velocity_weight_in_q_matrix - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced by smooth tanh-based gain scheduling that increases control effort for large angle/velocity errors.  \n- **Performance**: Achieves a combined score of 0.0 and fails validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly unstable pendulum (long, heavy pole with high friction), likely due to inaccuracies in linearization or insufficient robustness to nonlinear dynamics at large initial angles.\n**Program Identifier:** Generation 57 - Patch Name refine_q_weights - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced by smooth tanh-based gain scheduling that increases control effort for large angle/velocity errors.  \n- **Performance**: Achieved a high combined score of 4920.14, with strong time, energy, and success bonuses indicating stable, efficient balancing.  \n- **Feedback**: The adaptive gain scheduling significantly improved robustness on the challenging high-friction, long/heavy-pole setup, enabling fast stabilization (192 steps) and near-zero final errors while using minimal energy.\n**Program Identifier:** Generation 58 - Patch Name optimized_q_and_multiplicative_gains - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physics-consistent linearized model for LQR control with optimized state weights and smooth tanh-based gain scheduling that adapts to angle and angular velocity errors.  \n- **Performance**: Achieved a high combined score of 4920.13, with strong stabilization (success bonus = 800), low energy use (avg 0.01/step), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains improved transient response without introducing discontinuities, while precise alignment between the controller\u2019s A matrix and simulator dynamics was critical for stability under challenging conditions (heavy, long pole; high friction).\n**Program Identifier:** Generation 59 - Patch Name optimized_lqr_smooth_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a friction-aware linearized model with discrete-time LQR and smooth tanh-based gain scheduling to adaptively adjust control effort based on angle and angular velocity errors.  \n- **Performance**: Achieved a combined score of 4872.34 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20).  \n- **Feedback**: The adaptive gains improved responsiveness to large initial angles while maintaining stability; however, the conservative Q/R tuning limited time efficiency despite low energy use.\n**Program Identifier:** Generation 60 - Patch Name discrete_lqr_with_refined_gains - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based modulation on angle and angular velocity errors to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a combined score of 4878.25 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) indicating slow convergence.  \n- **Feedback**: The conservative LQR weights and high friction tolerance enabled stability under challenging dynamics, yet the \"lazy\" control response limited time-based rewards; adaptive gains improved large-error handling without introducing discontinuities.\n**Program Identifier:** Generation 61 - Patch Name increase_angular_velocity_penalty - Correct Program: True\n\n**Program Name: Phase-Based Adaptive Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-detection mechanism to switch between four specialized control strategies (Recovery, Swing-Back, Capture, Centering), combining LQR near equilibrium with nonlinear adaptive gains and integral cart-position control. The controller dynamically blends base LQR actions with phase-specific forces based on real-time state analysis.  \n- **Performance**: Achieved a high combined score of 4739.13, with strong time, energy, and success bonuses, stabilizing the pole in 218 steps and maintaining low final errors.  \n- **Feedback**: The phase-based adaptation effectively handles the challenging dynamics of a heavy, long pole with high friction, enabling both aggressive recovery and precise stabilization. Energy efficiency and smooth phase transitions contributed significantly to the high performance.\n**Program Identifier:** Generation 62 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a discretized LQR controller with friction-aware linearization, integral action activated near upright via tanh scheduling, and adaptive gain blending based on angle/velocity error dominance.  \n- **Performance**: Achieved a combined score of 3219.32 with moderate time and energy bonuses but low stabilization ratio (0.33).  \n- **Feedback**: The conservative LQR weights and high friction tolerance led to sluggish response and poor stabilization; integral action helped reduce steady-state position error but wasn't sufficient to maintain balance under challenging initial conditions (0.9 rad).\n**Program Identifier:** Generation 63 - Patch Name add_integral_action_with_tanh_activation - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, then applies smooth tanh-based gain scheduling on angle and angular velocity to adaptively scale control effort.  \n- **Performance**: Achieves a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction); likely due to inaccurate linearization or insufficient robustness to nonlinearities and large initial angles.\n**Program Identifier:** Generation 64 - Patch Name fine_tune_q_matrix - Correct Program: False\n\n**Program Name: Adaptive Discrete-Time LQR Controller**  \n- **Implementation**: Uses a discrete-time LQR controller derived from linearized continuous dynamics with Zero-Order Hold discretization, augmented with adaptive gain scheduling based on pole angle and angular velocity. The physics simulation includes high friction, a long/heavy pole, and large initial angle for increased difficulty.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite accurate modeling and adaptive gains, the controller fails to stabilize the highly unstable system\u2014likely due to excessive nonlinearity beyond the LQR\u2019s linear approximation range and insufficient robustness to large initial deviations.\n**Program Identifier:** Generation 65 - Patch Name adaptive_lqr_with_discrete_tuning - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linearized model for LQR control with optimized Q weights and smooth tanh-based gain scheduling that adapts to pole angle and angular velocity errors.  \n- **Performance**: Achieves a combined score of 4920.14 with full success bonus, low energy use (6.45 total), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains significantly improve robustness under challenging conditions (heavy/long pole, high friction), while the accurate linearization and tuned LQR weights enable precise stabilization with minimal control effort.\n**Program Identifier:** Generation 66 - Patch Name adaptive_lqr_tuned_q - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, incorporating friction terms matching the simulator, and applies adaptive gain scheduling based on angle and angular velocity errors to boost responsiveness during large deviations.  \n- **Performance**: Achieved a high combined score of 4920.14 with fast stabilization (192 steps), low energy use (avg 0.01 per step), and perfect final error metrics.  \n- **Feedback**: The controller\u2019s precise modeling of friction and intelligent gain adaptation enabled robust handling of a challenging, heavy, and long pole while maintaining energy efficiency and rapid stabilization.\n**Program Identifier:** Generation 67 - Patch Name optimized_q_matrix_and_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linear quadratic regulator (LQR) with physically accurate system matrices and proven optimal state weights; enhances robustness via multiplicative gain scheduling using tanh-based adaptation on angle and angular velocity.  \n- **Performance**: Achieved a combined score of 4920.13, including full success bonus, low energy usage (total_energy=6.44), and fast stabilization (192 steps).  \n- **Feedback**: The exact LQR tuning and smooth gain scheduling enabled stable control despite challenging parameters (heavy/long pole, high friction); minor Q-matrix adjustment (Q[3]=3.28) improved damping without sacrificing performance.\n**Program Identifier:** Generation 68 - Patch Name energy_based_sliding_mode_control - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based blending of position and velocity gains, tailored to a challenging high-friction, long/heavy-pole environment.  \n- **Performance**: Achieved a combined score of 4893.84 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) indicating slow convergence.  \n- **Feedback**: The conservative LQR weights and adaptive gains enable stability under difficult dynamics but result in sluggish response (low time bonus relative to max), suggesting room for tuning aggressiveness without sacrificing robustness.\n**Program Identifier:** Generation 69 - Patch Name optimize_lqr_weights_and_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive Swing-Up LQR with Integral Control**  \n- **Implementation**: Combines discrete-time LQR on an extended state (including integral of position), physics-informed swing-up assistance using hyperbolic tangent switching, and soft-switched integral action that deactivates far from equilibrium. Uses accurate linearized dynamics and zero-order hold discretization.  \n- **Performance**: Achieved a combined score of 0.00 due to failure to stabilize the pendulum within limits (final theta error: 2.81 rad, final x error: 3498 m).  \n- **Feedback**: Despite aggressive swing-up and integral terms, the controller failed to stabilize the highly challenging system (heavy, long pole with high friction); large steady-state errors suggest insufficient control authority or poor tuning for this extreme configuration.\n**Program Identifier:** Generation 70 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model to compute LQR gains with optimized state weights, augmented by smooth gain scheduling via tanh and physics-informed swing-up feedforward based on angle and angular velocity.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests despite design efforts for high difficulty conditions.  \n- **Feedback**: The controller\u2019s linearization may be inaccurate for large angles (e.g., 0.9 rad initial condition), and the swing-up logic might introduce destabilizing forces; friction modeling in the linearized A matrix appears inconsistent with the nonlinear simulator dynamics.\n**Program Identifier:** Generation 71 - Patch Name additive_swing_assist_feedforward - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with gain scheduling based on pole angle and angular velocity, plus an additive swing-up term that activates for large angles to counteract falling dynamics.  \n- **Performance**: Achieved a high combined score of 4920.15 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The adaptive gains and physics-informed swing-up assist effectively handle the challenging heavy/long pole configuration, enabling robust stabilization while minimizing energy use; precise friction modeling in both simulation and controller contributes to real-world fidelity and performance.\n**Program Identifier:** Generation 72 - Patch Name additive_swing_assist_with_proven_base - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized model to compute LQR gains with suboptimal Q/R weights, enhanced by smooth tanh-based gain scheduling that increases control effort only when angle or angular velocity errors exceed thresholds.  \n- **Performance**: Achieved a combined score of 4920.14 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The conservative LQR tuning ensures stability under challenging physics (long/heavy pole, high friction), while adaptive gains improve transient response without sacrificing smoothness; however, delayed aggressive correction limits time-based rewards.\n**Program Identifier:** Generation 73 - Patch Name none - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR Controller**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation combined with hybrid gain scheduling that blends linear and nonlinear responses based on angle and angular velocity magnitude; includes robust angle normalization and force clipping.  \n- **Performance**: Achieved a high combined score of 4865.75 with strong energy efficiency, full success bonus, and excellent stabilization.  \n- **Feedback**: The adaptive gain scheduling significantly improved control responsiveness for large deviations while maintaining precision near equilibrium, enabling stable performance even under challenging conditions (heavy, long pole with high friction and large initial angle).\n**Program Identifier:** Generation 74 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**\n\n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based smooth transitions and additive physics-informed swing-up assistance for large angles. The system parameters reflect a highly unstable pendulum (long, heavy pole; high friction).\n\n- **Performance**: Achieves an initial score near 3000 but fails to stabilize consistently under validation tests, resulting in a combined score of 0.0.\n\n- **Feedback**: Despite sophisticated control logic, the controller is too \"lazy\" due to conservative LQR weights (low Q, high R), leading to insufficient correction for the challenging dynamics. Validation likely fails because the pendulum exceeds the 1.0 rad failure threshold before stabilization.\n**Program Identifier:** Generation 75 - Patch Name optimized_q_with_swing_assist - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with suboptimal gains, enhanced by physics-informed swing-up logic and adaptive gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 4724.67, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite slow stabilization (stabilization_ratio: 0.22).  \n- **Feedback**: The conservative LQR tuning prioritizes low energy use over fast response, resulting in high energy and success bonuses but modest time and base scores; the swing-up assist effectively handles large initial angles in a challenging high-friction, long-pole environment.\n**Program Identifier:** Generation 76 - Patch Name enhanced_swing_up_assist_with_adaptive_gain - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with tanh-based gain scheduling and additive swing-up control for large angles (>0.8 rad) to handle highly unstable configurations (long, heavy pole).  \n- **Performance**: Achieved a high combined score of 4920.15, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (192 steps).  \n- **Feedback**: The hybrid approach effectively balances optimal linear control near equilibrium with non-linear corrective actions during large deviations, enabling robust performance under extreme physical parameters; angle normalization and friction-aware dynamics modeling contributed to numerical stability and realism.\n**Program Identifier:** Generation 77 - Patch Name enhanced_swingup_assist_and_q_optimization - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in no functional logic or algorithm implementation.  \n- **Performance**: Combined score is 0.0, indicating complete failure to meet validation requirements.  \n- **Feedback**: The absence of any implementation means the program cannot process inputs, produce outputs, or satisfy basic correctness criteria. A valid solution must include core logic aligned with the problem specification.\n**Program Identifier:** Generation 78 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assistance**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based activation and continuous swing-up assistance for large angles, tailored to a highly unstable pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a combined score of 4920.19, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, despite slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The \"lazy\" LQR weights (low Q, high R) prioritize energy over speed, limiting time bonus but enabling robust stability; the smooth, differentiable swing-up logic effectively handles large initial deviations without abrupt control switches.\n**Program Identifier:** Generation 79 - Patch Name continuous_falling_factor_swing_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based smooth transitions and an additive swing-up term for large angles (>0.8 rad), tailored to a challenging pendulum (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 4920.22, with strong energy efficiency (avg_energy_per_step=0.01) and full success bonus, but slow stabilization (stabilization_ratio=0.19).  \n- **Feedback**: The conservative LQR weights (low Q, high R) prioritize low energy use over fast response, resulting in delayed stabilization; however, the swing-up assist effectively recovers from large initial deviations, enabling consistent success despite difficult dynamics.\n**Program Identifier:** Generation 80 - Patch Name falling_severity_swing_assist - Correct Program: True\n\n**Program Name: Adaptive Momentum Compensation Controller**  \n- **Implementation**: Uses phase-aware switching between swing-up, transition, and fine-tuning control modes with momentum compensation, energy shaping, nonlinear damping, and friction compensation; includes integral action near equilibrium and smooth force limiting.  \n- **Performance**: Achieved a combined score of 0.00 due to failure to stabilize the pendulum (final theta error: 1.81 rad, x error: 20.07 m), despite high energy usage (total_energy: 123.20).  \n- **Feedback**: The controller fails to balance the highly unstable system (long, heavy pole with high friction); aggressive swing-up terms may destabilize early dynamics, and LQR-like gains appear insufficient for stabilization under these extreme parameters.\n**Program Identifier:** Generation 81 - Patch Name adaptive_momentum_compensation_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-consistent linearized LQR controller with adaptive gain scheduling and a momentum-based swing-up assist triggered for large angles (>0.8 rad), using normalized angular momentum and smooth tanh activations for differentiable control adjustments.  \n- **Performance**: Achieved a high combined score of 4920.12, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00), stabilizing in 192 steps.  \n- **Feedback**: The swing-up assist effectively handles the challenging initial condition (0.9 rad) with a heavy, long pole, while precise friction modeling and consistent linearization between simulation and LQR design ensure robust stabilization and minimal steady-state error.\n**Program Identifier:** Generation 82 - Patch Name none - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Action**  \n- **Implementation**: Combines a physically accurate linearized LQR controller with conditional integral action (activated near equilibrium), robust angle normalization, smooth gain scheduling via tanh, and physics-informed swing-up assist for large angles.  \n- **Performance**: Achieved a high combined score of 4691.82 with full success bonus, low energy use (avg 0.01/step), and rapid stabilization (224 steps).  \n- **Feedback**: The hybrid approach effectively balances optimal linear control near equilibrium with non-linear corrections during large deviations, enabling stable control despite challenging parameters (heavy/long pole, high friction). Conditional integration prevents windup while improving steady-state cart position accuracy.\n**Program Identifier:** Generation 83 - Patch Name hybrid_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Gated Integral Control**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, augmented with a gated integral term on cart position that activates only near upright equilibrium, and applies smooth gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 1278.20, with perfect stabilization ratio (1.00) over 1000 steps but zero success bonus due to failure to meet terminal position/angle thresholds.  \n- **Feedback**: Despite excellent energy efficiency and stability maintenance, the controller fails to center the cart (final_x_error = 25.87), indicating the integral action is insufficiently aggressive or misaligned with the true control objective; the lack of success bonus suggests the final state does not satisfy evaluation criteria for \"solved\" episodes.\n**Program Identifier:** Generation 84 - Patch Name adaptive_lqr_with_integrated_correction - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model to compute LQR gains, augmented with tanh-based gain scheduling, soft integral control, and swing-up assistance for large angles. The controller adapts force output based on pole angle and angular velocity deviations.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests despite sophisticated design.  \n- **Feedback**: Overly aggressive gain scheduling and incorrect swing-up logic likely destabilize the system under high-difficulty conditions (heavy/long pole, large initial angle). The integral term and friction modeling may also introduce instability rather than robustness.\n**Program Identifier:** Generation 85 - Patch Name soft_integral_cart_centering - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with adaptive gain scheduling based on angle and angular velocity, plus swing-up assistance for large deviations. Friction terms in the dynamics are carefully matched between simulation and controller.  \n- **Performance**: Achieved a high combined score of 4920.22 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The controller excels in balancing aggressive correction for large errors with minimal energy use near equilibrium; precise modeling of friction and well-tuned LQR weights were critical to success.\n**Program Identifier:** Generation 86 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-informed linearized LQR controller with adaptive gain scheduling and phase-aware swing-up assistance based on normalized angular momentum and pole dynamics. Uses precise friction modeling, consistent linearization around upright equilibrium, and smooth exponential transitions for gain adaptation.  \n- **Performance**: Achieved a high combined score of 4635.10 with full success bonus, low energy usage (avg 0.01/step), and fast stabilization (232 steps).  \n- **Feedback**: The swing-up assist effectively handles large initial deviations (0.9 rad) despite challenging pole parameters (heavy, long, high friction), while adaptive gains improve robustness without destabilizing the LQR core. Energy efficiency and stability were maintained through careful physics-consistent design.\n**Program Identifier:** Generation 87 - Patch Name phase_aware_swing_assist - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Action**  \n- **Implementation**: Combines a physically accurate linearized LQR controller with adaptive gain scheduling, swing-up assist for large angles, and soft-switched integral action to eliminate steady-state cart position error. Uses robust angle normalization and Euler integration matching the simulator dynamics exactly.  \n- **Performance**: Achieved a combined score of 1308.26 with perfect stabilization (1000 steps), low average energy per step (0.01), and high energy efficiency (energy_bonus: 2492.28).  \n- **Feedback**: The hybrid approach effectively stabilizes an extremely challenging system (heavy, long pole with high friction and large initial angle). Integral action and swing-up logic prevent drift and aid recovery, while adaptive gains enhance responsiveness without destabilizing the system near equilibrium.\n**Program Identifier:** Generation 88 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Combines energy-shaping swing-up control with LQR-based stabilization, using phase-based switching and friction compensation; includes adaptive gains and predictive terms for improved response.  \n- **Performance**: Achieved high energy usage (2499.93) but failed to stabilize the pendulum (final theta error: 2.93 rad), resulting in zero success bonus and low combined score (0.00).  \n- **Feedback**: Despite sophisticated control logic, the controller could not stabilize the highly challenging system (heavy, long pole with high friction); likely due to inaccurate dynamics modeling or insufficient robustness in transition between control phases.\n**Program Identifier:** Generation 89 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a suboptimal LQR controller with friction-aware linearization, adaptive gain scheduling via smooth tanh-based modulation, and soft-switched integral action for cart centering.  \n- **Performance**: Achieved a combined score of 1196.58 with perfect stabilization ratio (1.00) over 1000 steps but low base accuracy due to high position errors.  \n- **Feedback**: The conservative LQR tuning (low Q, high R) prioritizes stability over precision, resulting in high energy efficiency but poor final/state error metrics; adaptive gains help maintain balance under challenging dynamics (long/heavy pole, high friction).\n**Program Identifier:** Generation 90 - Patch Name add_soft_cart_integral - Correct Program: True\n\n**Program Name: Adaptive LQR with Synergy Gain**  \n- **Implementation**: Uses a high-precision discrete-time LQR controller derived from physically consistent linearized dynamics, enhanced with smooth adaptive gains based on pole angle, angular velocity, and falling-away detection via tanh-based scheduling.  \n- **Performance**: Achieved a combined score of 4842.54 with full success bonus, low energy use (5.86 total), and fast stabilization (203 steps).  \n- **Feedback**: The synergy gain effectively prevents divergence during large-angle excursions, while accurate friction-aware linearization ensures robustness; the adaptive gains significantly improve damping without destabilizing the system.\n**Program Identifier:** Generation 91 - Patch Name discrete_time_lqr_with_tuned_gains - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linearized LQR controller derived from consistent physics equations, enhanced with smooth tanh-based gain scheduling that adaptively increases control effort for large angle or angular velocity errors.  \n- **Performance**: Achieved a combined score of 4920.13 with full success bonus, fast stabilization (192 steps), and very low energy use (total_energy=6.44).  \n- **Feedback**: The adaptive gains significantly improved stabilization speed without sacrificing energy efficiency, while precise alignment between the controller\u2019s linear model and the nonlinear simulator ensured robustness under challenging conditions (heavy, long pole; high friction; large initial angle).\n**Program Identifier:** Generation 92 - Patch Name increase_angular_velocity_weight - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with gain scheduling based on pole angle and angular velocity, plus an additive swing-up assist for large deviations (>0.8 rad) to recover from near-failure states.  \n- **Performance**: Achieved a combined score of 4920.15, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization was slow (stabilization_ratio: 0.19).  \n- **Feedback**: The adaptive gains and swing-up logic effectively prevented failure and enabled eventual stabilization despite aggressive initial conditions and challenging physics; however, conservative base LQR weights limited responsiveness, resulting in low time bonus and delayed convergence.\n**Program Identifier:** Generation 93 - Patch Name multiplicative_gain_with_swing_assist - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Energy Assist**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance and adaptive gain scheduling, using physically accurate system linearization and friction-aware dynamics.  \n- **Performance**: Achieved a high combined score of 4920.21 with strong stabilization (success_bonus: 800), low energy use (avg_energy_per_step: 0.01), and fast stabilization (stabilization_time: 192).  \n- **Feedback**: The controller effectively handles large initial angles and high system instability due to a heavy, long pole; energy-efficient swing-up logic and precise LQR tuning enable both rapid recovery and stable balance.\n**Program Identifier:** Generation 94 - Patch Name robust_swingup_lqr - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a high-precision linearized LQR controller with optimized Q weights, adaptive gain scheduling via tanh-based activation, swing-up assistance for large angles, and soft integral action on cart position with anti-windup.  \n- **Performance**: Achieved a combined score of 4913.35 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (193 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging heavy/long pole and high friction, while integral control improved steady-state positioning without causing windup during transients.\n**Program Identifier:** Generation 95 - Patch Name swing_up_assist_with_integral - Correct Program: True\n\n**Program Name: Phase-Based Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-based control architecture with smooth transitions between recovery, transition, and precision phases; integrates LQR gains computed from friction-aware linearization, adaptive swing-up assistance for large deviations, and gated integral action for cart centering.  \n- **Performance**: Achieved a high combined score of 4845.21 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (203 steps), and perfect final pole angle (0.00 rad).  \n- **Feedback**: The phase-blending strategy effectively handles the challenging dynamics of a heavy, long pole with high friction, while anti-windup and upright-gated integral action prevent instability and ensure precise steady-state positioning.\n**Program Identifier:** Generation 96 - Patch Name phase_based_controller - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control derived from physically accurate linearized dynamics with adaptive gain scheduling and a physics-informed swing-up assist for large angles, using smooth tanh-based activation and falling severity estimation.  \n- **Performance**: Achieved a high combined score of 4920.19 with full success bonus, low energy usage (total_energy=6.48), and rapid stabilization (stabilization_time=192).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the highly unstable configuration (heavy, long pole; high friction); precise angle normalization and divergence-aware assistance enabled robust recovery and stable balancing.\n**Program Identifier:** Generation 97 - Patch Name adaptive_lqr_with_swing_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model with physically accurate friction terms to compute LQR gains, augmented with tanh-based adaptive gain scheduling and soft-switched integral action on cart position.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated control design, the controller fails to stabilize the highly unstable pendulum (long, heavy pole with high friction); likely due to inaccurate linearization, poor handling of nonlinear dynamics, or inadequate robustness to large initial angles.\n**Program Identifier:** Generation 98 - Patch Name robust_arctan2_angle_norm - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a suboptimal LQR controller with gain scheduling, integral action gated near upright, and physics-informed swing-up assistance for large angles. The linearized model accounts for friction, and control gains are adaptively scaled based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 1313.02 with perfect stabilization ratio (1.0) over 1000 steps but low base accuracy due to conservative tuning.  \n- **Feedback**: High energy efficiency (avg 0.01 per step) and full time utilization indicate stable balancing, yet poor position/angle precision (final_x_error=7.98, stable_x_error=2.79) reveals intentionally lazy control from low Q/high R weights, limiting base score despite robustness.\n**Program Identifier:** Generation 99 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a suboptimal LQR controller with friction-aware linearization and smooth gain scheduling via tanh-based adaptive gains to handle large pole angles and high friction.  \n- **Performance**: Achieves a combined score of 4920.14 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite challenging physics (long, heavy pole; high friction).  \n- **Feedback**: The conservative LQR weights result in slower response (low time bonus relative to max), but adaptive gains improve stability at large errors without discontinuities, enabling consistent stabilization within 192 steps.\n**Program Identifier:** Generation 100 - Patch Name update_to_best_lqr_weights - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a friction-aware linearized model for LQR gain computation, enhanced with smooth gain scheduling via tanh-based adaptive gains, swing-up assistance for large angles, and soft-switched integral action on cart position to improve steady-state accuracy.  \n- **Performance**: Achieved a combined score of 4920.10 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization ratio was low (0.19).  \n- **Feedback**: The controller effectively balances the challenging pendulum (long, heavy pole; high friction) and achieves stability, but slow response due to conservative LQR weights limits time bonus; integral action and adaptive gains help maintain precision once stabilized.\n**Program Identifier:** Generation 101 - Patch Name integral_action_for_zero_drift - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with adaptive gain scheduling via tanh-based blending, swing-up assist for large angles, and gated integral action for position error correction.  \n- **Performance**: Achieved a combined score of 1308.25 with perfect stabilization (1000 steps, stabilization ratio 1.0) and very low energy usage (avg 0.01 per step).  \n- **Feedback**: The controller successfully stabilizes an extremely challenging inverted pendulum (long, heavy pole; high friction; large initial angle), demonstrating robustness through hybrid nonlinear adaptations while maintaining energy efficiency\u2014though it incurs a positional drift (final x_error = 7.01), indicating room for improved steady-state accuracy.\n**Program Identifier:** Generation 102 - Patch Name increase_angular_velocity_damping - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**  \n- **Implementation**: Uses a friction-aware linearized model for LQR control with adaptive gain scheduling, integral action gated by pole angle, and physics-informed swing-up assistance for large deviations.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and environmental difficulty, failing validation tests with a combined score of 0.0.  \n- **Feedback**: The controller\u2019s deliberate \"laziness\" (low Q/high R weights) causes slow response and poor precision; despite advanced features like gain scheduling and swing-up logic, it cannot stabilize the highly unstable system within required limits.\n**Program Identifier:** Generation 103 - Patch Name robust_angle_norm - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute discrete-time LQR gains, enhanced by adaptive gain scheduling based on pole angle and angular velocity, plus swing-up assistance for large deviations.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive control logic, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction), likely due to inaccuracies in linearization or insufficient robustness in the gain adaptation under extreme initial conditions.\n**Program Identifier:** Generation 104 - Patch Name discrete_lqr_zoh - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Uses a high-precision linearized LQR controller with optimized Q weights, adaptive gain scheduling via tanh-based activation, swing-up assistance for large angles, and soft integral action on cart position with anti-windup and gating near equilibrium.  \n- **Performance**: Achieved a combined score of 4912.79 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (193 steps), and full success bonus.  \n- **Feedback**: The combination of physically consistent linearization, adaptive gains, and targeted swing-up logic effectively handles the challenging heavy/long pole and high-friction dynamics; integral action improves steady-state positioning without destabilizing transients.\n**Program Identifier:** Generation 105 - Patch Name integral_action_refinement - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 106 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a friction-aware linearized model for LQR control, enhanced with smooth gain scheduling via tanh, swing-up assist for large angles, and anti-windup integral action on cart position.  \n- **Performance**: Achieved a combined score of 4920.09 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle large initial angles and prevent failure, while conservative LQR weights and integral action improve steady-state accuracy without destabilizing the system. However, the controller\u2019s sluggish response limits time-based rewards.\n**Program Identifier:** Generation 107 - Patch Name integral_anti_windup - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with friction-aware dynamics, combined with gain scheduling based on pole angle/velocity and an additive swing-up term for large deviations.  \n- **Performance**: Achieved a high combined score of 4920.20 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The adaptive gains and physics-informed swing-up assist effectively handle the challenging heavy/long pole configuration, enabling robust recovery from large initial angles while minimizing energy use.\n**Program Identifier:** Generation 108 - Patch Name none - Correct Program: True\n\n**Program Name: Phase-Predictive LQR Controller**  \n- **Implementation**: Uses a discretized LQR controller with phase-aware switching between recovery, transition, and precision modes; includes predictive correction, swing-up assistance, and integral action for steady-state accuracy.  \n- **Performance**: Achieved a high combined score of 4782.29 with strong energy efficiency, fast stabilization (212 steps), and perfect final error metrics.  \n- **Feedback**: The adaptive phase logic and predictive corrections effectively handled the challenging high-friction, long-pole dynamics, enabling robust swing-up and precise stabilization while minimizing control effort.\n**Program Identifier:** Generation 109 - Patch Name phase_predictive_lqr - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a linearized friction-aware model for LQR control with adaptive gain scheduling via tanh-based smooth switching, integral action for position regulation (with anti-windup), and physics-informed swing-up assistance for large angles.  \n- **Performance**: Achieved a combined score of 4920.10 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The controller effectively balances the challenging high-friction, long/heavy-pole system and avoids failure, but its conservative gains and delayed response limit time-based rewards despite excellent steady-state accuracy.\n**Program Identifier:** Generation 110 - Patch Name integral_action_with_soft_switching - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Gains**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, augmented with smooth tanh-based gain scheduling and soft-switched integral action for cart position.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and fails validation tests under challenging physics (long/heavy pole, high friction).  \n- **Feedback**: The controller\u2019s \"lazy\" Q/R weights and insufficient robustness to large initial angles cause poor stabilization; adaptive gains help but don\u2019t compensate for fundamental underperformance in hard conditions.\n**Program Identifier:** Generation 111 - Patch Name increase_dtheta_weight - Correct Program: False\n\n**Program Name: Energy-Phase Adaptive LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based adaptive gains, phase-specific strategies (recovery, precision, predictive damping), and integral action with anti-windup for steady-state accuracy. The controller dynamically adjusts force output based on normalized mechanical energy, angular velocity, and state magnitude to handle a highly unstable, heavy, and long inverted pendulum.  \n- **Performance**: Achieved a high combined score of 4755.01 with strong bonuses for success, time, and energy efficiency, stabilizing the system in 215 steps with near-zero final errors.  \n- **Feedback**: The adaptive gain scheduling and phase-aware logic effectively managed the challenging dynamics (high mass, length, friction), while predictive damping and integral control ensured precise stabilization without overshoot or windup.\n**Program Identifier:** Generation 112 - Patch Name energy_phase_controller - Correct Program: True\n\n**Program Name: Adaptive Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-based hybrid controller with energy-based swing-up for large angles, blended transition control, and LQR with integral action for stabilization; includes nonlinear modifications like velocity damping and control authority scaling.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization despite high energy usage (total_energy=8.22) and full simulation duration (1000 steps).  \n- **Feedback**: The controller failed to stabilize the pendulum (final_theta_error=2.77 rad), indicating ineffective mode transitions or insufficient robustness for the highly unstable system (long, heavy pole with high friction); integral windup or aggressive swing-up may have hindered convergence.\n**Program Identifier:** Generation 113 - Patch Name none - Correct Program: False\n\n**Program Name: Feedback Linearization with Adaptive LQR**  \n- **Implementation**: Combines discrete LQR control on a linearized model with exact nonlinear inverse dynamics to track a desired angular acceleration, augmented by adaptive gain scheduling and gated integral action for position regulation.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: The controller struggles with the highly unstable, high-friction, and heavy/long pole configuration; likely due to singularities in inverse dynamics near \u00b1\u03c0/2 and insufficient robustness in the feedback linearization under large initial angles (0.9 rad). The adaptive gains and integral term may not compensate adequately for model inaccuracies or aggressive nonlinearities.\n**Program Identifier:** Generation 114 - Patch Name fbl_discretelqr_integral - Correct Program: False\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with optimized Q-matrix weights, enhanced by adaptive gain scheduling and a continuous swing-up assist that activates based on angle magnitude and falling severity.  \n- **Performance**: Achieved a combined score of 4920.22 with perfect stabilization (final theta error = 0.00) and high energy efficiency (avg_energy_per_step = 0.01).  \n- **Feedback**: The controller effectively balances aggressive correction for large deviations with fine-tuned stabilization near upright, while the swing-up logic ensures recovery from highly unstable initial conditions; the exact match between simulation dynamics and LQR model was critical to success.\n**Program Identifier:** Generation 115 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, then applies adaptive gain scheduling via tanh-based scaling and adds swing-up assistance for large angles (>0.8 rad).  \n- **Performance**: Achieves low initial score (~3000) due to conservative Q/R tuning and insufficient robustness in highly unstable conditions.  \n- **Feedback**: The controller fails validation tests because the linearized LQR is inadequate for the highly nonlinear, high-friction, long-pole regime; swing-up logic lacks smooth integration and doesn\u2019t prevent early failure beyond 1.0 rad.\n**Program Identifier:** Generation 116 - Patch Name fix_duplicate_init_remove_integral - Correct Program: False\n\n**Program Name: Phase-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Uses a three-phase control strategy (Recovery, Transition, Precision) with physics-informed LQR gains, adaptive swing-up logic, and anti-windup integral action for steady-state error correction.  \n- **Performance**: Achieved a high combined score of 4760.24 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps.  \n- **Feedback**: The aggressive recovery phase effectively handles large initial angles, while the gated integral term ensures precise stabilization without windup; high friction and pole mass make this a robust solution under challenging dynamics.\n**Program Identifier:** Generation 117 - Patch Name adaptive_lqr_with_integral_correction - Correct Program: True\n\n**Program Name: Adaptive Energy-Phase Inverted Pendulum Controller**  \n- **Implementation**: Combines LQR-based linear control with energy-aware swing-up logic, phase-specific gains, integral action for precision stabilization, and predictive damping to handle a highly unstable pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a high combined score of 4755.01 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps with minimal final errors.  \n- **Feedback**: The adaptive gain scheduling and phase-aware switching effectively managed the challenging dynamics, while integral control and predictive damping significantly improved steady-state accuracy and reduced overshoot near equilibrium.\n**Program Identifier:** Generation 118 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Uses an enhanced adaptive LQR controller with integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, and a swing-up assist activated during large deviations. The dynamics include high-friction, heavy/long pole parameters for increased difficulty.  \n- **Performance**: Achieved a combined score of 4927.10 with full success bonus, low energy usage (6.46 total), and stabilization after 191 steps.  \n- **Feedback**: The integral control effectively eliminated steady-state cart error when upright, while the swing-up logic and adaptive gains enabled recovery from highly unstable initial conditions (0.9 rad). Energy efficiency and stability were maintained despite aggressive physical challenges.\n**Program Identifier:** Generation 119 - Patch Name adaptive_lqr_integral_swingup - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-informed linear quadratic regulator (LQR) with adaptive gain scheduling and a smooth, momentum-based swing-up assist for large angles; includes soft-switched integral action to eliminate steady-state cart position error.  \n- **Performance**: Achieved a high combined score of 4920.25, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and perfect upright maintenance (final_theta_error: 0.00).  \n- **Feedback**: The swing-up assist effectively handles the challenging initial condition (0.9 rad) with a heavy, long pole, while adaptive gains and integral action improve robustness and precision without destabilizing the system.\n**Program Identifier:** Generation 120 - Patch Name increase_dtheta_q_weight - Correct Program: True\n\n**Program Name: Phase-Adaptive Discrete LQR Controller**  \n- **Implementation**: Uses discrete-time LQR with zero-order hold discretization, enhanced by phase-based swing-up assistance, adaptive gain scheduling, filtered state estimation, and integral control activated only during fine-tuning.  \n- **Performance**: Achieved a combined score of 0.0 and failed validation tests.  \n- **Feedback**: The controller\u2019s complex phase logic and energy-based swing-up may be misaligned with the highly unstable system dynamics (heavy, long pole); discretization via an incorrect matrix exponential approximation likely introduced instability, and aggressive clipping or filtering may degrade responsiveness.\n**Program Identifier:** Generation 121 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physics-informed linearized model for LQR control with adaptive gain scheduling based on pole angle and velocity, plus swing-up assistance for large deviations and leaky integral action for cart position regulation.  \n- **Performance**: Achieved a high combined score of 4920.25 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The controller effectively balances aggressive stabilization for large angles with precise upright maintenance, while friction-aware modeling and smooth gain transitions prevent instability and energy waste.\n**Program Identifier:** Generation 122 - Patch Name add_swing_up_assist_and_integral_control - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state error and a physics-informed swing-up assist that activates for large pole angles, using smooth gain scheduling via tanh-based activation functions.  \n- **Performance**: Achieved a high combined score of 4920.25, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging dynamics (heavy, long pole; high friction), while integral control ensured precise cart positioning once upright\u2014demonstrating robustness and efficiency in both transient and steady-state phases.\n**Program Identifier:** Generation 123 - Patch Name Add integral control for cart position and swing-up assist for large angles - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with adaptive gain scheduling based on angle and angular velocity, plus physics-informed swing-up assistance for large deviations.  \n- **Performance**: Achieved a high combined score of 4898.88 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 195 steps).  \n- **Feedback**: The controller effectively balances aggressive correction for large errors with energy-efficient fine control near equilibrium; the swing-up assist and multiplicative gain adaptation were key to handling the challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 124 - Patch Name discrete_lqr_design - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance using predictive angle estimation, adaptive gain scheduling via tanh activation, and soft-switched integral action for steady-state error correction. The controller uses accurate linearized dynamics matching the nonlinear simulator and includes friction-aware modeling.  \n- **Performance**: Achieved a high combined score of 4872.08 with strong time, energy, and success bonuses, stabilizing the pendulum in 199 steps with minimal final errors.  \n- **Feedback**: The swing-up assist effectively handles large initial angles (0.9 rad), while adaptive gains and integral action improve robustness and precision; low average energy per step (0.01) indicates efficient control despite challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 125 - Patch Name predictive_feedforward_swing - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Energy Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart positioning; uses physically accurate linearization and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4913.06 with full success bonus, low energy use (avg 0.01/step), and fast stabilization (193 steps).  \n- **Feedback**: The controller effectively handles the challenging pole parameters (heavy, long) by intelligently switching between swing-up and stabilization modes; integral control and adaptive gains ensure precise cart centering and robustness without excessive energy expenditure.\n**Program Identifier:** Generation 126 - Patch Name add_cart_vel_cross_coupling - Correct Program: True\n\n**Program Name: Discrete LQR with Energy-Aware Swing-Up**\n\n- **Implementation**: Uses a discrete-time LQR controller derived via zero-order hold to match Euler integration dynamics, enhanced with energy-based swing-up logic and adaptive integral control that decays aggressively when the pole is far from upright. The controller applies gain scheduling based on angle and angular velocity, and only activates swing-up assistance when energy falls below 85% of the upright potential energy.\n\n- **Performance**: Achieved a high combined score of 4892.07, with strong contributions from time bonus (2206.23), energy efficiency (2493.47), and full success bonus (800.00), stabilizing in 196 steps with near-zero final errors.\n\n- **Feedback**: The energy-aware swing-up prevents unnecessary force application while effectively recovering from large initial angles; adaptive integral decay and gain scheduling significantly improve robustness under challenging conditions (heavy, long pole with high friction).\n**Program Identifier:** Generation 127 - Patch Name discrete_lqr_energy_adaptive - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action for cart position, and physics-informed swing-up assistance triggered at large angles (>0.8 rad). Friction is accurately modeled in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4933.93 with excellent energy efficiency (avg_energy_per_step: 0.01) and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up assist effectively stabilized the highly challenging pendulum (long, heavy pole; large initial angle), while integral action ensured precise long-term positioning without compromising stability.\n**Program Identifier:** Generation 128 - Patch Name none - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) stabilization around the upright position with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart position error elimination; uses physically accurate linearization and directional momentum awareness to enhance swing-up efficiency.  \n- **Performance**: Achieved a high combined score of 4927.07 with full success bonus, low energy usage (total_energy=6.50), and fast stabilization (191 steps).  \n- **Feedback**: The controller effectively balances aggressive swing-up and precise stabilization, with integral action and adaptive gains contributing to zero final errors and robustness under challenging conditions (heavy/long pole, high friction).\n**Program Identifier:** Generation 129 - Patch Name directional_momentum_integral_control - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed gain scheduling, swing-up assistance for large angles, and soft-switched integral action to eliminate steady-state cart position error. The controller uses accurate linearized dynamics matching the simulator and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4920.25 with full success bonus, low energy use (total_energy=6.47), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging pole parameters (heavy, long, high friction), while integral control ensured precise upright stabilization without windup due to leaky integration and gating.\n**Program Identifier:** Generation 130 - Patch Name enhanced_swing_up_with_integral_control - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action gated by angle/velocity thresholds, and physics-informed swing-up assistance for large deviations. Friction-aware system modeling ensures accurate controller synthesis.  \n- **Performance**: Achieved a high combined score of 4926.67 with excellent energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up assist effectively handle the challenging heavy/long pole configuration, while gated integral action prevents destabilization during transients\u2014key to balancing speed, stability, and energy use.\n**Program Identifier:** Generation 131 - Patch Name joint_integral_gate_and_tuned_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed adaptive gain scheduling, swing-up assistance for large angles, and gated integral action for zero steady-state cart position error. The controller uses robust angle normalization, smooth activation functions (tanh), and leaky integration with angle-dependent decay.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and near-perfect stabilization.  \n- **Feedback**: The enhanced swing-up logic and adaptive gains effectively handled the challenging pole parameters (longer, heavier), while the gated integral term improved long-term positioning without destabilizing the system during large swings.\n**Program Identifier:** Generation 132 - Patch Name integral_windup_prevention_with_angle_velocity_gate - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines gain-scheduled LQR with robust swing-up assist, enhanced angular damping, and gated integral action that activates only near upright equilibrium to minimize drift.  \n- **Performance**: Achieved a high combined score of 4926.67 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00).  \n- **Feedback**: The controller effectively stabilizes a highly challenging pendulum (long, heavy, high friction) by blending aggressive swing-up logic with precise integral correction, though stabilization ratio remains low (0.19), indicating delayed convergence.\n**Program Identifier:** Generation 133 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model to compute LQR gains, enhanced with adaptive gain scheduling, robust angle normalization via `arctan2`, soft-switched integral control gated by pole angle and velocity, and a swing-up assist that activates based on normalized falling severity.  \n- **Performance**: Achieved a high combined score of 4926.64, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (stabilization_time: 191).  \n- **Feedback**: The integral action effectively eliminated steady-state cart position error without destabilizing transients, while the swing-up logic and adaptive gains enabled reliable recovery from large initial angles (0.9 rad) despite challenging pole parameters (longer and heavier than standard).\n**Program Identifier:** Generation 134 - Patch Name refined_integral_gating_with_velocity - Correct Program: True\n\n**Program Name: Phase-Based Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Uses a hybrid control strategy with energy-based swing-up for large angles, smooth transition blending, and LQR with integral action near equilibrium; includes friction modeling and Euler integration.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization despite high energy usage (total_energy: 101.95) and full simulation duration (stabilization_time: 1000).  \n- **Feedback**: The controller failed to stabilize the pendulum (final_theta_error: 2.95 rad), indicating ineffective phase transitions or insufficient robustness for the challenging pole parameters (longer, heavier, high friction).\n**Program Identifier:** Generation 135 - Patch Name none - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments and whitespace.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass any validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 136 - Patch Name None - Correct Program: False\n\n**Program Name: Energy-Shaping Hybrid Controller**  \n- **Implementation**: Combines energy-based swing-up control with LQR stabilization and smooth phase transitions based on angle, velocity, and energy thresholds; includes integral action with anti-windup and centrifugal compensation.  \n- **Performance**: Achieved a combined score of 34.79 but failed to stabilize the pendulum (success_bonus: 0.00), despite maintaining full simulation duration (stabilization_ratio: 1.00).  \n- **Feedback**: The controller could not balance the pole (final_theta_error: 3.12 rad), indicating ineffective transition or stabilization logic; high friction and aggressive pole parameters likely exposed weaknesses in the LQR design or phase-switching criteria.\n**Program Identifier:** Generation 137 - Patch Name energy_shaping_hybrid_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and soft-switched integral action to eliminate steady-state cart position error, using gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and perfect final pole alignment (final_theta_error: 0.00).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging high-mass, long-pole configuration, while integral control with angle/velocity gating prevented windup and improved precision without destabilizing the system.\n**Program Identifier:** Generation 138 - Patch Name enhanced_integral_suppression - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up**\n\n- **Implementation**: Uses an enhanced adaptive LQR controller featuring integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, and a swing-up assist activated during large deviations. Friction and nonlinear dynamics are explicitly modeled in simulation.\n\n- **Performance**: Achieved a high combined score of 4926.69 with full success bonus, low energy usage (total_energy=6.50), and perfect final stabilization (final_theta_error=0.00).\n\n- **Feedback**: The soft-switched integral control effectively eliminated steady-state cart drift without destabilizing transients, while the normalized swing-up logic provided reliable recovery from extreme angles. Optimized angular velocity damping (Q[3]=3.28) reduced oscillations and contributed to early stabilization (stabilization_time=191).\n**Program Identifier:** Generation 139 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up Assist**  \n- **Implementation**: Uses an enhanced adaptive Linear Quadratic Regulator (LQR) controller featuring integral action for steady-state cart position correction, robust angle normalization via `arctan2`, gain scheduling based on pole angle and angular velocity, and energy-aware swing-up assistance triggered when the pole is far from upright.  \n- **Performance**: Achieved a high combined score of 4906.68, with strong contributions from time bonus (2220.04), energy efficiency (2492.62), and full success bonus (800.00), stabilizing in 194 steps with near-zero final errors.  \n- **Feedback**: The integral control effectively eliminated steady-state cart drift without destabilizing the system due to intelligent gating by angle and velocity; the swing-up logic efficiently managed the challenging initial condition (0.9 rad) with a heavy, long pole, while adaptive gains improved phase-specific responsiveness.\n**Program Identifier:** Generation 140 - Patch Name energy_aware_swingup_assist - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in a complete lack of functionality for any intended algorithm or task.  \n- **Performance**: Combined score is 0.0, indicating failure to pass any validation tests.  \n- **Feedback**: An empty implementation cannot process inputs or produce outputs, leading to total failure in evaluation; actual logic must be implemented to address the problem requirements.\n**Program Identifier:** Generation 141 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced with adaptive gain scheduling, predictive correction using angular acceleration, physics-informed swing-up assistance for large angles, and gated integral control for cart position.  \n- **Performance**: Achieved a high combined score of 4787.89 with strong time, energy, and success bonuses, stabilizing in 211 steps with near-zero final errors.  \n- **Feedback**: The controller effectively balances aggressive early-phase control with energy-efficient stabilization; the swing-up assist and adaptive gains are critical for handling the challenging pole parameters (heavy, long, high friction), while integral action with gating prevents windup and improves steady-state accuracy.\n**Program Identifier:** Generation 142 - Patch Name aggressive_early_phase_control - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines gain-scheduled LQR with robust integral action, enhanced angular damping, and physics-informed swing-up assistance; uses precise linearized dynamics matching the simulator\u2019s friction model and adaptive gains based on pole angle/velocity.  \n- **Performance**: Achieved a high combined score of 4926.94 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large angles with fine stabilization near upright, while leaky integral action and soft-switched damping reduce overshoot and improve cart centering without destabilizing the pole.\n**Program Identifier:** Generation 143 - Patch Name optimized_integral_and_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and integral action for cart position regulation, using gain scheduling and energy-based scaling to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a high combined score of 4907.13 with full success bonus, low energy usage, and stable final state.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large deviations with precise LQR stabilization near upright, while integral action eliminates steady-state position error without windup due to soft gating and leaky integration.\n**Program Identifier:** Generation 144 - Patch Name energy_aware_swing_up_assist - Correct Program: True\n\n**Program Name: Phase-Based Adaptive Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-switching strategy with four LQR controllers (swing-up, transition, balancing, fine-tune), energy shaping for swing-up, integral action with windup protection, predictive compensation, and nonlinear damping.  \n- **Performance**: Combined score of 0.0; fails to stabilize the pendulum under challenging initial conditions and system parameters.  \n- **Feedback**: Despite sophisticated control logic, the controller struggles with the highly unstable configuration (long, heavy pole; high friction; large initial angle), likely due to inadequate swing-up energy injection or poor phase transition timing.\n**Program Identifier:** Generation 145 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with adaptive gain scheduling based on angle and angular velocity, enhanced by soft-switched integral action and energy-aware swing-up assistance.  \n- **Performance**: Achieved a high combined score of 4907.12 with strong time, energy, and success bonuses, stabilizing in 194 steps with minimal final errors.  \n- **Feedback**: The controller effectively balances aggressive stabilization for large deviations with energy efficiency near equilibrium; the inclusion of friction in the linearized model and intelligent gain adaptation were key to handling the challenging heavy/long pole configuration.\n**Program Identifier:** Generation 146 - Patch Name sharpen_gain_transitions - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, robust swing-up assistance based on mechanical energy deficit, and gated integral action for cart position stabilization. The controller uses a physically accurate linearized model and includes enhanced damping near equilibrium.  \n- **Performance**: Achieved a high combined score of 4926.90, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 191), and full success bonus.  \n- **Feedback**: The hybrid approach effectively handles the challenging dynamics of a heavy, long pole with high friction, demonstrating that adaptive gains and energy-based swing-up significantly improve robustness and stabilization speed, while leaky integral action prevents windup without degrading performance.\n**Program Identifier:** Generation 147 - Patch Name energy_based_swing_up - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, robust swing-up assistance based on falling severity, and soft-switched integral action for cart position regulation. The controller uses a physically accurate linearized model and dynamically adjusts gains based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4933.93, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (190 steps), and full success bonus.  \n- **Feedback**: The hybrid approach effectively stabilizes a highly challenging inverted pendulum (long, heavy pole; high friction; large initial angle). Integral action and swing-up logic are key to maintaining stability and achieving near-zero final errors, while adaptive gains prevent overreaction during large deviations.\n**Program Identifier:** Generation 148 - Patch Name simplified_integral_gating_with_optimal_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control derived from physically accurate linearized dynamics with adaptive gain scheduling, swing-up assistance based on normalized angular momentum, and dual-gated integral action that activates only near upright and low-velocity conditions.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192 steps), and full success bonus.  \n- **Feedback**: The hybrid approach effectively handles the challenging pole parameters (heavy, long, high friction), with adaptive gains and swing-up logic enabling robust recovery from large initial angles, while gated integral control prevents windup and improves steady-state accuracy.\n**Program Identifier:** Generation 149 - Patch Name adaptive_lqr_integral_swingup_enhanced - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and gated integral action to eliminate steady-state cart position error; uses adaptive gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and full success bonus.  \n- **Feedback**: The enhanced swing-up logic effectively handles the challenging initial conditions (heavy, long pole with large initial angle), while the soft-gated integral term prevents windup and improves final positioning without destabilizing the system.\n**Program Identifier:** Generation 150 - Patch Name adaptive_energy_aware_swingup - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Uses a refined LQR controller with gain scheduling, angle-only integral gating, and swing-up assistance for large deviations; includes physically accurate linearization and friction-aware dynamics.  \n- **Performance**: Achieved a high combined score of 4920.84 with full success bonus, low energy use (6.58 total), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains and simplified integral control effectively balanced stability and efficiency, while the swing-up logic enabled recovery from large initial angles (0.9 rad); tight anti-windup and decay prevented integral windup without sacrificing responsiveness.\n**Program Identifier:** Generation 151 - Patch Name refined_adaptive_lqr_optimized - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control and Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with gain scheduling, integral action for cart position regulation, and physics-informed swing-up assistance triggered by pole angle and falling dynamics. Uses accurate friction modeling and Euler integration for simulation.  \n- **Performance**: Achieved a high combined score of 4934.54 with full success bonus, low energy use (6.61 total), and fast stabilization (190 steps).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle large initial angles and system instability from heavy/long poles, while integral control ensures precise long-term positioning without windup due to velocity-gated activation.\n**Program Identifier:** Generation 152 - Patch Name sharper_gain_transitions_and_velocity_gated_integral - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Integral Action**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart position stabilization; uses physically accurate dynamics and directional momentum awareness to enhance recovery during large-angle excursions.  \n- **Performance**: Achieved a high combined score of 4933.59 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 190 steps), and perfect final state accuracy.  \n- **Feedback**: The controller effectively balances aggressive swing-up and precise stabilization, with integral action eliminating steady-state cart error without windup; directional momentum logic prevents overshoot during recovery, contributing to robustness under challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 153 - Patch Name enhanced_integral_control_with_dynamic_gating - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state cart position error, adaptive gain scheduling via tanh-based angle/velocity thresholds, and physics-informed swing-up assist using energy deficit and angular momentum cues.  \n- **Performance**: Achieved a high combined score of 4926.95 with strong stability (final theta error \u2248 0), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The integration of swing-up logic enabled recovery from large initial angles (0.9 rad), while adaptive gains and integral control ensured precise stabilization; energy-efficient actuation and leaky integration prevented windup and maintained robustness under high-friction, heavy-pole conditions.\n**Program Identifier:** Generation 154 - Patch Name energy_scaled_swing_up_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state error and a physics-informed swing-up assist that activates for large pole angles (>0.8 rad), using smooth tanh-based gain scheduling and dual-gated integral feedback.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00).  \n- **Feedback**: The adaptive gains and swing-up logic enabled rapid recovery from large initial angles (0.9 rad), while integral control ensured precise stabilization; the design effectively balanced aggressive correction with energy conservation under high-friction, unstable dynamics.\n**Program Identifier:** Generation 155 - Patch Name enhanced_integral_control_with_velocity_gating_and_adaptive_decay - Correct Program: True\n\n**Program Name: Phase-Aware Adaptive LQR Controller**  \n- **Implementation**: Uses three phase-specific LQR controllers (swing-up, transition, stabilize) combined with energy-based swing-up logic and adaptive integral action with anti-windup. Angle normalization and momentum-aware control enhance robustness under high-friction, heavy-pole conditions.  \n- **Performance**: Achieved a combined score of 4478.10 with full success bonus, low final errors, and efficient energy use (avg 0.01 per step).  \n- **Feedback**: The phase-adaptive strategy effectively handles the challenging dynamics (long, heavy pole; high friction), enabling rapid stabilization (253 steps) and maintaining balance with minimal energy expenditure. Integral action and smooth phase transitions prevent windup and ensure precision.\n**Program Identifier:** Generation 156 - Patch Name none - Correct Program: True\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with cross-coupled state weighting, adaptive gain scheduling, energy-based swing-up assistance for large angles, and enhanced integral action with angle/velocity gating and decay.  \n- **Performance**: Achieved a high combined score of 4953.84, with strong stabilization (final theta error \u2248 0), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The energy-aware swing-up and adaptive gains effectively handled the challenging heavy/long pole, while integral control with tight gating improved cart centering without destabilizing the system near equilibrium.\n**Program Identifier:** Generation 157 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Energy-Based Swing-Up**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, dual-gated integral action for cart position, and energy-based swing-up assistance triggered by large pole angles. The controller uses physically accurate linearization and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4947.50, with strong time, energy, and success bonuses, stabilizing the pendulum in 188 steps despite extreme parameters (heavy, long pole; high friction).  \n- **Feedback**: The energy-based swing-up and adaptive gains effectively handled large initial deviations, while dual-gated integral control improved steady-state cart positioning without destabilizing the pole. Evaluation confirms robustness and efficiency under challenging dynamics.\n**Program Identifier:** Generation 158 - Patch Name adaptive_lqr_integral_v2 - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in a complete lack of functionality for any intended algorithm or task.  \n- **Performance**: Combined score is 0.0, indicating failure to pass any validation tests.  \n- **Feedback**: An empty implementation cannot perform computation or meet problem requirements; adding core logic is essential for correctness and scoring.\n**Program Identifier:** Generation 159 - Patch Name energy_coupled_lqr - Correct Program: False\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, predictive falling severity estimation, and enhanced integral action with state-dependent gating and decay. The controller uses a physically accurate linearized model and cross-coupled Q-matrix terms for coordinated cart-pole dynamics.  \n- **Performance**: Achieved a high combined score of 4953.84, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 187 steps), and full success bonus.  \n- **Feedback**: The energy-aware swing-up logic and adaptive gains effectively handled large initial angles and system instability from the heavy, long pole. Integral control with tight velocity/angle gating prevented windup while improving steady-state cart positioning, contributing to low final errors and high time/energy bonuses.\n**Program Identifier:** Generation 160 - Patch Name enhanced_swingup_with_momentum_prediction - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines gain-scheduled LQR control with adaptive integral action, enhanced angular damping, and physics-informed swing-up assistance to stabilize a challenging inverted pendulum (heavy, long pole with high friction).  \n- **Performance**: Achieved a high combined score of 4933.93, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00), stabilizing in 190 steps.  \n- **Feedback**: The adaptive gains and integral clamping effectively handled large initial angles and high system instability, while the swing-up logic provided critical recovery during aggressive falls\u2014key to robust performance under extreme parameters.\n**Program Identifier:** Generation 161 - Patch Name adaptive_integral_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines physically accurate linearized dynamics with gain-scheduled LQR, swing-up assistance for large angles, and gated integral action with anti-windup to handle a challenging high-friction, long/heavy-pole inverted pendulum.  \n- **Performance**: Achieved a high combined score of 4920.84 with full success bonus, low energy use (6.58 total), and fast stabilization (192 steps).  \n- **Feedback**: The sharpened gain scheduling and angle-gated integral control effectively balanced aggressive swing-up and precise stabilization, while the tuned LQR weights (notably Q[3]=3.2) and friction-aware modeling were critical to robustness under difficult physical parameters.\n**Program Identifier:** Generation 162 - Patch Name adaptive_integral_lqr_hybrid - Correct Program: True\n\n**Program Name: Adaptive Energy-Aware LQR with Swing-Up Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, directional momentum-aware swing-up logic, and soft-switched integral action for cart position, all tuned for extreme pendulum parameters (heavy, long pole; high friction).  \n- **Performance**: Achieved a high combined score of 4927.07, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and stable final state.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large deviations with precise LQR stabilization near upright, while integral action eliminates steady-state position error without causing overshoot\u2014key to handling the challenging system dynamics.\n**Program Identifier:** Generation 163 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Energy-Based Swing-Up**\n\n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, dual-gated integral action for cart position, predictive momentum compensation, and energy-based swing-up assistance tailored to extreme pendulum parameters (heavy, long pole with high friction).\n\n- **Performance**: Achieved a high combined score of 4947.47, with strong bonuses for success (800), time (2261), and energy efficiency (2492), stabilizing in 188 steps with near-zero final errors.\n\n- **Feedback**: The controller effectively handles challenging initial conditions and system dynamics through intelligent nonlinear enhancements; the energy-based swing-up and adaptive gains were critical for rapid stabilization and robustness under high friction and instability.\n**Program Identifier:** Generation 164 - Patch Name predictive_momentum_compensation - Correct Program: True\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance, predictive falling severity estimation, and adaptive integral action that activates only near equilibrium; includes cross-coupled terms in the Q-matrix and state-dependent gain scheduling for robustness.  \n- **Performance**: Achieved a high combined score of 4953.83 with strong time, energy, and success bonuses, stabilizing the pendulum in 187 steps with minimal final errors.  \n- **Feedback**: The controller effectively handles the challenging pole parameters (longer, heavier) by blending aggressive swing-up logic with precise stabilization; integral control and predictive momentum compensation significantly improve centering and robustness without excessive energy use.\n**Program Identifier:** Generation 165 - Patch Name predictive_momentum_compensation - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action gated by pole angle/velocity, and energy-based swing-up assistance for large deviations. Uses physically accurate friction modeling in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4926.68 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle the challenging heavy/long pole configuration, while gated integral action prevents destabilization during transients\u2014key to balancing speed and stability.\n**Program Identifier:** Generation 166 - Patch Name enhanced_swing_up_with_energy_awareness - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines physically accurate linearized dynamics with LQR control (Q[3]=3.2), gain scheduling based on angle/velocity thresholds, energy-aware swing-up logic, and gated integral action with anti-windup for cart centering.  \n- **Performance**: Achieved a high combined score of 4920.85 with full success bonus, low energy use (avg 0.01/step), and rapid stabilization (192 steps).  \n- **Feedback**: The sharpened gain scheduling and energy-aware swing-up effectively handled the challenging heavy/long pole, while the simplified integral gating proved robust for steady-state accuracy without windup issues.\n**Program Identifier:** Generation 167 - Patch Name enhanced_swing_up_with_energy_awareness - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control and Swing-Up**\n\n- **Implementation**: Uses an enhanced adaptive LQR controller with integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, swing-up assistance triggered at large angles, and mid-swing angular damping. The dynamics include high-friction terms and a heavy, long pole to increase difficulty.\n\n- **Performance**: Achieved a combined score of 0.00 despite a base score of 7.16, failing to stabilize the pendulum (final theta error: 2.94 rad, final x error: 4432 m) over 1000 steps.\n\n- **Feedback**: The controller fails to stabilize the system due to excessive drift in cart position and inability to recover from large initial angles; integral action may be too weak or improperly gated, and swing-up logic might interfere with stabilization rather than assist it.\n**Program Identifier:** Generation 168 - Patch Name sharper_gain_scheduling - Correct Program: False\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines LQR with velocity-gated integral action, gentle gain scheduling, and swing-up assistance for large angles; uses physically accurate linearized dynamics and proven Q/R weights.  \n- **Performance**: Achieved a high combined score of 4954.47 with full success bonus, low energy use (6.51 total), and fast stabilization (187 steps).  \n- **Feedback**: The hybrid approach effectively balances aggressive swing-up for large deviations with precise LQR control near upright, while velocity-gated integration prevents windup and improves stability.\n**Program Identifier:** Generation 169 - Patch Name adaptive_integral_lqr_hybrid - Correct Program: True\n\n**Program Name: Adaptive Cross-Coupled LQR Controller**  \n- **Implementation**: Uses an adaptive LQR with state-dependent cross-coupling in the Q-matrix, mid-swing angular damping, predictive momentum compensation, and dual-gated integral control to stabilize a highly unstable inverted pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a combined score of 5015.73 with full success bonus, low energy use (7.54 total), and fast stabilization (178 steps).  \n- **Feedback**: The controller effectively handles large initial angles and aggressive dynamics through adaptive gains and predictive corrections; the dual-gated integral term prevents windup while aiding precision near upright equilibrium.\n**Program Identifier:** Generation 170 - Patch Name adaptive_cross_coupled_lqr_with_midswing_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with gain scheduling, robust swing-up logic based on energy deficit and momentum prediction, velocity-gated integral action for cart centering, and enhanced damping near equilibrium. The controller uses a physically accurate linearized model and adaptive gains that increase responsiveness during large deviations.  \n- **Performance**: Achieved a high combined score of 4756.64 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps with minimal final errors.  \n- **Feedback**: The hybrid approach effectively handles the challenging dynamics (heavy, long pole; high friction), with integral action and swing-up assist crucial for recovery and stabilization. Energy-efficient control and precise gain scheduling contributed to low average energy use and fast settling.\n**Program Identifier:** Generation 171 - Patch Name none - Correct Program: True\n\n**Program Name: Phase-Aware Momentum-Cancellation LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with phase-based adaptive gains, momentum-aware integral action, and energy-injection recovery logic tailored to a highly unstable inverted pendulum (heavy, long pole with high friction). Uses stability-index-driven phase blending, predictive feedforward, and dual-gated integrals for precision stabilization.  \n- **Performance**: Achieved a high combined score of 4967.97, with strong time, energy, and success bonuses, stabilizing the system in 185 steps with near-zero final errors.  \n- **Feedback**: The controller effectively handles extreme initial conditions and physical challenges through its hybrid strategy; evaluation confirms robustness, efficient energy use, and precise stabilization, validating the novel momentum-integral and phase-aware design choices.\n**Program Identifier:** Generation 172 - Patch Name phase_aware_momentum_lqr - Correct Program: True\n\n**Program Name: Hybrid Adaptive Cross-Coupled LQR Controller**  \n- **Implementation**: Combines cross-coupled LQR with mid-swing damping, sharpened gain scheduling, predictive momentum compensation, and angle-gated integral control to stabilize a challenging inverted pendulum (heavy, long pole; high friction; large initial angle).  \n- **Performance**: Achieved a high combined score of 4983.31 with full success bonus, low energy use (7.54 total), and fast stabilization (183 steps).  \n- **Feedback**: The controller effectively balances aggressive swing-up for large angles with precise linear regulation near upright, while adaptive gains and predictive terms prevent overshoot and divergence; integral gating avoids windup during transients.\n**Program Identifier:** Generation 173 - Patch Name adaptive_crosscoupled_lqr_with_midswing_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Predictive Damping**\n\n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, predictive angular acceleration estimation for mid-swing damping, energy-aware integral action, and swing-up assistance for large angles. Uses physically accurate linearized dynamics and friction-aware modeling.\n\n- **Performance**: Achieved a high combined score of 4934.14, with strong time, energy, and success bonuses, stabilizing the pendulum in 190 steps (19% of max time) and maintaining near-zero final errors.\n\n- **Feedback**: The integration of predictive damping and energy-based integral gain adaptation significantly improved robustness under challenging conditions (heavy, long pole; high friction). Early stabilization and low energy usage indicate efficient, proactive control that balances swing-up and balancing phases effectively.\n**Program Identifier:** Generation 174 - Patch Name enhanced_adaptive_lqr_with_predictive_damping - Correct Program: True",
        "meta_scratch_pad": "## Successful Algorithmic Patterns\n\n- **State-dependent cross-coupling in the LQR Q-matrix significantly improves coordination between angle and angular velocity**, enabling faster stabilization under high-inertia conditions. The current best program (*Adaptive Cross-Coupled LQR Controller*, **5015.73**) introduces off-diagonal terms `Q[1,3] = Q[3,1] = 0.8`, dynamically boosted by `cross_coupling_boost = 1.0 + 0.5 * tanh(3.0 * abs(theta))`. This yields the fastest stabilization time (**178 steps**) among all evaluated programs\u2014beating even the previously top-performing Generation 165 (**187 steps**)\u2014and contributes directly to its record **time_bonus: 2328.30**.\n\n- **Gaussian-shaped mid-swing damping provides targeted momentum suppression without destabilizing near-equilibrium control**. The current best uses `mid_swing_activation = exp(-4.0 * (abs(theta) - 0.5)**2)` to apply damping only in the critical **0.25\u20130.85 rad** range, avoiding interference with swing-up or balancing phases. This contrasts with blunt constant damping and explains why this program achieves **stable_x_error: 0.05** despite aggressive swing-up, outperforming *Hybrid Adaptive Integral LQR* (**4756.64**) which lacks such phase-specific damping.\n\n- **Predictive momentum compensation using estimated angular acceleration enables earlier counteraction of divergence**, especially crucial for the heavy, long pole (`M_POLE=0.35`, `L_POLE=2.5`). The current best estimates `theta_acc_est` from gravity and joint friction, then computes `pred_divergence = theta * dtheta_pred / (omega_n * L_COM)` to generate a corrective term. This proactive strategy\u2014absent in Generations 171 and 174\u2014contributes to its near-zero final errors (**final_theta_error: 0.00**, **final_x_error: 0.00**) and supports its elite energy efficiency (**total_energy: 7.54**) by reducing corrective overactuation.\n\n- **Dual-gated integral control with exponential decay during inactivity maintains precision without windup**, building on prior successful patterns but refined for extreme dynamics. The current best uses identical gating (`angle_gate = tanh(12.0*(0.1\u2212|\u03b8|))`, `velocity_gate = tanh(8.0*(1.0\u2212|d\u03b8|))`) as the previous best (**4954.47**), but couples it with **integral decay (`*= 0.95`)** when inactive\u2014a technique validated in Generation 167. This synergy ensures rapid reactivation near equilibrium, achieving **stable_x_error: 0.05**, matching the precision of top predecessors while operating under more challenging physical parameters.\n\n## Ineffective Approaches\n\n- **Omission of predictive or mid-swing damping mechanisms leads to momentum overshoot and slower stabilization**, even with otherwise strong LQR foundations. *Hybrid Adaptive Integral LQR Controller* (**4756.64**)\u2014despite excellent integral gating and gain scheduling\u2014stabilizes in **215 steps**, **37 steps slower** than the current best, due to lack of explicit mid-phase damping or predictive correction. This confirms that reactive LQR alone is insufficient for the heaviest/longest poles tested.\n\n- **Swing-up logic that lacks falling-severity normalization wastes energy and delays transition to balancing**. While all top programs use swing-up, the current best\u2019s use of `falling_factor = 1.0 + tanh(2.0 * theta * dtheta / (L_COM * omega_n))` makes assistance directionally intelligent. In contrast, *Adaptive LQR with Predictive Damping* (**4934.14**) uses less refined swing-up, resulting in **190-step stabilization**\u20147 steps slower than the current best\u2014indicating that unnormalized swing-up is suboptimal under high inertia.\n\n- **Static or weakly adaptive cross-coupling fails to exploit state-dependent coordination benefits**. Earlier correct programs like Generation 171 (**4756.64**) and 172 (**4967.97**) omit cross-coupled Q-terms entirely, relying solely on diagonal weights and gain scheduling. Their scores\u2014while high\u2014are consistently **>48 points below** the current best (**5015.73**), demonstrating that cross-coupling is not merely optional but a key differentiator at the performance frontier.\n\n## Implementation Insights\n\n- **The current best program integrates four synergistic layers\u2014adaptive cross-coupled LQR, Gaussian mid-swing damping, predictive momentum compensation, and dual-gated integral control\u2014into a single coherent architecture**, each layer addressing a distinct phase of the control challenge. Unlike piecemeal approaches, this holistic design ensures smooth transitions: swing-up activates only beyond **0.8 rad**, mid-swing damping peaks at **0.5 rad**, predictive correction scales with divergence, and integral action engages only within **0.1 rad** and low velocity. This layered phase-awareness is absent in all lower-scoring programs.\n\n- **Precise friction-aware linearized dynamics in the A-matrix are non-negotiable for zero final angle error under high joint friction (`FRICTION_JOINT=0.25`)**. The current best explicitly models joint friction in `A[3,3] = -b_j / (m*l*denom0)`, matching the structure used in all correct programs. This fidelity\u2014combined with the proven Q-matrix `diag([4.5, 44.0, 0.6, 3.2])`\u2014enables perfect upright stabilization (**final_theta_error: 0.00**), whereas even minor model inaccuracies would cause residual oscillation under these extreme parameters.\n\n- **Force clipping redundancy (inside controller and simulation loop) ensures robustness during aggressive swing-up maneuvers**, such as the current best\u2019s `u_swing = 8.0 * ...` term. This dual safeguard prevents numerical overflow and guarantees actuator compliance, directly enabling its full **success_bonus: 800.00** across all test conditions\u2014a pattern consistently present in all correct programs but missing in failed ones like Generation 168.\n\n- **Natural frequency normalization (`omega_n = sqrt(G / L_COM)`) enables scalable, physics-informed decision-making across varying pole lengths and masses**. The current best uses `omega_n` to normalize both predictive divergence (`theta * dtheta_pred / (omega_n * L_COM)`) and falling severity, ensuring control intensity adapts to the actual dynamic timescale. This explains its ability to maintain **avg_energy_per_step: 0.01**\u2014matching saturated elite efficiency\u2014while stabilizing **9 steps faster** than Generation 174, which uses similar but less normalized predictive logic.\n\n## Performance Analysis\n\n- **The current best program (**5015.73**) sets a new performance ceiling, surpassing all predecessors by \u226532 points**, primarily through unprecedented speed (**178 steps**) rather than energy savings (which remain saturated at ~7.5 total). Its **time_bonus: 2328.30** exceeds the previous best (**2267.91**) by **60.39 points**, confirming that **swing-up and mid-phase efficiency\u2014not just balancing precision\u2014is now the dominant scoring factor** under extreme physical parameters.\n\n- **Cross-coupled LQR with predictive damping outperforms hybrid integral-LQR and phase-aware variants**, despite their sophistication. *Phase-Aware Momentum-Cancellation LQR* (**4967.97**) and *Hybrid Adaptive Cross-Coupled LQR* (**4983.31**) both score **>32 points below** the current best, indicating that **explicit cross-coupling in the cost function combined with Gaussian mid-swing damping provides superior transient control** compared to phase-blending or sharpened gain scheduling alone.\n\n- **Energy efficiency has plateaued across all correct implementations (total_energy \u2248 6.5\u20137.54)**, making time-to-stabilization the sole meaningful differentiator above **4920**. The current best\u2019s **178-step stabilization**\u2014**7\u201337 steps faster** than peers\u2014accounts for nearly all of its score advantage, as all top programs earn the same **success_bonus: 800.00** and achieve near-identical final errors.\n\n- **The combination of predictive compensation and mid-swing damping uniquely enables both fast swing-up and minimal overshoot**, resolving a key trade-off that limited earlier controllers. Programs without both features\u2014e.g., Generation 171 (no predictive/mid-swing) or Generation 174 (predictive but no Gaussian damping)\u2014show measurably slower convergence, proving that **proactive momentum management across the entire swing-up trajectory is essential for record performance**.",
        "embed_cost": 0.0,
        "novelty_cost": 0.0,
        "stdout_log": "Evaluating program: results_pendulum_liter/gen_176/main.py\nSaving results to: results_pendulum_liter/gen_176/results\nEvaluation error: [Errno 2] No such file or directory: '/home/lance/Documents/sakana/shinka/ShinkaEvolve/examples/pendulum_liter/results_pendulum_liter/gen_176/main.py'\nCorrectness and error status saved to results_pendulum_liter/gen_176/results/correct.json\nMetrics saved to results_pendulum_liter/gen_176/results/metrics.json\nEvaluation or Validation failed: [Errno 2] No such file or directory: '/home/lance/Documents/sakana/shinka/ShinkaEvolve/examples/pendulum_liter/results_pendulum_liter/gen_176/main.py'\nMetrics:\n  combined_score: 0.0\n  execution_time_mean: 0.0\n  execution_time_std: 0.0\n  num_successful_runs: 0\n  num_valid_runs: 0\n  num_invalid_runs: 0\n  all_validation_errors: []\n",
        "stderr_log": "",
        "code_analysis_metrics": {
          "cyclomatic_complexity": 0,
          "average_cyclomatic_complexity": 0,
          "halstead_volume": 0,
          "halstead_difficulty": 0,
          "halstead_effort": 0,
          "lines_of_code": 0,
          "logical_lines_of_code": 0,
          "comments": 0,
          "maintainability_index": 171.0,
          "max_nesting_depth": 0,
          "complexity_score": 0.0
        }
      },
      "in_archive": false
    },
    {
      "id": "272630ea-0854-47f1-94e7-f58544dd998f",
      "code": "",
      "language": "python",
      "parent_id": "738b86a0-a93c-489b-af98-690c689ef5a2",
      "archive_inspiration_ids": [
        "cf208ebc-01a7-4a3b-8051-e3fbe4c0dccf",
        "c1c49a4d-70c9-4bed-8939-4d8b31aacd11",
        "f6d4f824-3717-4cd1-891c-5c995b9545b7",
        "1be84b3f-242f-4dd7-8277-fad2553a48cc"
      ],
      "top_k_inspiration_ids": [
        "ce1336dc-62df-48df-8593-ee6b652da3bf",
        "d37fa0dd-a945-435f-9512-1e41054e7d5b",
        "fba93c49-b8aa-4513-b1f1-1f72fc863346"
      ],
      "island_idx": 1,
      "generation": 177,
      "timestamp": 1764096776.7764761,
      "code_diff": null,
      "combined_score": 0.0,
      "public_metrics": {},
      "private_metrics": {},
      "text_feedback": "",
      "correct": false,
      "children_count": 0,
      "complexity": 0.0,
      "embedding": [],
      "embedding_pca_2d": [],
      "embedding_pca_3d": [],
      "embedding_cluster_id": null,
      "migration_history": [],
      "metadata": {
        "compute_time": 189.14793586730957,
        "patch_type": "full",
        "api_costs": 0,
        "num_applied": 0,
        "patch_name": null,
        "patch_description": null,
        "error_attempt": "LLM response content was None.",
        "novelty_attempt": 1,
        "resample_attempt": 3,
        "patch_attempt": 2,
        "model_name": "glm-4.6",
        "temperature": 0.5,
        "max_output_tokens": 4096,
        "llm_result": null,
        "diff_summary": {},
        "meta_recommendations": "1. **Enhance predictive momentum compensation with joint-friction-aware angular acceleration estimation**: Replace the current simplified `theta_acc_est` with a full friction-inclusive model that accounts for both cart and joint friction terms in the denominator, mirroring the precision of the A-matrix used in LQR design. This would align predictive correction with the high-fidelity dynamics already proven essential for zero final error under `FRICTION_JOINT=0.25`, potentially enabling even earlier divergence counteraction and shaving 2\u20133 steps off the 178-step stabilization time.\n\n2. **Introduce state-dependent mid-swing damping gain modulated by predicted energy deficit**: Scale the `K_d_midswing = 2.5` coefficient using the normalized mechanical energy: `K_d = 2.5 * (1.0 + 0.4 * max(0, 1 - E_current / E_max))`, where `E_max = M_POLE*G*L_COM`. This leverages the Gaussian damping\u2019s phase specificity while making its intensity responsive to swing-up progress\u2014applying stronger damping when the pole is under-energized and prone to stall, which could reduce overshoot variability across extreme initial conditions without compromising balancing precision.\n\n3. **Make cross-coupling boost adaptive to both angle and angular velocity magnitude**: Extend the current `cross_coupling_boost = 1.0 + 0.5 * tanh(3.0 * abs(theta))` to include velocity dependence: `boost = 1.0 + 0.5 * tanh(3.0 * abs(theta)) * (1.0 + 0.3 * tanh(2.0 * abs(dtheta)))`. Since high `dtheta` during mid-swing exacerbates coordination challenges, this dual-state adaptation would intensify theta-dtheta coupling precisely when momentum is highest, improving transient response during the critical 0.3\u20130.8 rad transition zone identified as pivotal for record time_bonus.\n\n4. **Implement exponential decay on the integral state only when outside both gating thresholds**, rather than applying decay whenever the gate product is low. Specifically, decay `self.integral_x *= 0.95` only if `abs(theta) > 0.12 OR abs(dtheta) > 1.1`, preserving accumulated correction during brief excursions beyond equilibrium. This refines the current dual-gating strategy to better handle high-inertia recovery transients, potentially reducing `stable_x_error` below 0.05 while maintaining windup protection\u2014addressing the last remaining precision gap among elite controllers.\n\n5. **Replace the fixed Gaussian center (0.5 rad) in mid-swing damping with an inertia-scaled peak location**: Set the activation center as `theta_peak = 0.5 * np.sqrt(L_COM / 2.5)` so the damping maximum adapts to the actual pole length, consistent with the `omega_n` normalization already used in predictive logic. Given that longer poles exhibit delayed momentum peaks, this ensures damping aligns with the true high-risk region of the swing-up trajectory, preventing premature or late intervention that could otherwise waste energy or permit overshoot under extreme `L_POLE=2.5` conditions.",
        "meta_summary": "**Program Name: Suboptimal LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with conservative LQR gains (low Q, high R) to stabilize a challenging inverted pendulum system with heavy/long pole and high friction; control actions are clipped to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 3195.17, with strong energy efficiency but slow stabilization (459 steps, 46% ratio).  \n- **Feedback**: The overly conservative LQR tuning results in sluggish response and poor error correction early on, limiting time and precision bonuses despite eventual stability and low energy use.\n**Program Identifier:** Generation 0 - Patch Name initial_program - Correct Program: False\n\n**Program Name: Suboptimal LQR with Nonlinear Compensation**  \n- **Implementation**: Uses a linearized LQR controller with aggressive Q/R weights combined with gravity and damping compensation terms to stabilize a highly unstable inverted pendulum (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 0.00 due to large final position/angle errors despite running all 1000 steps.  \n- **Feedback**: The controller fails to stabilize the system\u2014final cart position diverges drastically (>4000 m) and pole angle remains far from upright (~2.94 rad), indicating poor control authority or incorrect linearization assumptions under large deviations.\n**Program Identifier:** Generation 1 - Patch Name tune_lqr_nonlinear_comp - Correct Program: False\n\n**Program Name: Sliding Mode Energy Controller**\n\n- **Implementation**: Combines energy-based swing-up with sliding mode stabilization, featuring adaptive gain scheduling, friction compensation, and smooth sign approximation to reduce chattering; uses hybrid control modes based on pole angle magnitude.\n\n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilizing the pendulum (final theta error = 2.52 rad), despite moderate energy efficiency (avg_energy_per_step = 0.35) and full episode length (stabilization_time = 1000).\n\n- **Feedback**: The controller fails to stabilize the pendulum within the required bounds (|theta| \u2264 1.0 rad), indicating insufficient robustness or incorrect tuning for the highly challenging system parameters (heavy, long pole with high friction); energy shaping may dominate over stabilization near upright, preventing convergence.\n**Program Identifier:** Generation 2 - Patch Name sliding_mode_energy_shaping_hybrid - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized continuous-time LQR controller with aggressive Q/R weights on a highly unstable pendulum (long, heavy pole; high friction), but applies it directly to the nonlinear system without accounting for large-angle deviations or proper state wrapping in dynamics.\n- **Performance**: Achieves full 1000-step simulation (stabilization_ratio=1.0) but with very poor control accuracy (final_theta_error=3.11 rad, final_x_error=4433 m) and low combined score (0.00).\n- **Feedback**: Despite running to completion, the controller fails to stabilize the pendulum near upright due to mismatch between linear LQR assumptions and highly nonlinear, challenging dynamics; large steady-state errors indicate inadequate gain tuning or linearization validity.\n**Program Identifier:** Generation 3 - Patch Name correct_linearization_tune_weights - Correct Program: False\n\n**Program Name: Hybrid Energy-LQR Inverted Pendulum Controller**  \n- **Implementation**: Combines energy-based swing-up control for large angles with a nonlinear LQR stabilizer using state-dependent gain scheduling; includes friction modeling and Euler integration.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum within limits (final theta error: 2.52 rad, exceeded 1.0 rad threshold).  \n- **Feedback**: Despite sophisticated hybrid control design, the controller fails to recover from the aggressive initial condition (0.9 rad) given the heavy, long pole and high friction; likely issues include inaccurate linearization in LQR design and insufficient robustness in transition logic.\n**Program Identifier:** Generation 4 - Patch Name nonlinear_lqr_with_energy_shaping - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized LQR controller with hand-tuned Q and R matrices on a highly unstable inverted pendulum (long, heavy pole; high friction); applies Euler integration for dynamics and clips control force to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 0.00 due to poor stabilization\u2014final angle error of 2.95 rad and position error over 4000 m.  \n- **Feedback**: Despite aggressive LQR tuning (high angle weight, low control penalty), the controller fails to stabilize the system, likely due to severe nonlinearity from large initial angle (0.9 rad) and model mismatch between linear LQR and highly nonlinear dynamics.\n**Program Identifier:** Generation 5 - Patch Name aggressive_lqr_tuning - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses two LQR controllers (conservative for large angles, aggressive near upright) with linearized dynamics around the upright equilibrium; applies state feedback with angle wrapping and force clipping.  \n- **Performance**: Scored 0.00 due to failure to stabilize, despite full simulation steps (stabilization_ratio=1.00), with high final position error (4357.35 m) and pole angle error (2.91 rad).  \n- **Feedback**: The controller fails to handle the highly unstable system (long, heavy pole; high friction); aggressive gains activate too late, and conservative gains are too weak to recover from large initial angle (0.9 rad). Linearization may be invalid for such large deviations.\n**Program Identifier:** Generation 6 - Patch Name accurate_linearization_and_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses three gain-scheduled LQR controllers selected based on angle and angular velocity magnitude, with state normalization and energy-based force scaling near equilibrium.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization (final theta error: 2.82 rad, cart position diverged to 4340 m).  \n- **Feedback**: Despite adaptive gains and proper linearization, the controller failed to stabilize the highly unstable system (long, heavy pole with high friction), suggesting insufficient robustness or incorrect dynamics modeling in the control law.\n**Program Identifier:** Generation 7 - Patch Name adaptive_lqr_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses two LQR controllers (recovery and balance modes) blended via a sigmoid-like function based on pole angle; control gains are scheduled smoothly to handle large initial deviations.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum within limits, despite high base score (14.54), indicating simulation ran full duration but ended far from upright.  \n- **Feedback**: The controller failed to correct large initial angle (0.9 rad) under challenging dynamics (heavy, long pole); final theta error (2.76 rad) and massive cart drift (x \u2248 3318 m) suggest instability or incorrect linearization assumptions in LQR design.\n**Program Identifier:** Generation 8 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a suboptimal adaptive LQR controller that switches between conservative and aggressive gain matrices based on pole angle; implemented with Euler integration and nonlinear dynamics simulation.  \n- **Performance**: Achieved a combined score of 0.00 due to high final position error (4262.33 m) and angle error (3.06 rad), despite running all 1000 steps.  \n- **Feedback**: The controller fails to stabilize the cart-pole system under challenging conditions (long, heavy pole; high friction); poor state regulation suggests inadequate gain tuning or linearization mismatch in the highly nonlinear regime.\n**Program Identifier:** Generation 9 - Patch Name adaptive_lqr_gains_near_equilibrium - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**\n\n- **Implementation**: Uses gain-scheduled LQR control blending aggressive and precision gains based on pole angle magnitude, augmented with integral action for friction compensation and additional damping terms for high velocities. Includes anti-windup, friction compensation, and smooth saturation.\n\n- **Performance**: Achieved a combined score of 0.00 despite moderate base_score (9.47) and time_bonus (1.01), due to complete failure in stabilization (final_theta_error: 3.03 rad, final_x_error: 4425 m).\n\n- **Feedback**: The controller fails to stabilize the pendulum under challenging conditions (heavy, long pole; high friction; large initial angle). Integral action and adaptive gains are insufficient to overcome model inaccuracies and aggressive dynamics, leading to divergence rather than recovery.\n**Program Identifier:** Generation 10 - Patch Name adaptive_gain_lqr_with_integral - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized model with friction-aware dynamics to compute an LQR controller; state feedback gain is derived from continuous-time algebraic Riccati equation with modest Q/R weights favoring low control effort over fast stabilization.  \n- **Performance**: Achieved a combined score of 3252.50, with strong energy efficiency (avg_energy_per_step: 0.01) but moderate stabilization ratio (0.44) and time bonus due to slow response.  \n- **Feedback**: The conservative LQR tuning successfully stabilized the challenging high-friction, long/heavy-pole system but sacrificed speed and precision\u2014evidenced by low base and time scores despite perfect final error and high success bonus.\n**Program Identifier:** Generation 11 - Patch Name correct_linear_model_friction_tune_gains - Correct Program: True\n\n**Program Name: High-Performance LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized model of an inverted pendulum with aggressive LQR cost weights (`Q=[15,35,2,4]`, `R=0.8`) and includes angle normalization; control forces are clipped to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: The LQR controller is based on a linear approximation that likely fails to capture the highly nonlinear dynamics of the heavy, long pole; early termination conditions (e.g., angle >1.0 rad) were commented out, possibly allowing unstable behavior to continue unchecked.\n**Program Identifier:** Generation 12 - Patch Name lqr_optimized_structural - Correct Program: False\n\n**Program Name: Aggressive LQR with Gain Scheduling**  \n- **Implementation**: Uses a state-dependent blend between aggressive and conservative LQR controllers based on pole angle magnitude, with high Q weights for tight angle control and lower R to allow stronger forces; designed for a heavy, long pole with high friction.  \n- **Performance**: Combined score: 0.0 \u2014 fails to stabilize the pendulum under validation tests.  \n- **Feedback**: Despite sophisticated gain scheduling and tuning for challenging dynamics, the controller fails to handle large initial angles or nonlinearities beyond the linearized model\u2019s validity; early termination due to instability suggests insufficient robustness in recovery phase.\n**Program Identifier:** Generation 13 - Patch Name aggressive_lqr_with_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses an extended state-space model with integral action for steady-state error reduction and adaptive gain scheduling based on pole angle to handle nonlinearities. The controller computes LQR gains offline using continuous-time Riccati equation and applies angle normalization and anti-windup logic for the integrator.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated control design, the linearized model used for LQR does not adequately capture the highly nonlinear dynamics of the heavy, long pole under high friction; additionally, the simulation lacks early termination on failure (e.g., pole angle >1.0 rad), leading to invalid trajectories that likely caused evaluation failures.\n**Program Identifier:** Generation 14 - Patch Name adaptive_lqr_with_integral_action - Correct Program: False\n\n**Program Name: Gain-Scheduled LQR Controller**  \n- **Implementation**: Uses a gain-scheduled LQR controller with low-gain (robust) and high-gain (precise) modes blended based on pole angle magnitude; system linearized around upright equilibrium but applied to highly nonlinear, challenging dynamics (long/heavy pole, high friction).  \n- **Performance**: Achieved only 10.40 base score with poor stabilization (final theta error 2.87 rad, large cart drift), indicating failure to control the pendulum despite full simulation duration.  \n- **Feedback**: The conservative LQR tuning (low Q, high R) and reliance on linearization in a highly nonlinear regime caused sluggish, inaccurate responses; controller failed to stabilize even within generous time limits, suggesting need for more aggressive gains or nonlinear control strategies.\n**Program Identifier:** Generation 15 - Patch Name gainscheduled_lqr - Correct Program: False\n\n**Program Name: Friction-Aware LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with friction terms in the A matrix and optimized LQR weights; includes angle normalization and semi-implicit state estimation for improved feedback.  \n- **Performance**: Achieved a combined score of 0.00 despite high base_score (9.85), failing to stabilize the pendulum within limits (final theta error: 2.82 rad, cart position diverged).  \n- **Feedback**: The controller failed to handle the highly unstable system (long, heavy pole with high friction); aggressive LQR tuning was insufficient without proper nonlinear handling or robustness to large initial angles.\n**Program Identifier:** Generation 16 - Patch Name friction_aware_lqr_optimized - Correct Program: False\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses a linearized state-space model with friction approximations to compute LQR gains, augmented with adaptive gain scheduling that increases control effort for large pole angles (>0.6 rad). Control forces are clipped to \u00b1100 N for physical realism.  \n- **Performance**: Achieved a high combined score of 3381.03, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800), stabilizing the pendulum in 422 steps.  \n- **Feedback**: The adaptive gain scheduling effectively handled the challenging initial condition (0.9 rad) and high-friction dynamics, enabling rapid stabilization despite the heavy, long pole. The controller maintained near-zero final errors, demonstrating robustness and precision.\n**Program Identifier:** Generation 17 - Patch Name advanced_friction_compensated_lqr - Correct Program: True\n\n**Program Name: Enhanced LQR with Friction Compensation**  \n- **Implementation**: Uses a linearized state-space model with friction terms to compute an LQR gain matrix, augmented with velocity-based gain scheduling and anti-windup force clipping.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilizing the pendulum within acceptable bounds despite high energy use.  \n- **Feedback**: The controller failed to stabilize the pole (final theta error = 3.04 rad), likely due to inaccurate linearization for large angles and insufficient robustness to the system's high nonlinearity, mass, and length.\n**Program Identifier:** Generation 18 - Patch Name none - Correct Program: False\n\n**Program Name: Suboptimal LQR with Friction Compensation**  \n- **Implementation**: Uses a linearized LQR controller with conservative Q/R weights and adds heuristic friction compensation for both cart and joint; state is wrapped to [\u2212\u03c0, \u03c0].  \n- **Performance**: Achieved a combined score of 3373.21, with moderate time and energy bonuses but low base accuracy due to high error tolerance.  \n- **Feedback**: The overly conservative LQR gains cause sluggish response and poor stabilization (only 42% stable steps), while friction compensation helps marginally but cannot overcome the weak control authority; evolution potential remains high.\n**Program Identifier:** Generation 19 - Patch Name nonlinear_friction_compensation_lqr - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware model to compute an LQR controller with deliberately conservative gains (low Q, high R), resulting in sluggish but stable control under challenging physics (long/heavy pole, high friction).  \n- **Performance**: Achieved a combined score of 3945.50, with strong energy efficiency and full success bonus despite slow stabilization.  \n- **Feedback**: The controller successfully stabilizes the pendulum but does so slowly (stabilization ratio 0.32), reflecting the trade-off from tuning for low control effort over responsiveness; this validates the design intent of a \"lazy\" yet functional baseline for evolutionary improvement.\n**Program Identifier:** Generation 20 - Patch Name friction_aware_lqr_model - Correct Program: True\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with LQR control, but the linearization inaccurately models friction and ignores nonlinear dynamics; Q/R weights prioritize angle over position yet are mismatched to the highly unstable physical parameters (long/heavy pole, high friction).  \n- **Performance**: Achieved near-zero combined score due to catastrophic failure in stabilization\u2014final theta error of 2.76 rad and cart drift exceeding 4000 m.  \n- **Feedback**: The controller fails because the linearized model is invalid for large initial angles (0.9 rad) and aggressive system dynamics; joint friction was incorrectly incorporated into the A matrix instead of being treated as a disturbance or nonlinear term, leading to poor real-world approximation and instability.\n**Program Identifier:** Generation 21 - Patch Name refine_lqr_model_with_physical_consistency - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware model to compute an LQR controller with deliberately conservative gains (low Q, high R) for stabilizing a challenging inverted pendulum (long, heavy pole; high friction). The controller applies clipped forces based on full-state feedback with angle normalization.\n\n- **Performance**: Achieved a combined score of 3933.91, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate time and stabilization performance (stabilization_ratio: 0.32).\n\n- **Feedback**: The conservative LQR tuning prioritizes low control effort over fast response, resulting in slow stabilization and lower time/stabilization scores despite perfect final error and high energy efficiency\u2014indicating room for improvement via gain optimization or nonlinear control.\n**Program Identifier:** Generation 22 - Patch Name fix_joint_friction_in_a_matrix - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling based on pole angle magnitude and additional angular velocity damping for small angles.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: Despite adaptive gains and damping, the controller is too conservative (high R, low Q) for the challenging environment (long/heavy pole, high friction), leading to instability and early failure; the initial angle of 0.9 rad may exceed its recovery capability.\n**Program Identifier:** Generation 23 - Patch Name smooth_adaptive_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a hybrid adaptive LQR controller with precise linearization of the nonlinear pendulum dynamics and gain scheduling based on pole angle and velocity magnitude. The controller boosts control gains when the pole deviates significantly from upright or when system velocity is high.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite accurate modeling and aggressive gain tuning, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction). Likely issues include inadequate handling of nonlinearities beyond small-angle approximations and possible instability from gain scheduling logic interacting poorly with the clipped control force.\n**Program Identifier:** Generation 24 - Patch Name adaptive_friction_compensated_lqr - Correct Program: False\n\n**Program Name: Friction-Compensated LQR Controller**  \n- **Implementation**: Uses a linearized state-space model with corrected friction terms and smooth gain scheduling based on pole angle; applies LQR control with optimized weights and clips force output.  \n- **Performance**: Achieved base score of 14.15 but failed to stabilize (final theta error = 2.62 rad, large position drift), resulting in zero success bonus and low combined score (0.00).  \n- **Feedback**: Despite improved modeling (e.g., correct joint friction term) and gain scheduling, the controller cannot stabilize the highly unstable system (heavy, long pole with high friction); poor handling of nonlinearities and insufficient robustness lead to failure.\n**Program Identifier:** Generation 25 - Patch Name smooth_adaptive_gains_with_corrected_friction_model - Correct Program: False\n\n**Program Name: Suboptimal LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware dynamics model to compute LQR gains with low state penalties (Q) and high control cost (R), plus gain scheduling based on pole angle.  \n- **Performance**: Achieved a combined score of 4431.51 with moderate time and energy bonuses but slow stabilization (259 steps, 26% ratio).  \n- **Feedback**: The conservative LQR tuning prioritizes low energy use over fast response, resulting in delayed stabilization; gain scheduling helps recover from large angles but doesn't fully compensate for the intentionally sluggish baseline policy.\n**Program Identifier:** Generation 26 - Patch Name tune_lqr_weights_angular_priority - Correct Program: True\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses an LQR controller with linearized dynamics including friction approximations, enhanced with nonlinear friction compensation via tanh-smoothed sign functions and adaptive gain scheduling for large angle deviations.  \n- **Performance**: Achieved a high combined score of 3447.47, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus due to stable final state.  \n- **Feedback**: The friction compensation and gain scheduling effectively stabilized a challenging high-friction, long-pole system; early termination was avoided by maintaining stability beyond 400 steps despite aggressive initial conditions.\n**Program Identifier:** Generation 27 - Patch Name nonlinear_friction_compensation - Correct Program: True\n\n**Program Name: Suboptimal LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains with intentionally conservative Q/R weights; applies gain scheduling based on pole angle.  \n- **Performance**: Achieves low initial score (~3000) due to sluggish response and high error tolerance, failing validation tests.  \n- **Feedback**: The controller\u2019s \"lazy\" tuning (low state penalties, high control cost) and lack of robustness to the challenging physics (long/heavy pole, high friction) prevent effective stabilization; early termination logic is commented out, risking wasted computation after failure.\n**Program Identifier:** Generation 28 - Patch Name switch_to_discrete_lqr - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model with corrected friction terms to compute LQR gains, enhanced by velocity- and angle-aware adaptive gain scheduling for aggressive stabilization.  \n- **Performance**: Achieved a high combined score of 4491.63 with fast stabilization (251 steps), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The adaptive gain scheduling significantly improved control authority during large deviations while maintaining efficiency near equilibrium, enabling robust handling of the challenging heavy/long pole configuration.\n**Program Identifier:** Generation 29 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Swing-Up Assistance**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with gain scheduling and energy-based swing-up logic for large angles; state feedback is blended during transition from swing-up to stabilization.  \n- **Performance**: Achieves low initial score (~3000) due to conservative LQR weights and challenging physics (long/heavy pole, high friction).  \n- **Feedback**: The controller stabilizes the pendulum but responds slowly and tolerates large errors; evaluation indicates room for improvement in both control aggressiveness and handling of extreme initial conditions.\n**Program Identifier:** Generation 30 - Patch Name enhanced_lqr_with_swing_up_trigger - Correct Program: False\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses a linearized state-space model at equilibrium to compute LQR gains, augmented with nonlinear friction compensation and smooth gain scheduling based on pole angle magnitude. However, the `Controller.__init__` references undefined variable `cos_theta` when constructing matrix `A`, causing runtime errors.  \n- **Performance**: Combined score: 0.0 \u2014 fails validation due to incorrect dynamics linearization and implementation bugs.  \n- **Feedback**: The controller logic is conceptually sound but flawed in execution; the undefined `cos_theta` breaks initialization, and the linearized `A` and `B` matrices do not correctly reflect the true system dynamics around \u03b8=0, undermining stability guarantees.\n**Program Identifier:** Generation 31 - Patch Name refine_lqr_gain_scheduling - Correct Program: False\n\n**Program Name: Hybrid Energy-LQR with Friction Compensation**  \n- **Implementation**: Combines energy-based control for large pole angles and LQR for small angles, with adaptive friction compensation and smooth blending between modes. Uses nonlinear dynamics simulation with Euler integration and high-friction-aware linearization for LQR design.  \n- **Performance**: Scored 0.00 due to failure to stabilize; achieved full 1000 steps but with large final errors (theta: 1.24 rad, x: 85.26 m) and no success bonus.  \n- **Feedback**: Despite sophisticated hybrid control and friction handling, the controller failed to stabilize the pendulum within acceptable bounds, indicating insufficient robustness for the extreme parameters (heavy, long pole, high friction). The system remained unstable throughout, suggesting poor tuning or inadequate energy management during transitions.\n**Program Identifier:** Generation 32 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with velocity- and angle-based adaptive gain scheduling to handle large initial deviations and high system instability from a heavy, long pole and high friction.  \n- **Performance**: Achieved a high combined score of 4469.91 with fast stabilization (254 steps), low energy use (total_energy: 6.18), and full success bonus.  \n- **Feedback**: The adaptive gains significantly improved robustness and speed of stabilization under challenging dynamics, while the precise friction modeling in the LQR design contributed to energy efficiency and stable final error near zero.\n**Program Identifier:** Generation 33 - Patch Name None - Correct Program: True\n\n**Program Name: Nonlinear Adaptive Energy-Shaping Controller**  \n- **Implementation**: Uses a composite control law combining nonlinear PD terms for angle/position, energy-shaping feedback, adaptive gain scheduling, and friction compensation to stabilize a challenging inverted pendulum (heavy, long pole; high friction).  \n- **Performance**: Achieved a combined score of 0.00 despite full simulation duration (1000 steps), with poor final stabilization (final_theta_error=1.44 rad, final_x_error=31.17 m) and no success bonus.  \n- **Feedback**: The controller fails to stabilize the pendulum upright\u2014likely due to aggressive energy shaping or insufficient coupling between cart position and pole dynamics\u2014resulting in divergence rather than recovery from large initial angles.\n**Program Identifier:** Generation 34 - Patch Name nonlinear_adaptive_controller - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must include functional code to address the problem requirements.\n**Program Identifier:** Generation 35 - Patch Name None - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform its intended task; implementation must include functional code to address the problem requirements.\n**Program Identifier:** Generation 36 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physics-matched continuous-time LQR controller with smooth, differentiable gain scheduling via tanh-based adaptive gains on position and velocity errors. The linearized dynamics matrix precisely aligns with the nonlinear simulator\u2019s structure, including consistent friction and denominator terms.  \n- **Performance**: Achieved a high combined score of 4878.27, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 198), and full success bonus.  \n- **Feedback**: The precise alignment between the controller\u2019s linear model and the simulator\u2019s physics enabled robust control despite challenging parameters (heavy/long pole, high friction). Adaptive gain scheduling improved transient response without introducing discontinuities, contributing to both stability and low energy use.\n**Program Identifier:** Generation 37 - Patch Name adaptive_lqr_friction_aware - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, incorporating friction terms matching the simulator, and applies smooth gain scheduling via tanh-based adaptation on angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4831.64 with fast stabilization (205 steps), low energy use (total_energy=6.26), and full success bonus.  \n- **Feedback**: The adaptive gains significantly improved robustness to large initial angles and high friction, enabling stable control of an otherwise highly unstable system; precise friction modeling in the LQR design was key to performance.\n**Program Identifier:** Generation 38 - Patch Name corrected_friction_and_aggressive_tuning - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model matching the simulator\u2019s physics to compute LQR gains, augmented with smooth tanh-based gain scheduling for large-angle recovery and swing-up logic when |\u03b8| > 0.85 rad.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization (final \u03b8 error = 1.59 rad, x error = 1408 m), despite full simulation runtime (1000 steps).  \n- **Feedback**: The controller fails to stabilize the pendulum\u2014likely because the swing-up logic applies incorrect force direction (based on sign(\u03b8\u00b7d\u03b8)) and the linear LQR is insufficient for highly nonlinear regimes; energy usage is high but ineffective.\n**Program Identifier:** Generation 39 - Patch Name add_swingup_trigger - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with smooth, multiplicative gain scheduling based on pole angle, angular velocity, and falling direction to enable faster stabilization under high-friction, heavy-pole conditions.  \n- **Performance**: Achieved a high combined score of 4666.63 with rapid stabilization (228 steps), low energy use (7.01 total), and perfect final error metrics.  \n- **Feedback**: The adaptive gains significantly improved response to large initial angles and instability while maintaining energy efficiency; the controller robustly handled challenging dynamics from increased mass, length, and friction.\n**Program Identifier:** Generation 40 - Patch Name aggressive_angular_damping_tuned_gains - Correct Program: True\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based modulation on angle and angular velocity errors to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a combined score of 4878.27 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but modest stabilization ratio (0.20) and base score (392.34).  \n- **Feedback**: The conservative LQR weights and high-friction dynamics limited responsiveness and precision, resulting in slower stabilization; however, the adaptive gains improved robustness enough to maintain balance for nearly the full duration despite challenging initial conditions.\n**Program Identifier:** Generation 41 - Patch Name optimized_lqr_weights_with_adaptive_gain - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control**  \n- **Implementation**: Uses an augmented-state LQR controller with integral action for zero steady-state error and adaptive gain scheduling based on angle/velocity error dominance; includes friction modeling and nonlinear dynamics.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum (final theta error: 2.95 rad, x error: >4000 m).  \n- **Feedback**: Despite sophisticated control design, the system diverges\u2014likely due to inaccurate linearization for large initial angles (0.9 rad) combined with aggressive or mis-tuned gains that destabilize rather than correct motion.\n**Program Identifier:** Generation 42 - Patch Name adaptive_lqr_integral - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling based on pole angle and angular velocity via smooth tanh transitions; system parameters reflect a challenging environment (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 4893.84 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) and base score (393.45).  \n- **Feedback**: The conservative LQR weights and adaptive gains enabled stability under difficult dynamics, yielding high time and energy bonuses, but limited responsiveness resulted in slower stabilization and lower precision.\n**Program Identifier:** Generation 43 - Patch Name optimized_adaptive_lqr_with_tanh_scheduling - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 44 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, enhanced with smooth gain scheduling via tanh-based adaptive terms that increase control effort during large deviations or falling-away dynamics.  \n- **Performance**: Achieved a combined score of 4657.90 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization was slow (stabilization_ratio: 0.23).  \n- **Feedback**: The adaptive gains improved robustness in a challenging high-friction, long-pole environment, but conservative base LQR weights limited responsiveness\u2014evident in low time bonus despite eventual stability.\n**Program Identifier:** Generation 45 - Patch Name optimized_q_weights_and_earlier_gain_activation - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with smooth, state-dependent adaptive gains based on angle and angular velocity to handle the highly unstable, high-friction pendulum dynamics.  \n- **Performance**: Achieved a combined score of 4839.16, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (204 steps).  \n- **Feedback**: The refined gain scheduling significantly improved robustness against large initial angles and high friction, enabling stable control despite aggressive system parameters; precise friction modeling in the LQR design was critical for real-world alignment.\n**Program Identifier:** Generation 46 - Patch Name adaptive_lqr_optimized_qr - Correct Program: True\n\n**Program Name: Discrete LQR with Integral Control**  \n- **Implementation**: Uses a discrete-time LQR controller derived from linearized dynamics via matrix exponential, augmented with integral action on cart position and adaptive gain based on angle error. The controller matches the simulator\u2019s physics in its linearization and applies smooth error-weighted blending for robustness.  \n- **Performance**: Achieved a combined score of 1116.13 with perfect stabilization ratio (1.00) over 1000 steps but failed to balance the pole (final_theta_error: 0.91), resulting in zero success bonus.  \n- **Feedback**: Despite strong energy efficiency and sustained simulation time, the controller fails to stabilize the pole upright\u2014likely due to excessive reliance on integral action causing drift and insufficient handling of nonlinearities in the highly unstable system (long, heavy pole with high friction).\n**Program Identifier:** Generation 47 - Patch Name discrete_lqr_integral_control - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a 5-state linearized model with integral control for position regulation, combined with nonlinear gain scheduling based on angle and angular velocity to adaptively scale control effort. Friction terms are accurately modeled in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4284.15 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (278 steps), and full success bonus.  \n- **Feedback**: The integral action effectively eliminated steady-state position error, while adaptive gains improved robustness during large deviations; accurate friction modeling and tuned LQR weights enabled stable control despite the challenging pole parameters.\n**Program Identifier:** Generation 48 - Patch Name lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control**  \n- **Implementation**: Uses a discrete-time LQR controller with integral action on cart position, augmented by sigmoid-based gain scheduling that blends angle- and velocity-dependent adaptive gains for smoother control.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly challenging pendulum (long, heavy pole; high friction; large initial angle), likely due to inaccuracies in linearization, improper integral handling, or insufficient robustness to nonlinear dynamics.\n**Program Identifier:** Generation 49 - Patch Name friction_aware_lqr_v2 - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model of the inverted pendulum to compute LQR gains, enhanced with smooth, differentiable gain scheduling via tanh-based adaptive scaling on position and velocity errors. The controller enforces physics-consistent linearization and applies force clipping for actuator limits.  \n- **Performance**: Achieved a high combined score of 4920.14, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192), and full success bonus.  \n- **Feedback**: The adaptive gain scheduling significantly improved stabilization speed and robustness under challenging conditions (heavy, long pole; high friction; large initial angle). Precise alignment between simulation dynamics and LQR linearization ensured stable control without divergence.\n**Program Identifier:** Generation 50 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Gains**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, then applies adaptive scaling via tanh-based gain scheduling and adds swing-up assistance for large angles.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and fails validation tests.  \n- **Feedback**: The controller\u2019s intentionally lazy response (high R, low Q) and aggressive physical parameters cause instability; adaptive gains and swing-up logic are insufficient to compensate for poor baseline LQR performance under high difficulty.\n**Program Identifier:** Generation 51 - Patch Name discrete_lqr_zoh - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a friction-aware linearized model for LQR gain computation with adaptive, differentiable gain scheduling based on angle and angular velocity errors via sigmoid blending.  \n- **Performance**: Achieved a combined score of 3917.04 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite challenging pole parameters.  \n- **Feedback**: The controller stabilizes the pendulum reliably but is intentionally \"lazy\" (low Q/R tuning), resulting in slower response (stabilization_ratio: 0.33) and moderate base score; adaptive gains help maintain stability without excessive force.\n**Program Identifier:** Generation 52 - Patch Name smooth_error_mode_blending - Correct Program: True\n\n**Program Name: Adaptive LQR with Synergy Gain**  \n- **Implementation**: Uses a high-precision linearized model matching the simulator's physics to compute LQR gains, enhanced with adaptive multiplicative gains based on angle, angular velocity, and falling-away detection via smooth tanh-based scheduling.  \n- **Performance**: Achieved a combined score of 4864.32 with full success bonus, fast stabilization (200 steps), and low energy use (avg 0.01 per step).  \n- **Feedback**: The controller\u2019s accurate system modeling and adaptive gains enabled robust stabilization of an extremely challenging (long, heavy, high-friction) pendulum, balancing speed and efficiency while preventing divergence during large initial angles.\n**Program Identifier:** Generation 53 - Patch Name enhanced_adaptive_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**\n\n- **Implementation**: Uses a linear-quadratic regulator (LQR) controller on an extended state space that includes an integral term for cart position, combined with smooth gain scheduling via tanh-based adaptive gains to handle nonlinearities and large initial errors.\n- **Performance**: Achieved a high combined score of 4502.04, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 248), and full success bonus.\n- **Feedback**: The integral action effectively eliminated steady-state position error, while adaptive gain scheduling improved robustness to the system's challenging dynamics (heavy, long pole with high friction). The controller maintained stability even under aggressive initial conditions.\n**Program Identifier:** Generation 54 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a discrete-time LQR controller extended with integral action for cart position regulation, enhanced by adaptive gain scheduling based on pole angle, angular velocity, and falling direction. The linearized dynamics model matches the simulator\u2019s physics, including consistent treatment of friction and denominator terms.  \n- **Performance**: Achieved a high combined score of 4383.78, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, stabilizing within 264 steps.  \n- **Feedback**: The adaptive gains and integral action effectively handled the challenging setup (heavy, long pole; high friction; large initial angle), enabling stable control while minimizing energy use. Precise alignment between the controller\u2019s linear model and the nonlinear simulator was critical to performance.\n**Program Identifier:** Generation 55 - Patch Name improved_lqr_with_discrete_time_and_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with suboptimal gains and smooth gain scheduling via tanh-based adaptive scaling on angle and angular velocity errors.  \n- **Performance**: Achieved a combined score of 4878.24 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20).  \n- **Feedback**: The conservative LQR tuning prioritizes stability over responsiveness, resulting in low energy use and high success rate but slower convergence (low time bonus relative to max), indicating room for improved transient performance through more aggressive control near instability thresholds.\n**Program Identifier:** Generation 56 - Patch Name increase_angular_velocity_weight_in_q_matrix - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced by smooth tanh-based gain scheduling that increases control effort for large angle/velocity errors.  \n- **Performance**: Achieves a combined score of 0.0 and fails validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly unstable pendulum (long, heavy pole with high friction), likely due to inaccuracies in linearization or insufficient robustness to nonlinear dynamics at large initial angles.\n**Program Identifier:** Generation 57 - Patch Name refine_q_weights - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced by smooth tanh-based gain scheduling that increases control effort for large angle/velocity errors.  \n- **Performance**: Achieved a high combined score of 4920.14, with strong time, energy, and success bonuses indicating stable, efficient balancing.  \n- **Feedback**: The adaptive gain scheduling significantly improved robustness on the challenging high-friction, long/heavy-pole setup, enabling fast stabilization (192 steps) and near-zero final errors while using minimal energy.\n**Program Identifier:** Generation 58 - Patch Name optimized_q_and_multiplicative_gains - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physics-consistent linearized model for LQR control with optimized state weights and smooth tanh-based gain scheduling that adapts to angle and angular velocity errors.  \n- **Performance**: Achieved a high combined score of 4920.13, with strong stabilization (success bonus = 800), low energy use (avg 0.01/step), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains improved transient response without introducing discontinuities, while precise alignment between the controller\u2019s A matrix and simulator dynamics was critical for stability under challenging conditions (heavy, long pole; high friction).\n**Program Identifier:** Generation 59 - Patch Name optimized_lqr_smooth_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a friction-aware linearized model with discrete-time LQR and smooth tanh-based gain scheduling to adaptively adjust control effort based on angle and angular velocity errors.  \n- **Performance**: Achieved a combined score of 4872.34 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20).  \n- **Feedback**: The adaptive gains improved responsiveness to large initial angles while maintaining stability; however, the conservative Q/R tuning limited time efficiency despite low energy use.\n**Program Identifier:** Generation 60 - Patch Name discrete_lqr_with_refined_gains - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based modulation on angle and angular velocity errors to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a combined score of 4878.25 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) indicating slow convergence.  \n- **Feedback**: The conservative LQR weights and high friction tolerance enabled stability under challenging dynamics, yet the \"lazy\" control response limited time-based rewards; adaptive gains improved large-error handling without introducing discontinuities.\n**Program Identifier:** Generation 61 - Patch Name increase_angular_velocity_penalty - Correct Program: True\n\n**Program Name: Phase-Based Adaptive Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-detection mechanism to switch between four specialized control strategies (Recovery, Swing-Back, Capture, Centering), combining LQR near equilibrium with nonlinear adaptive gains and integral cart-position control. The controller dynamically blends base LQR actions with phase-specific forces based on real-time state analysis.  \n- **Performance**: Achieved a high combined score of 4739.13, with strong time, energy, and success bonuses, stabilizing the pole in 218 steps and maintaining low final errors.  \n- **Feedback**: The phase-based adaptation effectively handles the challenging dynamics of a heavy, long pole with high friction, enabling both aggressive recovery and precise stabilization. Energy efficiency and smooth phase transitions contributed significantly to the high performance.\n**Program Identifier:** Generation 62 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a discretized LQR controller with friction-aware linearization, integral action activated near upright via tanh scheduling, and adaptive gain blending based on angle/velocity error dominance.  \n- **Performance**: Achieved a combined score of 3219.32 with moderate time and energy bonuses but low stabilization ratio (0.33).  \n- **Feedback**: The conservative LQR weights and high friction tolerance led to sluggish response and poor stabilization; integral action helped reduce steady-state position error but wasn't sufficient to maintain balance under challenging initial conditions (0.9 rad).\n**Program Identifier:** Generation 63 - Patch Name add_integral_action_with_tanh_activation - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, then applies smooth tanh-based gain scheduling on angle and angular velocity to adaptively scale control effort.  \n- **Performance**: Achieves a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction); likely due to inaccurate linearization or insufficient robustness to nonlinearities and large initial angles.\n**Program Identifier:** Generation 64 - Patch Name fine_tune_q_matrix - Correct Program: False\n\n**Program Name: Adaptive Discrete-Time LQR Controller**  \n- **Implementation**: Uses a discrete-time LQR controller derived from linearized continuous dynamics with Zero-Order Hold discretization, augmented with adaptive gain scheduling based on pole angle and angular velocity. The physics simulation includes high friction, a long/heavy pole, and large initial angle for increased difficulty.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite accurate modeling and adaptive gains, the controller fails to stabilize the highly unstable system\u2014likely due to excessive nonlinearity beyond the LQR\u2019s linear approximation range and insufficient robustness to large initial deviations.\n**Program Identifier:** Generation 65 - Patch Name adaptive_lqr_with_discrete_tuning - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linearized model for LQR control with optimized Q weights and smooth tanh-based gain scheduling that adapts to pole angle and angular velocity errors.  \n- **Performance**: Achieves a combined score of 4920.14 with full success bonus, low energy use (6.45 total), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains significantly improve robustness under challenging conditions (heavy/long pole, high friction), while the accurate linearization and tuned LQR weights enable precise stabilization with minimal control effort.\n**Program Identifier:** Generation 66 - Patch Name adaptive_lqr_tuned_q - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, incorporating friction terms matching the simulator, and applies adaptive gain scheduling based on angle and angular velocity errors to boost responsiveness during large deviations.  \n- **Performance**: Achieved a high combined score of 4920.14 with fast stabilization (192 steps), low energy use (avg 0.01 per step), and perfect final error metrics.  \n- **Feedback**: The controller\u2019s precise modeling of friction and intelligent gain adaptation enabled robust handling of a challenging, heavy, and long pole while maintaining energy efficiency and rapid stabilization.\n**Program Identifier:** Generation 67 - Patch Name optimized_q_matrix_and_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linear quadratic regulator (LQR) with physically accurate system matrices and proven optimal state weights; enhances robustness via multiplicative gain scheduling using tanh-based adaptation on angle and angular velocity.  \n- **Performance**: Achieved a combined score of 4920.13, including full success bonus, low energy usage (total_energy=6.44), and fast stabilization (192 steps).  \n- **Feedback**: The exact LQR tuning and smooth gain scheduling enabled stable control despite challenging parameters (heavy/long pole, high friction); minor Q-matrix adjustment (Q[3]=3.28) improved damping without sacrificing performance.\n**Program Identifier:** Generation 68 - Patch Name energy_based_sliding_mode_control - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based blending of position and velocity gains, tailored to a challenging high-friction, long/heavy-pole environment.  \n- **Performance**: Achieved a combined score of 4893.84 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) indicating slow convergence.  \n- **Feedback**: The conservative LQR weights and adaptive gains enable stability under difficult dynamics but result in sluggish response (low time bonus relative to max), suggesting room for tuning aggressiveness without sacrificing robustness.\n**Program Identifier:** Generation 69 - Patch Name optimize_lqr_weights_and_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive Swing-Up LQR with Integral Control**  \n- **Implementation**: Combines discrete-time LQR on an extended state (including integral of position), physics-informed swing-up assistance using hyperbolic tangent switching, and soft-switched integral action that deactivates far from equilibrium. Uses accurate linearized dynamics and zero-order hold discretization.  \n- **Performance**: Achieved a combined score of 0.00 due to failure to stabilize the pendulum within limits (final theta error: 2.81 rad, final x error: 3498 m).  \n- **Feedback**: Despite aggressive swing-up and integral terms, the controller failed to stabilize the highly challenging system (heavy, long pole with high friction); large steady-state errors suggest insufficient control authority or poor tuning for this extreme configuration.\n**Program Identifier:** Generation 70 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model to compute LQR gains with optimized state weights, augmented by smooth gain scheduling via tanh and physics-informed swing-up feedforward based on angle and angular velocity.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests despite design efforts for high difficulty conditions.  \n- **Feedback**: The controller\u2019s linearization may be inaccurate for large angles (e.g., 0.9 rad initial condition), and the swing-up logic might introduce destabilizing forces; friction modeling in the linearized A matrix appears inconsistent with the nonlinear simulator dynamics.\n**Program Identifier:** Generation 71 - Patch Name additive_swing_assist_feedforward - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with gain scheduling based on pole angle and angular velocity, plus an additive swing-up term that activates for large angles to counteract falling dynamics.  \n- **Performance**: Achieved a high combined score of 4920.15 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The adaptive gains and physics-informed swing-up assist effectively handle the challenging heavy/long pole configuration, enabling robust stabilization while minimizing energy use; precise friction modeling in both simulation and controller contributes to real-world fidelity and performance.\n**Program Identifier:** Generation 72 - Patch Name additive_swing_assist_with_proven_base - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized model to compute LQR gains with suboptimal Q/R weights, enhanced by smooth tanh-based gain scheduling that increases control effort only when angle or angular velocity errors exceed thresholds.  \n- **Performance**: Achieved a combined score of 4920.14 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The conservative LQR tuning ensures stability under challenging physics (long/heavy pole, high friction), while adaptive gains improve transient response without sacrificing smoothness; however, delayed aggressive correction limits time-based rewards.\n**Program Identifier:** Generation 73 - Patch Name none - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR Controller**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation combined with hybrid gain scheduling that blends linear and nonlinear responses based on angle and angular velocity magnitude; includes robust angle normalization and force clipping.  \n- **Performance**: Achieved a high combined score of 4865.75 with strong energy efficiency, full success bonus, and excellent stabilization.  \n- **Feedback**: The adaptive gain scheduling significantly improved control responsiveness for large deviations while maintaining precision near equilibrium, enabling stable performance even under challenging conditions (heavy, long pole with high friction and large initial angle).\n**Program Identifier:** Generation 74 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**\n\n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based smooth transitions and additive physics-informed swing-up assistance for large angles. The system parameters reflect a highly unstable pendulum (long, heavy pole; high friction).\n\n- **Performance**: Achieves an initial score near 3000 but fails to stabilize consistently under validation tests, resulting in a combined score of 0.0.\n\n- **Feedback**: Despite sophisticated control logic, the controller is too \"lazy\" due to conservative LQR weights (low Q, high R), leading to insufficient correction for the challenging dynamics. Validation likely fails because the pendulum exceeds the 1.0 rad failure threshold before stabilization.\n**Program Identifier:** Generation 75 - Patch Name optimized_q_with_swing_assist - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with suboptimal gains, enhanced by physics-informed swing-up logic and adaptive gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 4724.67, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite slow stabilization (stabilization_ratio: 0.22).  \n- **Feedback**: The conservative LQR tuning prioritizes low energy use over fast response, resulting in high energy and success bonuses but modest time and base scores; the swing-up assist effectively handles large initial angles in a challenging high-friction, long-pole environment.\n**Program Identifier:** Generation 76 - Patch Name enhanced_swing_up_assist_with_adaptive_gain - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with tanh-based gain scheduling and additive swing-up control for large angles (>0.8 rad) to handle highly unstable configurations (long, heavy pole).  \n- **Performance**: Achieved a high combined score of 4920.15, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (192 steps).  \n- **Feedback**: The hybrid approach effectively balances optimal linear control near equilibrium with non-linear corrective actions during large deviations, enabling robust performance under extreme physical parameters; angle normalization and friction-aware dynamics modeling contributed to numerical stability and realism.\n**Program Identifier:** Generation 77 - Patch Name enhanced_swingup_assist_and_q_optimization - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in no functional logic or algorithm implementation.  \n- **Performance**: Combined score is 0.0, indicating complete failure to meet validation requirements.  \n- **Feedback**: The absence of any implementation means the program cannot process inputs, produce outputs, or satisfy basic correctness criteria. A valid solution must include core logic aligned with the problem specification.\n**Program Identifier:** Generation 78 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assistance**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based activation and continuous swing-up assistance for large angles, tailored to a highly unstable pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a combined score of 4920.19, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, despite slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The \"lazy\" LQR weights (low Q, high R) prioritize energy over speed, limiting time bonus but enabling robust stability; the smooth, differentiable swing-up logic effectively handles large initial deviations without abrupt control switches.\n**Program Identifier:** Generation 79 - Patch Name continuous_falling_factor_swing_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based smooth transitions and an additive swing-up term for large angles (>0.8 rad), tailored to a challenging pendulum (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 4920.22, with strong energy efficiency (avg_energy_per_step=0.01) and full success bonus, but slow stabilization (stabilization_ratio=0.19).  \n- **Feedback**: The conservative LQR weights (low Q, high R) prioritize low energy use over fast response, resulting in delayed stabilization; however, the swing-up assist effectively recovers from large initial deviations, enabling consistent success despite difficult dynamics.\n**Program Identifier:** Generation 80 - Patch Name falling_severity_swing_assist - Correct Program: True\n\n**Program Name: Adaptive Momentum Compensation Controller**  \n- **Implementation**: Uses phase-aware switching between swing-up, transition, and fine-tuning control modes with momentum compensation, energy shaping, nonlinear damping, and friction compensation; includes integral action near equilibrium and smooth force limiting.  \n- **Performance**: Achieved a combined score of 0.00 due to failure to stabilize the pendulum (final theta error: 1.81 rad, x error: 20.07 m), despite high energy usage (total_energy: 123.20).  \n- **Feedback**: The controller fails to balance the highly unstable system (long, heavy pole with high friction); aggressive swing-up terms may destabilize early dynamics, and LQR-like gains appear insufficient for stabilization under these extreme parameters.\n**Program Identifier:** Generation 81 - Patch Name adaptive_momentum_compensation_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-consistent linearized LQR controller with adaptive gain scheduling and a momentum-based swing-up assist triggered for large angles (>0.8 rad), using normalized angular momentum and smooth tanh activations for differentiable control adjustments.  \n- **Performance**: Achieved a high combined score of 4920.12, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00), stabilizing in 192 steps.  \n- **Feedback**: The swing-up assist effectively handles the challenging initial condition (0.9 rad) with a heavy, long pole, while precise friction modeling and consistent linearization between simulation and LQR design ensure robust stabilization and minimal steady-state error.\n**Program Identifier:** Generation 82 - Patch Name none - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Action**  \n- **Implementation**: Combines a physically accurate linearized LQR controller with conditional integral action (activated near equilibrium), robust angle normalization, smooth gain scheduling via tanh, and physics-informed swing-up assist for large angles.  \n- **Performance**: Achieved a high combined score of 4691.82 with full success bonus, low energy use (avg 0.01/step), and rapid stabilization (224 steps).  \n- **Feedback**: The hybrid approach effectively balances optimal linear control near equilibrium with non-linear corrections during large deviations, enabling stable control despite challenging parameters (heavy/long pole, high friction). Conditional integration prevents windup while improving steady-state cart position accuracy.\n**Program Identifier:** Generation 83 - Patch Name hybrid_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Gated Integral Control**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, augmented with a gated integral term on cart position that activates only near upright equilibrium, and applies smooth gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 1278.20, with perfect stabilization ratio (1.00) over 1000 steps but zero success bonus due to failure to meet terminal position/angle thresholds.  \n- **Feedback**: Despite excellent energy efficiency and stability maintenance, the controller fails to center the cart (final_x_error = 25.87), indicating the integral action is insufficiently aggressive or misaligned with the true control objective; the lack of success bonus suggests the final state does not satisfy evaluation criteria for \"solved\" episodes.\n**Program Identifier:** Generation 84 - Patch Name adaptive_lqr_with_integrated_correction - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model to compute LQR gains, augmented with tanh-based gain scheduling, soft integral control, and swing-up assistance for large angles. The controller adapts force output based on pole angle and angular velocity deviations.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests despite sophisticated design.  \n- **Feedback**: Overly aggressive gain scheduling and incorrect swing-up logic likely destabilize the system under high-difficulty conditions (heavy/long pole, large initial angle). The integral term and friction modeling may also introduce instability rather than robustness.\n**Program Identifier:** Generation 85 - Patch Name soft_integral_cart_centering - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with adaptive gain scheduling based on angle and angular velocity, plus swing-up assistance for large deviations. Friction terms in the dynamics are carefully matched between simulation and controller.  \n- **Performance**: Achieved a high combined score of 4920.22 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The controller excels in balancing aggressive correction for large errors with minimal energy use near equilibrium; precise modeling of friction and well-tuned LQR weights were critical to success.\n**Program Identifier:** Generation 86 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-informed linearized LQR controller with adaptive gain scheduling and phase-aware swing-up assistance based on normalized angular momentum and pole dynamics. Uses precise friction modeling, consistent linearization around upright equilibrium, and smooth exponential transitions for gain adaptation.  \n- **Performance**: Achieved a high combined score of 4635.10 with full success bonus, low energy usage (avg 0.01/step), and fast stabilization (232 steps).  \n- **Feedback**: The swing-up assist effectively handles large initial deviations (0.9 rad) despite challenging pole parameters (heavy, long, high friction), while adaptive gains improve robustness without destabilizing the LQR core. Energy efficiency and stability were maintained through careful physics-consistent design.\n**Program Identifier:** Generation 87 - Patch Name phase_aware_swing_assist - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Action**  \n- **Implementation**: Combines a physically accurate linearized LQR controller with adaptive gain scheduling, swing-up assist for large angles, and soft-switched integral action to eliminate steady-state cart position error. Uses robust angle normalization and Euler integration matching the simulator dynamics exactly.  \n- **Performance**: Achieved a combined score of 1308.26 with perfect stabilization (1000 steps), low average energy per step (0.01), and high energy efficiency (energy_bonus: 2492.28).  \n- **Feedback**: The hybrid approach effectively stabilizes an extremely challenging system (heavy, long pole with high friction and large initial angle). Integral action and swing-up logic prevent drift and aid recovery, while adaptive gains enhance responsiveness without destabilizing the system near equilibrium.\n**Program Identifier:** Generation 88 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Combines energy-shaping swing-up control with LQR-based stabilization, using phase-based switching and friction compensation; includes adaptive gains and predictive terms for improved response.  \n- **Performance**: Achieved high energy usage (2499.93) but failed to stabilize the pendulum (final theta error: 2.93 rad), resulting in zero success bonus and low combined score (0.00).  \n- **Feedback**: Despite sophisticated control logic, the controller could not stabilize the highly challenging system (heavy, long pole with high friction); likely due to inaccurate dynamics modeling or insufficient robustness in transition between control phases.\n**Program Identifier:** Generation 89 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a suboptimal LQR controller with friction-aware linearization, adaptive gain scheduling via smooth tanh-based modulation, and soft-switched integral action for cart centering.  \n- **Performance**: Achieved a combined score of 1196.58 with perfect stabilization ratio (1.00) over 1000 steps but low base accuracy due to high position errors.  \n- **Feedback**: The conservative LQR tuning (low Q, high R) prioritizes stability over precision, resulting in high energy efficiency but poor final/state error metrics; adaptive gains help maintain balance under challenging dynamics (long/heavy pole, high friction).\n**Program Identifier:** Generation 90 - Patch Name add_soft_cart_integral - Correct Program: True\n\n**Program Name: Adaptive LQR with Synergy Gain**  \n- **Implementation**: Uses a high-precision discrete-time LQR controller derived from physically consistent linearized dynamics, enhanced with smooth adaptive gains based on pole angle, angular velocity, and falling-away detection via tanh-based scheduling.  \n- **Performance**: Achieved a combined score of 4842.54 with full success bonus, low energy use (5.86 total), and fast stabilization (203 steps).  \n- **Feedback**: The synergy gain effectively prevents divergence during large-angle excursions, while accurate friction-aware linearization ensures robustness; the adaptive gains significantly improve damping without destabilizing the system.\n**Program Identifier:** Generation 91 - Patch Name discrete_time_lqr_with_tuned_gains - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linearized LQR controller derived from consistent physics equations, enhanced with smooth tanh-based gain scheduling that adaptively increases control effort for large angle or angular velocity errors.  \n- **Performance**: Achieved a combined score of 4920.13 with full success bonus, fast stabilization (192 steps), and very low energy use (total_energy=6.44).  \n- **Feedback**: The adaptive gains significantly improved stabilization speed without sacrificing energy efficiency, while precise alignment between the controller\u2019s linear model and the nonlinear simulator ensured robustness under challenging conditions (heavy, long pole; high friction; large initial angle).\n**Program Identifier:** Generation 92 - Patch Name increase_angular_velocity_weight - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with gain scheduling based on pole angle and angular velocity, plus an additive swing-up assist for large deviations (>0.8 rad) to recover from near-failure states.  \n- **Performance**: Achieved a combined score of 4920.15, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization was slow (stabilization_ratio: 0.19).  \n- **Feedback**: The adaptive gains and swing-up logic effectively prevented failure and enabled eventual stabilization despite aggressive initial conditions and challenging physics; however, conservative base LQR weights limited responsiveness, resulting in low time bonus and delayed convergence.\n**Program Identifier:** Generation 93 - Patch Name multiplicative_gain_with_swing_assist - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Energy Assist**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance and adaptive gain scheduling, using physically accurate system linearization and friction-aware dynamics.  \n- **Performance**: Achieved a high combined score of 4920.21 with strong stabilization (success_bonus: 800), low energy use (avg_energy_per_step: 0.01), and fast stabilization (stabilization_time: 192).  \n- **Feedback**: The controller effectively handles large initial angles and high system instability due to a heavy, long pole; energy-efficient swing-up logic and precise LQR tuning enable both rapid recovery and stable balance.\n**Program Identifier:** Generation 94 - Patch Name robust_swingup_lqr - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a high-precision linearized LQR controller with optimized Q weights, adaptive gain scheduling via tanh-based activation, swing-up assistance for large angles, and soft integral action on cart position with anti-windup.  \n- **Performance**: Achieved a combined score of 4913.35 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (193 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging heavy/long pole and high friction, while integral control improved steady-state positioning without causing windup during transients.\n**Program Identifier:** Generation 95 - Patch Name swing_up_assist_with_integral - Correct Program: True\n\n**Program Name: Phase-Based Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-based control architecture with smooth transitions between recovery, transition, and precision phases; integrates LQR gains computed from friction-aware linearization, adaptive swing-up assistance for large deviations, and gated integral action for cart centering.  \n- **Performance**: Achieved a high combined score of 4845.21 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (203 steps), and perfect final pole angle (0.00 rad).  \n- **Feedback**: The phase-blending strategy effectively handles the challenging dynamics of a heavy, long pole with high friction, while anti-windup and upright-gated integral action prevent instability and ensure precise steady-state positioning.\n**Program Identifier:** Generation 96 - Patch Name phase_based_controller - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control derived from physically accurate linearized dynamics with adaptive gain scheduling and a physics-informed swing-up assist for large angles, using smooth tanh-based activation and falling severity estimation.  \n- **Performance**: Achieved a high combined score of 4920.19 with full success bonus, low energy usage (total_energy=6.48), and rapid stabilization (stabilization_time=192).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the highly unstable configuration (heavy, long pole; high friction); precise angle normalization and divergence-aware assistance enabled robust recovery and stable balancing.\n**Program Identifier:** Generation 97 - Patch Name adaptive_lqr_with_swing_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model with physically accurate friction terms to compute LQR gains, augmented with tanh-based adaptive gain scheduling and soft-switched integral action on cart position.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated control design, the controller fails to stabilize the highly unstable pendulum (long, heavy pole with high friction); likely due to inaccurate linearization, poor handling of nonlinear dynamics, or inadequate robustness to large initial angles.\n**Program Identifier:** Generation 98 - Patch Name robust_arctan2_angle_norm - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a suboptimal LQR controller with gain scheduling, integral action gated near upright, and physics-informed swing-up assistance for large angles. The linearized model accounts for friction, and control gains are adaptively scaled based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 1313.02 with perfect stabilization ratio (1.0) over 1000 steps but low base accuracy due to conservative tuning.  \n- **Feedback**: High energy efficiency (avg 0.01 per step) and full time utilization indicate stable balancing, yet poor position/angle precision (final_x_error=7.98, stable_x_error=2.79) reveals intentionally lazy control from low Q/high R weights, limiting base score despite robustness.\n**Program Identifier:** Generation 99 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a suboptimal LQR controller with friction-aware linearization and smooth gain scheduling via tanh-based adaptive gains to handle large pole angles and high friction.  \n- **Performance**: Achieves a combined score of 4920.14 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite challenging physics (long, heavy pole; high friction).  \n- **Feedback**: The conservative LQR weights result in slower response (low time bonus relative to max), but adaptive gains improve stability at large errors without discontinuities, enabling consistent stabilization within 192 steps.\n**Program Identifier:** Generation 100 - Patch Name update_to_best_lqr_weights - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a friction-aware linearized model for LQR gain computation, enhanced with smooth gain scheduling via tanh-based adaptive gains, swing-up assistance for large angles, and soft-switched integral action on cart position to improve steady-state accuracy.  \n- **Performance**: Achieved a combined score of 4920.10 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization ratio was low (0.19).  \n- **Feedback**: The controller effectively balances the challenging pendulum (long, heavy pole; high friction) and achieves stability, but slow response due to conservative LQR weights limits time bonus; integral action and adaptive gains help maintain precision once stabilized.\n**Program Identifier:** Generation 101 - Patch Name integral_action_for_zero_drift - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with adaptive gain scheduling via tanh-based blending, swing-up assist for large angles, and gated integral action for position error correction.  \n- **Performance**: Achieved a combined score of 1308.25 with perfect stabilization (1000 steps, stabilization ratio 1.0) and very low energy usage (avg 0.01 per step).  \n- **Feedback**: The controller successfully stabilizes an extremely challenging inverted pendulum (long, heavy pole; high friction; large initial angle), demonstrating robustness through hybrid nonlinear adaptations while maintaining energy efficiency\u2014though it incurs a positional drift (final x_error = 7.01), indicating room for improved steady-state accuracy.\n**Program Identifier:** Generation 102 - Patch Name increase_angular_velocity_damping - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**  \n- **Implementation**: Uses a friction-aware linearized model for LQR control with adaptive gain scheduling, integral action gated by pole angle, and physics-informed swing-up assistance for large deviations.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and environmental difficulty, failing validation tests with a combined score of 0.0.  \n- **Feedback**: The controller\u2019s deliberate \"laziness\" (low Q/high R weights) causes slow response and poor precision; despite advanced features like gain scheduling and swing-up logic, it cannot stabilize the highly unstable system within required limits.\n**Program Identifier:** Generation 103 - Patch Name robust_angle_norm - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute discrete-time LQR gains, enhanced by adaptive gain scheduling based on pole angle and angular velocity, plus swing-up assistance for large deviations.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive control logic, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction), likely due to inaccuracies in linearization or insufficient robustness in the gain adaptation under extreme initial conditions.\n**Program Identifier:** Generation 104 - Patch Name discrete_lqr_zoh - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Uses a high-precision linearized LQR controller with optimized Q weights, adaptive gain scheduling via tanh-based activation, swing-up assistance for large angles, and soft integral action on cart position with anti-windup and gating near equilibrium.  \n- **Performance**: Achieved a combined score of 4912.79 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (193 steps), and full success bonus.  \n- **Feedback**: The combination of physically consistent linearization, adaptive gains, and targeted swing-up logic effectively handles the challenging heavy/long pole and high-friction dynamics; integral action improves steady-state positioning without destabilizing transients.\n**Program Identifier:** Generation 105 - Patch Name integral_action_refinement - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 106 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a friction-aware linearized model for LQR control, enhanced with smooth gain scheduling via tanh, swing-up assist for large angles, and anti-windup integral action on cart position.  \n- **Performance**: Achieved a combined score of 4920.09 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle large initial angles and prevent failure, while conservative LQR weights and integral action improve steady-state accuracy without destabilizing the system. However, the controller\u2019s sluggish response limits time-based rewards.\n**Program Identifier:** Generation 107 - Patch Name integral_anti_windup - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with friction-aware dynamics, combined with gain scheduling based on pole angle/velocity and an additive swing-up term for large deviations.  \n- **Performance**: Achieved a high combined score of 4920.20 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The adaptive gains and physics-informed swing-up assist effectively handle the challenging heavy/long pole configuration, enabling robust recovery from large initial angles while minimizing energy use.\n**Program Identifier:** Generation 108 - Patch Name none - Correct Program: True\n\n**Program Name: Phase-Predictive LQR Controller**  \n- **Implementation**: Uses a discretized LQR controller with phase-aware switching between recovery, transition, and precision modes; includes predictive correction, swing-up assistance, and integral action for steady-state accuracy.  \n- **Performance**: Achieved a high combined score of 4782.29 with strong energy efficiency, fast stabilization (212 steps), and perfect final error metrics.  \n- **Feedback**: The adaptive phase logic and predictive corrections effectively handled the challenging high-friction, long-pole dynamics, enabling robust swing-up and precise stabilization while minimizing control effort.\n**Program Identifier:** Generation 109 - Patch Name phase_predictive_lqr - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a linearized friction-aware model for LQR control with adaptive gain scheduling via tanh-based smooth switching, integral action for position regulation (with anti-windup), and physics-informed swing-up assistance for large angles.  \n- **Performance**: Achieved a combined score of 4920.10 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The controller effectively balances the challenging high-friction, long/heavy-pole system and avoids failure, but its conservative gains and delayed response limit time-based rewards despite excellent steady-state accuracy.\n**Program Identifier:** Generation 110 - Patch Name integral_action_with_soft_switching - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Gains**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, augmented with smooth tanh-based gain scheduling and soft-switched integral action for cart position.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and fails validation tests under challenging physics (long/heavy pole, high friction).  \n- **Feedback**: The controller\u2019s \"lazy\" Q/R weights and insufficient robustness to large initial angles cause poor stabilization; adaptive gains help but don\u2019t compensate for fundamental underperformance in hard conditions.\n**Program Identifier:** Generation 111 - Patch Name increase_dtheta_weight - Correct Program: False\n\n**Program Name: Energy-Phase Adaptive LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based adaptive gains, phase-specific strategies (recovery, precision, predictive damping), and integral action with anti-windup for steady-state accuracy. The controller dynamically adjusts force output based on normalized mechanical energy, angular velocity, and state magnitude to handle a highly unstable, heavy, and long inverted pendulum.  \n- **Performance**: Achieved a high combined score of 4755.01 with strong bonuses for success, time, and energy efficiency, stabilizing the system in 215 steps with near-zero final errors.  \n- **Feedback**: The adaptive gain scheduling and phase-aware logic effectively managed the challenging dynamics (high mass, length, friction), while predictive damping and integral control ensured precise stabilization without overshoot or windup.\n**Program Identifier:** Generation 112 - Patch Name energy_phase_controller - Correct Program: True\n\n**Program Name: Adaptive Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-based hybrid controller with energy-based swing-up for large angles, blended transition control, and LQR with integral action for stabilization; includes nonlinear modifications like velocity damping and control authority scaling.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization despite high energy usage (total_energy=8.22) and full simulation duration (1000 steps).  \n- **Feedback**: The controller failed to stabilize the pendulum (final_theta_error=2.77 rad), indicating ineffective mode transitions or insufficient robustness for the highly unstable system (long, heavy pole with high friction); integral windup or aggressive swing-up may have hindered convergence.\n**Program Identifier:** Generation 113 - Patch Name none - Correct Program: False\n\n**Program Name: Feedback Linearization with Adaptive LQR**  \n- **Implementation**: Combines discrete LQR control on a linearized model with exact nonlinear inverse dynamics to track a desired angular acceleration, augmented by adaptive gain scheduling and gated integral action for position regulation.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: The controller struggles with the highly unstable, high-friction, and heavy/long pole configuration; likely due to singularities in inverse dynamics near \u00b1\u03c0/2 and insufficient robustness in the feedback linearization under large initial angles (0.9 rad). The adaptive gains and integral term may not compensate adequately for model inaccuracies or aggressive nonlinearities.\n**Program Identifier:** Generation 114 - Patch Name fbl_discretelqr_integral - Correct Program: False\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with optimized Q-matrix weights, enhanced by adaptive gain scheduling and a continuous swing-up assist that activates based on angle magnitude and falling severity.  \n- **Performance**: Achieved a combined score of 4920.22 with perfect stabilization (final theta error = 0.00) and high energy efficiency (avg_energy_per_step = 0.01).  \n- **Feedback**: The controller effectively balances aggressive correction for large deviations with fine-tuned stabilization near upright, while the swing-up logic ensures recovery from highly unstable initial conditions; the exact match between simulation dynamics and LQR model was critical to success.\n**Program Identifier:** Generation 115 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, then applies adaptive gain scheduling via tanh-based scaling and adds swing-up assistance for large angles (>0.8 rad).  \n- **Performance**: Achieves low initial score (~3000) due to conservative Q/R tuning and insufficient robustness in highly unstable conditions.  \n- **Feedback**: The controller fails validation tests because the linearized LQR is inadequate for the highly nonlinear, high-friction, long-pole regime; swing-up logic lacks smooth integration and doesn\u2019t prevent early failure beyond 1.0 rad.\n**Program Identifier:** Generation 116 - Patch Name fix_duplicate_init_remove_integral - Correct Program: False\n\n**Program Name: Phase-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Uses a three-phase control strategy (Recovery, Transition, Precision) with physics-informed LQR gains, adaptive swing-up logic, and anti-windup integral action for steady-state error correction.  \n- **Performance**: Achieved a high combined score of 4760.24 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps.  \n- **Feedback**: The aggressive recovery phase effectively handles large initial angles, while the gated integral term ensures precise stabilization without windup; high friction and pole mass make this a robust solution under challenging dynamics.\n**Program Identifier:** Generation 117 - Patch Name adaptive_lqr_with_integral_correction - Correct Program: True\n\n**Program Name: Adaptive Energy-Phase Inverted Pendulum Controller**  \n- **Implementation**: Combines LQR-based linear control with energy-aware swing-up logic, phase-specific gains, integral action for precision stabilization, and predictive damping to handle a highly unstable pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a high combined score of 4755.01 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps with minimal final errors.  \n- **Feedback**: The adaptive gain scheduling and phase-aware switching effectively managed the challenging dynamics, while integral control and predictive damping significantly improved steady-state accuracy and reduced overshoot near equilibrium.\n**Program Identifier:** Generation 118 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Uses an enhanced adaptive LQR controller with integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, and a swing-up assist activated during large deviations. The dynamics include high-friction, heavy/long pole parameters for increased difficulty.  \n- **Performance**: Achieved a combined score of 4927.10 with full success bonus, low energy usage (6.46 total), and stabilization after 191 steps.  \n- **Feedback**: The integral control effectively eliminated steady-state cart error when upright, while the swing-up logic and adaptive gains enabled recovery from highly unstable initial conditions (0.9 rad). Energy efficiency and stability were maintained despite aggressive physical challenges.\n**Program Identifier:** Generation 119 - Patch Name adaptive_lqr_integral_swingup - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-informed linear quadratic regulator (LQR) with adaptive gain scheduling and a smooth, momentum-based swing-up assist for large angles; includes soft-switched integral action to eliminate steady-state cart position error.  \n- **Performance**: Achieved a high combined score of 4920.25, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and perfect upright maintenance (final_theta_error: 0.00).  \n- **Feedback**: The swing-up assist effectively handles the challenging initial condition (0.9 rad) with a heavy, long pole, while adaptive gains and integral action improve robustness and precision without destabilizing the system.\n**Program Identifier:** Generation 120 - Patch Name increase_dtheta_q_weight - Correct Program: True\n\n**Program Name: Phase-Adaptive Discrete LQR Controller**  \n- **Implementation**: Uses discrete-time LQR with zero-order hold discretization, enhanced by phase-based swing-up assistance, adaptive gain scheduling, filtered state estimation, and integral control activated only during fine-tuning.  \n- **Performance**: Achieved a combined score of 0.0 and failed validation tests.  \n- **Feedback**: The controller\u2019s complex phase logic and energy-based swing-up may be misaligned with the highly unstable system dynamics (heavy, long pole); discretization via an incorrect matrix exponential approximation likely introduced instability, and aggressive clipping or filtering may degrade responsiveness.\n**Program Identifier:** Generation 121 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physics-informed linearized model for LQR control with adaptive gain scheduling based on pole angle and velocity, plus swing-up assistance for large deviations and leaky integral action for cart position regulation.  \n- **Performance**: Achieved a high combined score of 4920.25 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The controller effectively balances aggressive stabilization for large angles with precise upright maintenance, while friction-aware modeling and smooth gain transitions prevent instability and energy waste.\n**Program Identifier:** Generation 122 - Patch Name add_swing_up_assist_and_integral_control - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state error and a physics-informed swing-up assist that activates for large pole angles, using smooth gain scheduling via tanh-based activation functions.  \n- **Performance**: Achieved a high combined score of 4920.25, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging dynamics (heavy, long pole; high friction), while integral control ensured precise cart positioning once upright\u2014demonstrating robustness and efficiency in both transient and steady-state phases.\n**Program Identifier:** Generation 123 - Patch Name Add integral control for cart position and swing-up assist for large angles - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with adaptive gain scheduling based on angle and angular velocity, plus physics-informed swing-up assistance for large deviations.  \n- **Performance**: Achieved a high combined score of 4898.88 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 195 steps).  \n- **Feedback**: The controller effectively balances aggressive correction for large errors with energy-efficient fine control near equilibrium; the swing-up assist and multiplicative gain adaptation were key to handling the challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 124 - Patch Name discrete_lqr_design - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance using predictive angle estimation, adaptive gain scheduling via tanh activation, and soft-switched integral action for steady-state error correction. The controller uses accurate linearized dynamics matching the nonlinear simulator and includes friction-aware modeling.  \n- **Performance**: Achieved a high combined score of 4872.08 with strong time, energy, and success bonuses, stabilizing the pendulum in 199 steps with minimal final errors.  \n- **Feedback**: The swing-up assist effectively handles large initial angles (0.9 rad), while adaptive gains and integral action improve robustness and precision; low average energy per step (0.01) indicates efficient control despite challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 125 - Patch Name predictive_feedforward_swing - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Energy Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart positioning; uses physically accurate linearization and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4913.06 with full success bonus, low energy use (avg 0.01/step), and fast stabilization (193 steps).  \n- **Feedback**: The controller effectively handles the challenging pole parameters (heavy, long) by intelligently switching between swing-up and stabilization modes; integral control and adaptive gains ensure precise cart centering and robustness without excessive energy expenditure.\n**Program Identifier:** Generation 126 - Patch Name add_cart_vel_cross_coupling - Correct Program: True\n\n**Program Name: Discrete LQR with Energy-Aware Swing-Up**\n\n- **Implementation**: Uses a discrete-time LQR controller derived via zero-order hold to match Euler integration dynamics, enhanced with energy-based swing-up logic and adaptive integral control that decays aggressively when the pole is far from upright. The controller applies gain scheduling based on angle and angular velocity, and only activates swing-up assistance when energy falls below 85% of the upright potential energy.\n\n- **Performance**: Achieved a high combined score of 4892.07, with strong contributions from time bonus (2206.23), energy efficiency (2493.47), and full success bonus (800.00), stabilizing in 196 steps with near-zero final errors.\n\n- **Feedback**: The energy-aware swing-up prevents unnecessary force application while effectively recovering from large initial angles; adaptive integral decay and gain scheduling significantly improve robustness under challenging conditions (heavy, long pole with high friction).\n**Program Identifier:** Generation 127 - Patch Name discrete_lqr_energy_adaptive - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action for cart position, and physics-informed swing-up assistance triggered at large angles (>0.8 rad). Friction is accurately modeled in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4933.93 with excellent energy efficiency (avg_energy_per_step: 0.01) and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up assist effectively stabilized the highly challenging pendulum (long, heavy pole; large initial angle), while integral action ensured precise long-term positioning without compromising stability.\n**Program Identifier:** Generation 128 - Patch Name none - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) stabilization around the upright position with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart position error elimination; uses physically accurate linearization and directional momentum awareness to enhance swing-up efficiency.  \n- **Performance**: Achieved a high combined score of 4927.07 with full success bonus, low energy usage (total_energy=6.50), and fast stabilization (191 steps).  \n- **Feedback**: The controller effectively balances aggressive swing-up and precise stabilization, with integral action and adaptive gains contributing to zero final errors and robustness under challenging conditions (heavy/long pole, high friction).\n**Program Identifier:** Generation 129 - Patch Name directional_momentum_integral_control - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed gain scheduling, swing-up assistance for large angles, and soft-switched integral action to eliminate steady-state cart position error. The controller uses accurate linearized dynamics matching the simulator and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4920.25 with full success bonus, low energy use (total_energy=6.47), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging pole parameters (heavy, long, high friction), while integral control ensured precise upright stabilization without windup due to leaky integration and gating.\n**Program Identifier:** Generation 130 - Patch Name enhanced_swing_up_with_integral_control - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action gated by angle/velocity thresholds, and physics-informed swing-up assistance for large deviations. Friction-aware system modeling ensures accurate controller synthesis.  \n- **Performance**: Achieved a high combined score of 4926.67 with excellent energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up assist effectively handle the challenging heavy/long pole configuration, while gated integral action prevents destabilization during transients\u2014key to balancing speed, stability, and energy use.\n**Program Identifier:** Generation 131 - Patch Name joint_integral_gate_and_tuned_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed adaptive gain scheduling, swing-up assistance for large angles, and gated integral action for zero steady-state cart position error. The controller uses robust angle normalization, smooth activation functions (tanh), and leaky integration with angle-dependent decay.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and near-perfect stabilization.  \n- **Feedback**: The enhanced swing-up logic and adaptive gains effectively handled the challenging pole parameters (longer, heavier), while the gated integral term improved long-term positioning without destabilizing the system during large swings.\n**Program Identifier:** Generation 132 - Patch Name integral_windup_prevention_with_angle_velocity_gate - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines gain-scheduled LQR with robust swing-up assist, enhanced angular damping, and gated integral action that activates only near upright equilibrium to minimize drift.  \n- **Performance**: Achieved a high combined score of 4926.67 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00).  \n- **Feedback**: The controller effectively stabilizes a highly challenging pendulum (long, heavy, high friction) by blending aggressive swing-up logic with precise integral correction, though stabilization ratio remains low (0.19), indicating delayed convergence.\n**Program Identifier:** Generation 133 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model to compute LQR gains, enhanced with adaptive gain scheduling, robust angle normalization via `arctan2`, soft-switched integral control gated by pole angle and velocity, and a swing-up assist that activates based on normalized falling severity.  \n- **Performance**: Achieved a high combined score of 4926.64, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (stabilization_time: 191).  \n- **Feedback**: The integral action effectively eliminated steady-state cart position error without destabilizing transients, while the swing-up logic and adaptive gains enabled reliable recovery from large initial angles (0.9 rad) despite challenging pole parameters (longer and heavier than standard).\n**Program Identifier:** Generation 134 - Patch Name refined_integral_gating_with_velocity - Correct Program: True\n\n**Program Name: Phase-Based Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Uses a hybrid control strategy with energy-based swing-up for large angles, smooth transition blending, and LQR with integral action near equilibrium; includes friction modeling and Euler integration.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization despite high energy usage (total_energy: 101.95) and full simulation duration (stabilization_time: 1000).  \n- **Feedback**: The controller failed to stabilize the pendulum (final_theta_error: 2.95 rad), indicating ineffective phase transitions or insufficient robustness for the challenging pole parameters (longer, heavier, high friction).\n**Program Identifier:** Generation 135 - Patch Name none - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments and whitespace.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass any validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 136 - Patch Name None - Correct Program: False\n\n**Program Name: Energy-Shaping Hybrid Controller**  \n- **Implementation**: Combines energy-based swing-up control with LQR stabilization and smooth phase transitions based on angle, velocity, and energy thresholds; includes integral action with anti-windup and centrifugal compensation.  \n- **Performance**: Achieved a combined score of 34.79 but failed to stabilize the pendulum (success_bonus: 0.00), despite maintaining full simulation duration (stabilization_ratio: 1.00).  \n- **Feedback**: The controller could not balance the pole (final_theta_error: 3.12 rad), indicating ineffective transition or stabilization logic; high friction and aggressive pole parameters likely exposed weaknesses in the LQR design or phase-switching criteria.\n**Program Identifier:** Generation 137 - Patch Name energy_shaping_hybrid_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and soft-switched integral action to eliminate steady-state cart position error, using gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and perfect final pole alignment (final_theta_error: 0.00).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging high-mass, long-pole configuration, while integral control with angle/velocity gating prevented windup and improved precision without destabilizing the system.\n**Program Identifier:** Generation 138 - Patch Name enhanced_integral_suppression - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up**\n\n- **Implementation**: Uses an enhanced adaptive LQR controller featuring integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, and a swing-up assist activated during large deviations. Friction and nonlinear dynamics are explicitly modeled in simulation.\n\n- **Performance**: Achieved a high combined score of 4926.69 with full success bonus, low energy usage (total_energy=6.50), and perfect final stabilization (final_theta_error=0.00).\n\n- **Feedback**: The soft-switched integral control effectively eliminated steady-state cart drift without destabilizing transients, while the normalized swing-up logic provided reliable recovery from extreme angles. Optimized angular velocity damping (Q[3]=3.28) reduced oscillations and contributed to early stabilization (stabilization_time=191).\n**Program Identifier:** Generation 139 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up Assist**  \n- **Implementation**: Uses an enhanced adaptive Linear Quadratic Regulator (LQR) controller featuring integral action for steady-state cart position correction, robust angle normalization via `arctan2`, gain scheduling based on pole angle and angular velocity, and energy-aware swing-up assistance triggered when the pole is far from upright.  \n- **Performance**: Achieved a high combined score of 4906.68, with strong contributions from time bonus (2220.04), energy efficiency (2492.62), and full success bonus (800.00), stabilizing in 194 steps with near-zero final errors.  \n- **Feedback**: The integral control effectively eliminated steady-state cart drift without destabilizing the system due to intelligent gating by angle and velocity; the swing-up logic efficiently managed the challenging initial condition (0.9 rad) with a heavy, long pole, while adaptive gains improved phase-specific responsiveness.\n**Program Identifier:** Generation 140 - Patch Name energy_aware_swingup_assist - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in a complete lack of functionality for any intended algorithm or task.  \n- **Performance**: Combined score is 0.0, indicating failure to pass any validation tests.  \n- **Feedback**: An empty implementation cannot process inputs or produce outputs, leading to total failure in evaluation; actual logic must be implemented to address the problem requirements.\n**Program Identifier:** Generation 141 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced with adaptive gain scheduling, predictive correction using angular acceleration, physics-informed swing-up assistance for large angles, and gated integral control for cart position.  \n- **Performance**: Achieved a high combined score of 4787.89 with strong time, energy, and success bonuses, stabilizing in 211 steps with near-zero final errors.  \n- **Feedback**: The controller effectively balances aggressive early-phase control with energy-efficient stabilization; the swing-up assist and adaptive gains are critical for handling the challenging pole parameters (heavy, long, high friction), while integral action with gating prevents windup and improves steady-state accuracy.\n**Program Identifier:** Generation 142 - Patch Name aggressive_early_phase_control - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines gain-scheduled LQR with robust integral action, enhanced angular damping, and physics-informed swing-up assistance; uses precise linearized dynamics matching the simulator\u2019s friction model and adaptive gains based on pole angle/velocity.  \n- **Performance**: Achieved a high combined score of 4926.94 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large angles with fine stabilization near upright, while leaky integral action and soft-switched damping reduce overshoot and improve cart centering without destabilizing the pole.\n**Program Identifier:** Generation 143 - Patch Name optimized_integral_and_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and integral action for cart position regulation, using gain scheduling and energy-based scaling to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a high combined score of 4907.13 with full success bonus, low energy usage, and stable final state.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large deviations with precise LQR stabilization near upright, while integral action eliminates steady-state position error without windup due to soft gating and leaky integration.\n**Program Identifier:** Generation 144 - Patch Name energy_aware_swing_up_assist - Correct Program: True\n\n**Program Name: Phase-Based Adaptive Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-switching strategy with four LQR controllers (swing-up, transition, balancing, fine-tune), energy shaping for swing-up, integral action with windup protection, predictive compensation, and nonlinear damping.  \n- **Performance**: Combined score of 0.0; fails to stabilize the pendulum under challenging initial conditions and system parameters.  \n- **Feedback**: Despite sophisticated control logic, the controller struggles with the highly unstable configuration (long, heavy pole; high friction; large initial angle), likely due to inadequate swing-up energy injection or poor phase transition timing.\n**Program Identifier:** Generation 145 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with adaptive gain scheduling based on angle and angular velocity, enhanced by soft-switched integral action and energy-aware swing-up assistance.  \n- **Performance**: Achieved a high combined score of 4907.12 with strong time, energy, and success bonuses, stabilizing in 194 steps with minimal final errors.  \n- **Feedback**: The controller effectively balances aggressive stabilization for large deviations with energy efficiency near equilibrium; the inclusion of friction in the linearized model and intelligent gain adaptation were key to handling the challenging heavy/long pole configuration.\n**Program Identifier:** Generation 146 - Patch Name sharpen_gain_transitions - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, robust swing-up assistance based on mechanical energy deficit, and gated integral action for cart position stabilization. The controller uses a physically accurate linearized model and includes enhanced damping near equilibrium.  \n- **Performance**: Achieved a high combined score of 4926.90, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 191), and full success bonus.  \n- **Feedback**: The hybrid approach effectively handles the challenging dynamics of a heavy, long pole with high friction, demonstrating that adaptive gains and energy-based swing-up significantly improve robustness and stabilization speed, while leaky integral action prevents windup without degrading performance.\n**Program Identifier:** Generation 147 - Patch Name energy_based_swing_up - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, robust swing-up assistance based on falling severity, and soft-switched integral action for cart position regulation. The controller uses a physically accurate linearized model and dynamically adjusts gains based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4933.93, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (190 steps), and full success bonus.  \n- **Feedback**: The hybrid approach effectively stabilizes a highly challenging inverted pendulum (long, heavy pole; high friction; large initial angle). Integral action and swing-up logic are key to maintaining stability and achieving near-zero final errors, while adaptive gains prevent overreaction during large deviations.\n**Program Identifier:** Generation 148 - Patch Name simplified_integral_gating_with_optimal_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control derived from physically accurate linearized dynamics with adaptive gain scheduling, swing-up assistance based on normalized angular momentum, and dual-gated integral action that activates only near upright and low-velocity conditions.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192 steps), and full success bonus.  \n- **Feedback**: The hybrid approach effectively handles the challenging pole parameters (heavy, long, high friction), with adaptive gains and swing-up logic enabling robust recovery from large initial angles, while gated integral control prevents windup and improves steady-state accuracy.\n**Program Identifier:** Generation 149 - Patch Name adaptive_lqr_integral_swingup_enhanced - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and gated integral action to eliminate steady-state cart position error; uses adaptive gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and full success bonus.  \n- **Feedback**: The enhanced swing-up logic effectively handles the challenging initial conditions (heavy, long pole with large initial angle), while the soft-gated integral term prevents windup and improves final positioning without destabilizing the system.\n**Program Identifier:** Generation 150 - Patch Name adaptive_energy_aware_swingup - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Uses a refined LQR controller with gain scheduling, angle-only integral gating, and swing-up assistance for large deviations; includes physically accurate linearization and friction-aware dynamics.  \n- **Performance**: Achieved a high combined score of 4920.84 with full success bonus, low energy use (6.58 total), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains and simplified integral control effectively balanced stability and efficiency, while the swing-up logic enabled recovery from large initial angles (0.9 rad); tight anti-windup and decay prevented integral windup without sacrificing responsiveness.\n**Program Identifier:** Generation 151 - Patch Name refined_adaptive_lqr_optimized - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control and Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with gain scheduling, integral action for cart position regulation, and physics-informed swing-up assistance triggered by pole angle and falling dynamics. Uses accurate friction modeling and Euler integration for simulation.  \n- **Performance**: Achieved a high combined score of 4934.54 with full success bonus, low energy use (6.61 total), and fast stabilization (190 steps).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle large initial angles and system instability from heavy/long poles, while integral control ensures precise long-term positioning without windup due to velocity-gated activation.\n**Program Identifier:** Generation 152 - Patch Name sharper_gain_transitions_and_velocity_gated_integral - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Integral Action**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart position stabilization; uses physically accurate dynamics and directional momentum awareness to enhance recovery during large-angle excursions.  \n- **Performance**: Achieved a high combined score of 4933.59 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 190 steps), and perfect final state accuracy.  \n- **Feedback**: The controller effectively balances aggressive swing-up and precise stabilization, with integral action eliminating steady-state cart error without windup; directional momentum logic prevents overshoot during recovery, contributing to robustness under challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 153 - Patch Name enhanced_integral_control_with_dynamic_gating - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state cart position error, adaptive gain scheduling via tanh-based angle/velocity thresholds, and physics-informed swing-up assist using energy deficit and angular momentum cues.  \n- **Performance**: Achieved a high combined score of 4926.95 with strong stability (final theta error \u2248 0), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The integration of swing-up logic enabled recovery from large initial angles (0.9 rad), while adaptive gains and integral control ensured precise stabilization; energy-efficient actuation and leaky integration prevented windup and maintained robustness under high-friction, heavy-pole conditions.\n**Program Identifier:** Generation 154 - Patch Name energy_scaled_swing_up_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state error and a physics-informed swing-up assist that activates for large pole angles (>0.8 rad), using smooth tanh-based gain scheduling and dual-gated integral feedback.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00).  \n- **Feedback**: The adaptive gains and swing-up logic enabled rapid recovery from large initial angles (0.9 rad), while integral control ensured precise stabilization; the design effectively balanced aggressive correction with energy conservation under high-friction, unstable dynamics.\n**Program Identifier:** Generation 155 - Patch Name enhanced_integral_control_with_velocity_gating_and_adaptive_decay - Correct Program: True\n\n**Program Name: Phase-Aware Adaptive LQR Controller**  \n- **Implementation**: Uses three phase-specific LQR controllers (swing-up, transition, stabilize) combined with energy-based swing-up logic and adaptive integral action with anti-windup. Angle normalization and momentum-aware control enhance robustness under high-friction, heavy-pole conditions.  \n- **Performance**: Achieved a combined score of 4478.10 with full success bonus, low final errors, and efficient energy use (avg 0.01 per step).  \n- **Feedback**: The phase-adaptive strategy effectively handles the challenging dynamics (long, heavy pole; high friction), enabling rapid stabilization (253 steps) and maintaining balance with minimal energy expenditure. Integral action and smooth phase transitions prevent windup and ensure precision.\n**Program Identifier:** Generation 156 - Patch Name none - Correct Program: True\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with cross-coupled state weighting, adaptive gain scheduling, energy-based swing-up assistance for large angles, and enhanced integral action with angle/velocity gating and decay.  \n- **Performance**: Achieved a high combined score of 4953.84, with strong stabilization (final theta error \u2248 0), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The energy-aware swing-up and adaptive gains effectively handled the challenging heavy/long pole, while integral control with tight gating improved cart centering without destabilizing the system near equilibrium.\n**Program Identifier:** Generation 157 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Energy-Based Swing-Up**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, dual-gated integral action for cart position, and energy-based swing-up assistance triggered by large pole angles. The controller uses physically accurate linearization and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4947.50, with strong time, energy, and success bonuses, stabilizing the pendulum in 188 steps despite extreme parameters (heavy, long pole; high friction).  \n- **Feedback**: The energy-based swing-up and adaptive gains effectively handled large initial deviations, while dual-gated integral control improved steady-state cart positioning without destabilizing the pole. Evaluation confirms robustness and efficiency under challenging dynamics.\n**Program Identifier:** Generation 158 - Patch Name adaptive_lqr_integral_v2 - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in a complete lack of functionality for any intended algorithm or task.  \n- **Performance**: Combined score is 0.0, indicating failure to pass any validation tests.  \n- **Feedback**: An empty implementation cannot perform computation or meet problem requirements; adding core logic is essential for correctness and scoring.\n**Program Identifier:** Generation 159 - Patch Name energy_coupled_lqr - Correct Program: False\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, predictive falling severity estimation, and enhanced integral action with state-dependent gating and decay. The controller uses a physically accurate linearized model and cross-coupled Q-matrix terms for coordinated cart-pole dynamics.  \n- **Performance**: Achieved a high combined score of 4953.84, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 187 steps), and full success bonus.  \n- **Feedback**: The energy-aware swing-up logic and adaptive gains effectively handled large initial angles and system instability from the heavy, long pole. Integral control with tight velocity/angle gating prevented windup while improving steady-state cart positioning, contributing to low final errors and high time/energy bonuses.\n**Program Identifier:** Generation 160 - Patch Name enhanced_swingup_with_momentum_prediction - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines gain-scheduled LQR control with adaptive integral action, enhanced angular damping, and physics-informed swing-up assistance to stabilize a challenging inverted pendulum (heavy, long pole with high friction).  \n- **Performance**: Achieved a high combined score of 4933.93, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00), stabilizing in 190 steps.  \n- **Feedback**: The adaptive gains and integral clamping effectively handled large initial angles and high system instability, while the swing-up logic provided critical recovery during aggressive falls\u2014key to robust performance under extreme parameters.\n**Program Identifier:** Generation 161 - Patch Name adaptive_integral_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines physically accurate linearized dynamics with gain-scheduled LQR, swing-up assistance for large angles, and gated integral action with anti-windup to handle a challenging high-friction, long/heavy-pole inverted pendulum.  \n- **Performance**: Achieved a high combined score of 4920.84 with full success bonus, low energy use (6.58 total), and fast stabilization (192 steps).  \n- **Feedback**: The sharpened gain scheduling and angle-gated integral control effectively balanced aggressive swing-up and precise stabilization, while the tuned LQR weights (notably Q[3]=3.2) and friction-aware modeling were critical to robustness under difficult physical parameters.\n**Program Identifier:** Generation 162 - Patch Name adaptive_integral_lqr_hybrid - Correct Program: True\n\n**Program Name: Adaptive Energy-Aware LQR with Swing-Up Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, directional momentum-aware swing-up logic, and soft-switched integral action for cart position, all tuned for extreme pendulum parameters (heavy, long pole; high friction).  \n- **Performance**: Achieved a high combined score of 4927.07, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and stable final state.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large deviations with precise LQR stabilization near upright, while integral action eliminates steady-state position error without causing overshoot\u2014key to handling the challenging system dynamics.\n**Program Identifier:** Generation 163 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Energy-Based Swing-Up**\n\n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, dual-gated integral action for cart position, predictive momentum compensation, and energy-based swing-up assistance tailored to extreme pendulum parameters (heavy, long pole with high friction).\n\n- **Performance**: Achieved a high combined score of 4947.47, with strong bonuses for success (800), time (2261), and energy efficiency (2492), stabilizing in 188 steps with near-zero final errors.\n\n- **Feedback**: The controller effectively handles challenging initial conditions and system dynamics through intelligent nonlinear enhancements; the energy-based swing-up and adaptive gains were critical for rapid stabilization and robustness under high friction and instability.\n**Program Identifier:** Generation 164 - Patch Name predictive_momentum_compensation - Correct Program: True\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance, predictive falling severity estimation, and adaptive integral action that activates only near equilibrium; includes cross-coupled terms in the Q-matrix and state-dependent gain scheduling for robustness.  \n- **Performance**: Achieved a high combined score of 4953.83 with strong time, energy, and success bonuses, stabilizing the pendulum in 187 steps with minimal final errors.  \n- **Feedback**: The controller effectively handles the challenging pole parameters (longer, heavier) by blending aggressive swing-up logic with precise stabilization; integral control and predictive momentum compensation significantly improve centering and robustness without excessive energy use.\n**Program Identifier:** Generation 165 - Patch Name predictive_momentum_compensation - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action gated by pole angle/velocity, and energy-based swing-up assistance for large deviations. Uses physically accurate friction modeling in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4926.68 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle the challenging heavy/long pole configuration, while gated integral action prevents destabilization during transients\u2014key to balancing speed and stability.\n**Program Identifier:** Generation 166 - Patch Name enhanced_swing_up_with_energy_awareness - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines physically accurate linearized dynamics with LQR control (Q[3]=3.2), gain scheduling based on angle/velocity thresholds, energy-aware swing-up logic, and gated integral action with anti-windup for cart centering.  \n- **Performance**: Achieved a high combined score of 4920.85 with full success bonus, low energy use (avg 0.01/step), and rapid stabilization (192 steps).  \n- **Feedback**: The sharpened gain scheduling and energy-aware swing-up effectively handled the challenging heavy/long pole, while the simplified integral gating proved robust for steady-state accuracy without windup issues.\n**Program Identifier:** Generation 167 - Patch Name enhanced_swing_up_with_energy_awareness - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control and Swing-Up**\n\n- **Implementation**: Uses an enhanced adaptive LQR controller with integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, swing-up assistance triggered at large angles, and mid-swing angular damping. The dynamics include high-friction terms and a heavy, long pole to increase difficulty.\n\n- **Performance**: Achieved a combined score of 0.00 despite a base score of 7.16, failing to stabilize the pendulum (final theta error: 2.94 rad, final x error: 4432 m) over 1000 steps.\n\n- **Feedback**: The controller fails to stabilize the system due to excessive drift in cart position and inability to recover from large initial angles; integral action may be too weak or improperly gated, and swing-up logic might interfere with stabilization rather than assist it.\n**Program Identifier:** Generation 168 - Patch Name sharper_gain_scheduling - Correct Program: False\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines LQR with velocity-gated integral action, gentle gain scheduling, and swing-up assistance for large angles; uses physically accurate linearized dynamics and proven Q/R weights.  \n- **Performance**: Achieved a high combined score of 4954.47 with full success bonus, low energy use (6.51 total), and fast stabilization (187 steps).  \n- **Feedback**: The hybrid approach effectively balances aggressive swing-up for large deviations with precise LQR control near upright, while velocity-gated integration prevents windup and improves stability.\n**Program Identifier:** Generation 169 - Patch Name adaptive_integral_lqr_hybrid - Correct Program: True\n\n**Program Name: Adaptive Cross-Coupled LQR Controller**  \n- **Implementation**: Uses an adaptive LQR with state-dependent cross-coupling in the Q-matrix, mid-swing angular damping, predictive momentum compensation, and dual-gated integral control to stabilize a highly unstable inverted pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a combined score of 5015.73 with full success bonus, low energy use (7.54 total), and fast stabilization (178 steps).  \n- **Feedback**: The controller effectively handles large initial angles and aggressive dynamics through adaptive gains and predictive corrections; the dual-gated integral term prevents windup while aiding precision near upright equilibrium.\n**Program Identifier:** Generation 170 - Patch Name adaptive_cross_coupled_lqr_with_midswing_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with gain scheduling, robust swing-up logic based on energy deficit and momentum prediction, velocity-gated integral action for cart centering, and enhanced damping near equilibrium. The controller uses a physically accurate linearized model and adaptive gains that increase responsiveness during large deviations.  \n- **Performance**: Achieved a high combined score of 4756.64 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps with minimal final errors.  \n- **Feedback**: The hybrid approach effectively handles the challenging dynamics (heavy, long pole; high friction), with integral action and swing-up assist crucial for recovery and stabilization. Energy-efficient control and precise gain scheduling contributed to low average energy use and fast settling.\n**Program Identifier:** Generation 171 - Patch Name none - Correct Program: True\n\n**Program Name: Phase-Aware Momentum-Cancellation LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with phase-based adaptive gains, momentum-aware integral action, and energy-injection recovery logic tailored to a highly unstable inverted pendulum (heavy, long pole with high friction). Uses stability-index-driven phase blending, predictive feedforward, and dual-gated integrals for precision stabilization.  \n- **Performance**: Achieved a high combined score of 4967.97, with strong time, energy, and success bonuses, stabilizing the system in 185 steps with near-zero final errors.  \n- **Feedback**: The controller effectively handles extreme initial conditions and physical challenges through its hybrid strategy; evaluation confirms robustness, efficient energy use, and precise stabilization, validating the novel momentum-integral and phase-aware design choices.\n**Program Identifier:** Generation 172 - Patch Name phase_aware_momentum_lqr - Correct Program: True\n\n**Program Name: Hybrid Adaptive Cross-Coupled LQR Controller**  \n- **Implementation**: Combines cross-coupled LQR with mid-swing damping, sharpened gain scheduling, predictive momentum compensation, and angle-gated integral control to stabilize a challenging inverted pendulum (heavy, long pole; high friction; large initial angle).  \n- **Performance**: Achieved a high combined score of 4983.31 with full success bonus, low energy use (7.54 total), and fast stabilization (183 steps).  \n- **Feedback**: The controller effectively balances aggressive swing-up for large angles with precise linear regulation near upright, while adaptive gains and predictive terms prevent overshoot and divergence; integral gating avoids windup during transients.\n**Program Identifier:** Generation 173 - Patch Name adaptive_crosscoupled_lqr_with_midswing_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Predictive Damping**\n\n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, predictive angular acceleration estimation for mid-swing damping, energy-aware integral action, and swing-up assistance for large angles. Uses physically accurate linearized dynamics and friction-aware modeling.\n\n- **Performance**: Achieved a high combined score of 4934.14, with strong time, energy, and success bonuses, stabilizing the pendulum in 190 steps (19% of max time) and maintaining near-zero final errors.\n\n- **Feedback**: The integration of predictive damping and energy-based integral gain adaptation significantly improved robustness under challenging conditions (heavy, long pole; high friction). Early stabilization and low energy usage indicate efficient, proactive control that balances swing-up and balancing phases effectively.\n**Program Identifier:** Generation 174 - Patch Name enhanced_adaptive_lqr_with_predictive_damping - Correct Program: True",
        "meta_scratch_pad": "## Successful Algorithmic Patterns\n\n- **State-dependent cross-coupling in the LQR Q-matrix significantly improves coordination between angle and angular velocity**, enabling faster stabilization under high-inertia conditions. The current best program (*Adaptive Cross-Coupled LQR Controller*, **5015.73**) introduces off-diagonal terms `Q[1,3] = Q[3,1] = 0.8`, dynamically boosted by `cross_coupling_boost = 1.0 + 0.5 * tanh(3.0 * abs(theta))`. This yields the fastest stabilization time (**178 steps**) among all evaluated programs\u2014beating even the previously top-performing Generation 165 (**187 steps**)\u2014and contributes directly to its record **time_bonus: 2328.30**.\n\n- **Gaussian-shaped mid-swing damping provides targeted momentum suppression without destabilizing near-equilibrium control**. The current best uses `mid_swing_activation = exp(-4.0 * (abs(theta) - 0.5)**2)` to apply damping only in the critical **0.25\u20130.85 rad** range, avoiding interference with swing-up or balancing phases. This contrasts with blunt constant damping and explains why this program achieves **stable_x_error: 0.05** despite aggressive swing-up, outperforming *Hybrid Adaptive Integral LQR* (**4756.64**) which lacks such phase-specific damping.\n\n- **Predictive momentum compensation using estimated angular acceleration enables earlier counteraction of divergence**, especially crucial for the heavy, long pole (`M_POLE=0.35`, `L_POLE=2.5`). The current best estimates `theta_acc_est` from gravity and joint friction, then computes `pred_divergence = theta * dtheta_pred / (omega_n * L_COM)` to generate a corrective term. This proactive strategy\u2014absent in Generations 171 and 174\u2014contributes to its near-zero final errors (**final_theta_error: 0.00**, **final_x_error: 0.00**) and supports its elite energy efficiency (**total_energy: 7.54**) by reducing corrective overactuation.\n\n- **Dual-gated integral control with exponential decay during inactivity maintains precision without windup**, building on prior successful patterns but refined for extreme dynamics. The current best uses identical gating (`angle_gate = tanh(12.0*(0.1\u2212|\u03b8|))`, `velocity_gate = tanh(8.0*(1.0\u2212|d\u03b8|))`) as the previous best (**4954.47**), but couples it with **integral decay (`*= 0.95`)** when inactive\u2014a technique validated in Generation 167. This synergy ensures rapid reactivation near equilibrium, achieving **stable_x_error: 0.05**, matching the precision of top predecessors while operating under more challenging physical parameters.\n\n## Ineffective Approaches\n\n- **Omission of predictive or mid-swing damping mechanisms leads to momentum overshoot and slower stabilization**, even with otherwise strong LQR foundations. *Hybrid Adaptive Integral LQR Controller* (**4756.64**)\u2014despite excellent integral gating and gain scheduling\u2014stabilizes in **215 steps**, **37 steps slower** than the current best, due to lack of explicit mid-phase damping or predictive correction. This confirms that reactive LQR alone is insufficient for the heaviest/longest poles tested.\n\n- **Swing-up logic that lacks falling-severity normalization wastes energy and delays transition to balancing**. While all top programs use swing-up, the current best\u2019s use of `falling_factor = 1.0 + tanh(2.0 * theta * dtheta / (L_COM * omega_n))` makes assistance directionally intelligent. In contrast, *Adaptive LQR with Predictive Damping* (**4934.14**) uses less refined swing-up, resulting in **190-step stabilization**\u20147 steps slower than the current best\u2014indicating that unnormalized swing-up is suboptimal under high inertia.\n\n- **Static or weakly adaptive cross-coupling fails to exploit state-dependent coordination benefits**. Earlier correct programs like Generation 171 (**4756.64**) and 172 (**4967.97**) omit cross-coupled Q-terms entirely, relying solely on diagonal weights and gain scheduling. Their scores\u2014while high\u2014are consistently **>48 points below** the current best (**5015.73**), demonstrating that cross-coupling is not merely optional but a key differentiator at the performance frontier.\n\n## Implementation Insights\n\n- **The current best program integrates four synergistic layers\u2014adaptive cross-coupled LQR, Gaussian mid-swing damping, predictive momentum compensation, and dual-gated integral control\u2014into a single coherent architecture**, each layer addressing a distinct phase of the control challenge. Unlike piecemeal approaches, this holistic design ensures smooth transitions: swing-up activates only beyond **0.8 rad**, mid-swing damping peaks at **0.5 rad**, predictive correction scales with divergence, and integral action engages only within **0.1 rad** and low velocity. This layered phase-awareness is absent in all lower-scoring programs.\n\n- **Precise friction-aware linearized dynamics in the A-matrix are non-negotiable for zero final angle error under high joint friction (`FRICTION_JOINT=0.25`)**. The current best explicitly models joint friction in `A[3,3] = -b_j / (m*l*denom0)`, matching the structure used in all correct programs. This fidelity\u2014combined with the proven Q-matrix `diag([4.5, 44.0, 0.6, 3.2])`\u2014enables perfect upright stabilization (**final_theta_error: 0.00**), whereas even minor model inaccuracies would cause residual oscillation under these extreme parameters.\n\n- **Force clipping redundancy (inside controller and simulation loop) ensures robustness during aggressive swing-up maneuvers**, such as the current best\u2019s `u_swing = 8.0 * ...` term. This dual safeguard prevents numerical overflow and guarantees actuator compliance, directly enabling its full **success_bonus: 800.00** across all test conditions\u2014a pattern consistently present in all correct programs but missing in failed ones like Generation 168.\n\n- **Natural frequency normalization (`omega_n = sqrt(G / L_COM)`) enables scalable, physics-informed decision-making across varying pole lengths and masses**. The current best uses `omega_n` to normalize both predictive divergence (`theta * dtheta_pred / (omega_n * L_COM)`) and falling severity, ensuring control intensity adapts to the actual dynamic timescale. This explains its ability to maintain **avg_energy_per_step: 0.01**\u2014matching saturated elite efficiency\u2014while stabilizing **9 steps faster** than Generation 174, which uses similar but less normalized predictive logic.\n\n## Performance Analysis\n\n- **The current best program (**5015.73**) sets a new performance ceiling, surpassing all predecessors by \u226532 points**, primarily through unprecedented speed (**178 steps**) rather than energy savings (which remain saturated at ~7.5 total). Its **time_bonus: 2328.30** exceeds the previous best (**2267.91**) by **60.39 points**, confirming that **swing-up and mid-phase efficiency\u2014not just balancing precision\u2014is now the dominant scoring factor** under extreme physical parameters.\n\n- **Cross-coupled LQR with predictive damping outperforms hybrid integral-LQR and phase-aware variants**, despite their sophistication. *Phase-Aware Momentum-Cancellation LQR* (**4967.97**) and *Hybrid Adaptive Cross-Coupled LQR* (**4983.31**) both score **>32 points below** the current best, indicating that **explicit cross-coupling in the cost function combined with Gaussian mid-swing damping provides superior transient control** compared to phase-blending or sharpened gain scheduling alone.\n\n- **Energy efficiency has plateaued across all correct implementations (total_energy \u2248 6.5\u20137.54)**, making time-to-stabilization the sole meaningful differentiator above **4920**. The current best\u2019s **178-step stabilization**\u2014**7\u201337 steps faster** than peers\u2014accounts for nearly all of its score advantage, as all top programs earn the same **success_bonus: 800.00** and achieve near-identical final errors.\n\n- **The combination of predictive compensation and mid-swing damping uniquely enables both fast swing-up and minimal overshoot**, resolving a key trade-off that limited earlier controllers. Programs without both features\u2014e.g., Generation 171 (no predictive/mid-swing) or Generation 174 (predictive but no Gaussian damping)\u2014show measurably slower convergence, proving that **proactive momentum management across the entire swing-up trajectory is essential for record performance**.",
        "embed_cost": 0.0,
        "novelty_cost": 0.0,
        "stdout_log": "Evaluating program: results_pendulum_liter/gen_177/main.py\nSaving results to: results_pendulum_liter/gen_177/results\nEvaluation error: [Errno 2] No such file or directory: '/home/lance/Documents/sakana/shinka/ShinkaEvolve/examples/pendulum_liter/results_pendulum_liter/gen_177/main.py'\nCorrectness and error status saved to results_pendulum_liter/gen_177/results/correct.json\nMetrics saved to results_pendulum_liter/gen_177/results/metrics.json\nEvaluation or Validation failed: [Errno 2] No such file or directory: '/home/lance/Documents/sakana/shinka/ShinkaEvolve/examples/pendulum_liter/results_pendulum_liter/gen_177/main.py'\nMetrics:\n  combined_score: 0.0\n  execution_time_mean: 0.0\n  execution_time_std: 0.0\n  num_successful_runs: 0\n  num_valid_runs: 0\n  num_invalid_runs: 0\n  all_validation_errors: []\n",
        "stderr_log": "",
        "code_analysis_metrics": {
          "cyclomatic_complexity": 0,
          "average_cyclomatic_complexity": 0,
          "halstead_volume": 0,
          "halstead_difficulty": 0,
          "halstead_effort": 0,
          "lines_of_code": 0,
          "logical_lines_of_code": 0,
          "comments": 0,
          "maintainability_index": 171.0,
          "max_nesting_depth": 0,
          "complexity_score": 0.0
        }
      },
      "in_archive": false
    },
    {
      "id": "fc8c652f-1991-4ed4-a4b0-c3e2e9cc5294",
      "code": "import numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    MPC-Inspired Phase-Switching Controller with Momentum Nullification\n    Uses short-horizon trajectory evaluation combined with LQR for precision control.\n    \"\"\"\n\n    def __init__(self):\n        # Precompute LQR gains for precision phase\n        m, M, l, g = M_POLE, M_CART, L_COM, G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c, b_j = FRICTION_CART, FRICTION_JOINT\n\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n        self.K = self._solve_lqr(A, B, Q, R)\n\n        self.integral_x = 0.0\n        self.K_i = 0.8\n        self.omega_n = np.sqrt(G / L_COM)\n        \n        # MPC parameters\n        self.horizon = 3\n        self.n_candidates = 7\n\n    def _solve_lqr(self, A, B, Q, R):\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        return np.linalg.inv(R) @ B.T @ P\n\n    def _predict_trajectory(self, state, force, steps):\n        \"\"\"Simulate forward trajectory\"\"\"\n        s = state.copy()\n        cost = 0.0\n        for _ in range(steps):\n            s = simulate_pendulum_step(s, force, DT)\n            # Weighted cost emphasizing angle stability\n            cost += 44.0 * s[1]**2 + 4.5 * s[0]**2 + 0.6 * s[2]**2 + 3.2 * s[3]**2\n        return cost, s\n\n    def _mpc_search(self, state, base_force):\n        \"\"\"Search for best control action using trajectory evaluation\"\"\"\n        best_force = base_force\n        best_cost = float('inf')\n        \n        # Generate candidate forces around base_force\n        spread = min(30.0, 10.0 + 20.0 * abs(state[1]))\n        candidates = np.linspace(base_force - spread, base_force + spread, self.n_candidates)\n        candidates = np.clip(candidates, -100.0, 100.0)\n        \n        for f in candidates:\n            cost, final_state = self._predict_trajectory(state, f, self.horizon)\n            # Add control effort penalty\n            cost += 0.5 * f**2 * self.horizon\n            # Penalty for predicted failure\n            if abs(final_state[1]) > 0.95 or abs(final_state[0]) > 9.0:\n                cost += 10000.0\n            if cost < best_cost:\n                best_cost = cost\n                best_force = f\n        \n        return best_force\n\n    def get_action(self, state):\n        x, theta, dx, dtheta = state\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n        \n        state_vec = np.array([x, theta, dx, dtheta])\n        \n        # Compute base LQR force\n        base_force = float((-self.K @ state_vec)[0])\n        \n        # Phase detection\n        abs_theta = abs(theta)\n        abs_dtheta = abs(dtheta)\n        \n        # Recovery phase: large angles\n        if abs_theta > 0.5:\n            # Adaptive gain scheduling\n            pos_gain = 1.0 + 0.6 * np.tanh(5.0 * (abs_theta - 0.5))\n            vel_gain = 1.0 + 0.35 * np.tanh(4.0 * (abs_dtheta - 0.8))\n            force = base_force * pos_gain * vel_gain\n            \n            # MPC refinement for critical situations\n            if abs_theta > 0.7:\n                force = self._mpc_search(state_vec, force)\n            \n            # Momentum-aware swing assist\n            if abs_theta > 0.8:\n                swing_act = np.tanh(6.0 * (abs_theta - 0.8))\n                # Normalized momentum term\n                norm_mom = (theta * dtheta) / (self.omega_n * L_COM)\n                falling_factor = 1.0 + np.tanh(2.5 * norm_mom)\n                force += 8.0 * swing_act * np.sign(theta) * falling_factor\n        \n        # Transition phase: moderate angles\n        elif abs_theta > 0.15:\n            # Smooth gain scheduling\n            pos_gain = 1.0 + 0.4 * np.tanh(6.0 * (abs_theta - 0.15))\n            vel_gain = 1.0 + 0.25 * np.tanh(5.0 * (abs_dtheta - 0.5))\n            force = base_force * pos_gain * vel_gain\n            \n            # Sliding mode inspired damping\n            lambda_s = 2.0 * self.omega_n\n            s = dtheta + lambda_s * theta\n            force += -0.6 * np.tanh(2.0 * s)\n        \n        # Precision phase: small angles\n        else:\n            force = base_force\n            \n            # Dual-gated integral control\n            angle_gate = np.tanh(15.0 * (0.12 - abs_theta))\n            vel_gate = np.tanh(10.0 * (1.0 - abs_dtheta))\n            gate = angle_gate * vel_gate\n            \n            if gate > 0.1:\n                self.integral_x += x * DT\n            else:\n                self.integral_x *= 0.92\n            \n            force += self.K_i * gate * self.integral_x\n            \n            # Precision cart damping\n            cart_damp = -0.12 * dx * (0.15 - abs_theta) / 0.15\n            force += cart_damp\n            \n            # Angular momentum nullification\n            momentum_correction = -0.08 * dtheta * np.sign(theta + 1e-8)\n            force += momentum_correction\n\n        return float(force)\n\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)",
      "language": "python",
      "parent_id": "fba93c49-b8aa-4513-b1f1-1f72fc863346",
      "archive_inspiration_ids": [
        "cf208ebc-01a7-4a3b-8051-e3fbe4c0dccf",
        "c1c49a4d-70c9-4bed-8939-4d8b31aacd11",
        "1be84b3f-242f-4dd7-8277-fad2553a48cc",
        "d37fa0dd-a945-435f-9512-1e41054e7d5b"
      ],
      "top_k_inspiration_ids": [
        "738b86a0-a93c-489b-af98-690c689ef5a2",
        "ce1336dc-62df-48df-8593-ee6b652da3bf",
        "84690baa-a822-495f-90f0-f48541e92d68"
      ],
      "island_idx": 1,
      "generation": 178,
      "timestamp": 1764096920.1108706,
      "code_diff": "--- a/original.py\n+++ b/original.py\n@@ -1,218 +1,257 @@\n import numpy as np\n \n # --- Physics Constants ---\n M_CART = 1.0       # Mass of the cart (kg)\n M_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\n L_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\n L_COM = L_POLE / 2 # Length to center of mass (m)\n G = 9.81           # Gravity (m/s^2)\n FRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\n FRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\n DT = 0.02          # Time step (s)\n MAX_STEPS = 1000   # 20 seconds simulation\n \n def simulate_pendulum_step(state, force, dt):\n     \"\"\"\n     Simulates one time step of the Single Inverted Pendulum.\n \n     State vector: [x, theta, dx, dtheta]\n     - x: Cart position (m)\n     - theta: Pole angle (rad), 0 is upright\n     - dx: Cart velocity (m/s)\n     - dtheta: Pole angular velocity (rad/s)\n \n     Args:\n         state: numpy array of shape (4,)\n         force: scalar float, force applied to the cart (N)\n         dt: float, time step (s)\n \n     Returns:\n         next_state: numpy array of shape (4,)\n     \"\"\"\n     x, theta, dx, dtheta = state\n \n     # Precompute trig terms\n     sin_theta = np.sin(theta)\n     cos_theta = np.cos(theta)\n \n     # Equations of Motion (Non-linear)\n     # derived from Lagrangian dynamics\n \n     # Total mass\n     M_total = M_CART + M_POLE\n \n     # Friction forces\n     f_cart = -FRICTION_CART * dx\n     f_joint = -FRICTION_JOINT * dtheta\n \n     # Denominator for solving linear system of accelerations\n     # Derived from solving the system:\n     # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n     # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n \n     temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n \n     theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                 (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n \n     x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n \n     # Euler integration\n     next_x = x + dx * dt\n     next_theta = theta + dtheta * dt\n     next_dx = dx + x_acc * dt\n     next_dtheta = dtheta + theta_acc * dt\n \n     return np.array([next_x, next_theta, next_dx, next_dtheta])\n \n \n # EVOLVE-BLOCK-START\n class Controller:\n     \"\"\"\n-    Hybrid Adaptive Integral LQR with Enhanced Damping and Robust Integral Control\n-    Combines proven gain scheduling with sophisticated integral action and enhanced\n-    angular velocity damping for superior stabilization performance.\n+    MPC-Inspired Phase-Switching Controller with Momentum Nullification\n+    Uses short-horizon trajectory evaluation combined with LQR for precision control.\n     \"\"\"\n \n     def __init__(self):\n-        # System parameters\n-        m = M_POLE\n-        M = M_CART\n-        l = L_COM\n-        g = G\n+        # Precompute LQR gains for precision phase\n+        m, M, l, g = M_POLE, M_CART, L_COM, G\n         Mtot = M + m\n         denom0 = l * (4.0 / 3.0 - m / Mtot)\n-        b_c = FRICTION_CART\n-        b_j = FRICTION_JOINT\n-\n-        # Physically accurate linearized A matrix with corrected joint friction\n+        b_c, b_j = FRICTION_CART, FRICTION_JOINT\n+\n         A = np.zeros((4, 4))\n         A[0, 2] = 1.0\n         A[1, 3] = 1.0\n-\n-        # theta_acc row (3) - using corrected joint friction term matching simulator\n         A[3, 1] = g / denom0\n         A[3, 2] = b_c / (Mtot * denom0)\n-        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n-\n-        # x_acc row (2)\n+        A[3, 3] = -b_j / (m * l * denom0)\n         A[2, 1] = -(m * l / Mtot) * A[3, 1]\n         A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n         A[2, 3] = b_j / (Mtot * denom0)\n \n-        # B matrix\n         B = np.zeros((4, 1))\n         B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n         B[3, 0] = -1.0 / (Mtot * denom0)\n \n-        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n         Q = np.diag([4.5, 44.0, 0.6, 3.2])\n         R = np.array([[1.0]])\n-\n-        # Solve LQR gains\n-        self.K = self.solve_lqr(A, B, Q, R)\n-\n-        # Integral control parameters\n+        self.K = self._solve_lqr(A, B, Q, R)\n+\n         self.integral_x = 0.0\n-        self.K_i = 0.8  # Integral gain for cart position\n-\n-        # Natural frequency for normalized falling severity\n+        self.K_i = 0.8\n         self.omega_n = np.sqrt(G / L_COM)\n-\n-    def solve_lqr(self, A, B, Q, R):\n-        \"\"\"Solve continuous-time LQR\"\"\"\n+        \n+        # MPC parameters\n+        self.horizon = 3\n+        self.n_candidates = 7\n+\n+    def _solve_lqr(self, A, B, Q, R):\n         from scipy.linalg import solve_continuous_are\n         P = solve_continuous_are(A, B, Q, R)\n-        K = np.linalg.inv(R) @ B.T @ P\n-        return K\n+        return np.linalg.inv(R) @ B.T @ P\n+\n+    def _predict_trajectory(self, state, force, steps):\n+        \"\"\"Simulate forward trajectory\"\"\"\n+        s = state.copy()\n+        cost = 0.0\n+        for _ in range(steps):\n+            s = simulate_pendulum_step(s, force, DT)\n+            # Weighted cost emphasizing angle stability\n+            cost += 44.0 * s[1]**2 + 4.5 * s[0]**2 + 0.6 * s[2]**2 + 3.2 * s[3]**2\n+        return cost, s\n+\n+    def _mpc_search(self, state, base_force):\n+        \"\"\"Search for best control action using trajectory evaluation\"\"\"\n+        best_force = base_force\n+        best_cost = float('inf')\n+        \n+        # Generate candidate forces around base_force\n+        spread = min(30.0, 10.0 + 20.0 * abs(state[1]))\n+        candidates = np.linspace(base_force - spread, base_force + spread, self.n_candidates)\n+        candidates = np.clip(candidates, -100.0, 100.0)\n+        \n+        for f in candidates:\n+            cost, final_state = self._predict_trajectory(state, f, self.horizon)\n+            # Add control effort penalty\n+            cost += 0.5 * f**2 * self.horizon\n+            # Penalty for predicted failure\n+            if abs(final_state[1]) > 0.95 or abs(final_state[0]) > 9.0:\n+                cost += 10000.0\n+            if cost < best_cost:\n+                best_cost = cost\n+                best_force = f\n+        \n+        return best_force\n \n     def get_action(self, state):\n-        \"\"\"Adaptive LQR with integral action and robust swing-up assist\"\"\"\n         x, theta, dx, dtheta = state\n-\n-        # Normalize angle to [-pi, pi]\n-        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n-\n+        theta = np.arctan2(np.sin(theta), np.cos(theta))\n+        \n         state_vec = np.array([x, theta, dx, dtheta])\n-        base_force = -self.K @ state_vec\n-\n-        # Sharper gain scheduling with steeper transitions for crisper mode separation\n-        pos_gain = 1.0 + 0.5 * np.tanh(7.0 * max(0.0, abs(theta) - 0.6))\n-        vel_gain = 1.0 + 0.3 * np.tanh(6.0 * max(0.0, abs(dtheta) - 1.0))\n-\n-        # Combined multiplicative gain\n-        adaptive_gain = pos_gain * vel_gain\n-\n-        force = base_force * adaptive_gain\n-\n-        # Enhanced swing-up assist with normalized falling severity\n-        if abs(theta) > 0.8:\n-            # Physics-informed assist: push cart in direction to catch falling pole\n-            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n-            # If theta*dtheta > 0, pole is falling further - need stronger correction\n-            # Normalized by natural frequency of the pendulum for better scaling\n-            natural_freq = np.sqrt(G / L_COM)\n-            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * natural_freq))\n-            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n-            force = force + u_swing\n-\n-        # Soft-switched integral action for cart position with velocity gating\n-        # Only activates when pole is nearly upright (<0.1 rad) AND moving slowly\n-        # Velocity gate prevents windup during rapid transient passes through upright\n-        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n-        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n-        integral_gate = angle_gate * velocity_gate\n-\n-        if integral_gate > 0.1:  # Only update integral when gate is significant\n-            self.integral_x += x * DT\n+        \n+        # Compute base LQR force\n+        base_force = float((-self.K @ state_vec)[0])\n+        \n+        # Phase detection\n+        abs_theta = abs(theta)\n+        abs_dtheta = abs(dtheta)\n+        \n+        # Recovery phase: large angles\n+        if abs_theta > 0.5:\n+            # Adaptive gain scheduling\n+            pos_gain = 1.0 + 0.6 * np.tanh(5.0 * (abs_theta - 0.5))\n+            vel_gain = 1.0 + 0.35 * np.tanh(4.0 * (abs_dtheta - 0.8))\n+            force = base_force * pos_gain * vel_gain\n+            \n+            # MPC refinement for critical situations\n+            if abs_theta > 0.7:\n+                force = self._mpc_search(state_vec, force)\n+            \n+            # Momentum-aware swing assist\n+            if abs_theta > 0.8:\n+                swing_act = np.tanh(6.0 * (abs_theta - 0.8))\n+                # Normalized momentum term\n+                norm_mom = (theta * dtheta) / (self.omega_n * L_COM)\n+                falling_factor = 1.0 + np.tanh(2.5 * norm_mom)\n+                force += 8.0 * swing_act * np.sign(theta) * falling_factor\n+        \n+        # Transition phase: moderate angles\n+        elif abs_theta > 0.15:\n+            # Smooth gain scheduling\n+            pos_gain = 1.0 + 0.4 * np.tanh(6.0 * (abs_theta - 0.15))\n+            vel_gain = 1.0 + 0.25 * np.tanh(5.0 * (abs_dtheta - 0.5))\n+            force = base_force * pos_gain * vel_gain\n+            \n+            # Sliding mode inspired damping\n+            lambda_s = 2.0 * self.omega_n\n+            s = dtheta + lambda_s * theta\n+            force += -0.6 * np.tanh(2.0 * s)\n+        \n+        # Precision phase: small angles\n         else:\n-            self.integral_x *= 0.95  # Leaky integration when inactive\n-\n-        integral_force = self.K_i * integral_gate * self.integral_x\n-        force = force + integral_force\n-\n-        return float(force[0])\n-\n-# Initialize controller\n+            force = base_force\n+            \n+            # Dual-gated integral control\n+            angle_gate = np.tanh(15.0 * (0.12 - abs_theta))\n+            vel_gate = np.tanh(10.0 * (1.0 - abs_dtheta))\n+            gate = angle_gate * vel_gate\n+            \n+            if gate > 0.1:\n+                self.integral_x += x * DT\n+            else:\n+                self.integral_x *= 0.92\n+            \n+            force += self.K_i * gate * self.integral_x\n+            \n+            # Precision cart damping\n+            cart_damp = -0.12 * dx * (0.15 - abs_theta) / 0.15\n+            force += cart_damp\n+            \n+            # Angular momentum nullification\n+            momentum_correction = -0.08 * dtheta * np.sign(theta + 1e-8)\n+            force += momentum_correction\n+\n+        return float(force)\n+\n controller = Controller()\n \n def get_control_action(state):\n     force = controller.get_action(state)\n-    # Apply force limits\n     return float(np.clip(force, -100.0, 100.0))\n # EVOLVE-BLOCK-END\n \n def run_simulation(seed=None):\n     \"\"\"\n     Runs the simulation loop.\n     \"\"\"\n     if seed is not None:\n         np.random.seed(seed)\n \n     # Initial state: 0.4 rad (~23 degrees)\n     # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n     state = np.array([0.0, 0.9, 0.0, 0.0])\n \n     states = [state]\n     forces = []\n \n     for _ in range(MAX_STEPS):\n         force = get_control_action(state)\n         # Clip force to realistic limits\n         force = np.clip(force, -100.0, 100.0)\n \n         next_state = simulate_pendulum_step(state, force, DT)\n \n         states.append(next_state)\n         forces.append(force)\n \n         state = next_state\n \n         # # Early termination checks\n         # if np.any(np.isnan(state)):\n         #     break\n         # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n         # if abs(state[1]) > 1.0:\n         #     break\n \n     return np.array(states), np.array(forces)",
      "combined_score": 0.0,
      "public_metrics": {
        "score": 0.0,
        "base_score": 12.71844072470201,
        "time_bonus": 1.0063878837075355,
        "energy_bonus": 9.423786028745421e-06,
        "success_bonus": 0.0,
        "stabilization_time": 1000,
        "stabilization_ratio": 1.0,
        "avg_energy_per_step": 0.8684929373063232,
        "total_energy": 868.4929373063231,
        "mean_step_reward": 0.01271844072470201,
        "final_theta_error": 1.679098929090209,
        "final_x_error": 3838.716002358118,
        "stable_theta_error": 1.6159599267658076,
        "stable_x_error": 1430.486532346738
      },
      "private_metrics": {
        "oscillation_penalty": -1.0,
        "angle_oscillation_penalty": -374.0,
        "boundary_penalty": -2000.0,
        "unstable_penalty": -500.0,
        "max_theta": 3.133639422775996,
        "max_x": 3838.716002358118,
        "max_force": 100.0,
        "force_std": 37.20535644121505
      },
      "text_feedback": "",
      "correct": false,
      "children_count": 0,
      "complexity": 1.0,
      "embedding": [
        0.0026543913,
        0.008581816,
        0.010230885,
        -0.099793606,
        -0.010571214,
        0.011227613,
        -0.007856062,
        0.014873164,
        0.014482401,
        -0.009086849,
        0.012377739,
        -0.030042397,
        0.0021492867,
        0.02203925,
        0.12761994,
        0.010987629,
        0.0008531687,
        -0.029692914,
        0.00024738116,
        0.007752763,
        -0.00925115,
        0.008109486,
        -0.02853406,
        0.0026096602,
        -0.010241576,
        -0.01088792,
        0.024192715,
        0.0013813095,
        0.066479936,
        -0.006879399,
        0.0015150093,
        0.004184077,
        0.01960606,
        0.022442466,
        0.014088306,
        -0.012862355,
        0.0007062299,
        0.001107396,
        0.0011478863,
        0.025071949,
        -0.02907161,
        0.028531037,
        0.016425205,
        0.011087753,
        -0.009977097,
        0.0022151012,
        -0.0011977351,
        -0.024689749,
        -0.003435342,
        0.031795755,
        0.009202766,
        -0.008762213,
        0.0199744,
        -0.17667122,
        0.010225088,
        0.0028088656,
        -0.0029871499,
        0.0012198548,
        -0.010106541,
        0.015774408,
        -0.035027154,
        0.023236003,
        0.010637846,
        0.0012019376,
        -0.0031423287,
        0.005363306,
        0.020562451,
        0.008393707,
        0.0038635074,
        -0.016324623,
        0.0115233855,
        -0.01703297,
        0.013390884,
        3.080145e-05,
        0.0063981675,
        -0.022464622,
        0.02916317,
        -0.006838508,
        0.0023261406,
        0.046825938,
        0.003664543,
        0.010783404,
        -0.00024075006,
        -0.026206126,
        -0.0030511739,
        0.006281508,
        -0.002578788,
        -0.0012922122,
        0.009438895,
        -0.030057972,
        -0.012562477,
        -0.012237662,
        0.010774842,
        0.007568845,
        0.0038648036,
        0.0029358168,
        0.0065461462,
        -0.006831064,
        -0.021874575,
        -0.017296476,
        -0.023986422,
        -0.017174536,
        -0.0002545941,
        -0.005778193,
        -0.022994587,
        -0.009658539,
        0.0009809209,
        -0.003836205,
        -0.0009432019,
        -0.00095624896,
        0.0032978652,
        -0.017994318,
        -0.010425351,
        0.03606776,
        0.0052578636,
        -0.17959258,
        -0.0067413473,
        0.012469246,
        0.0072408863,
        0.0060297027,
        -0.0032394864,
        0.014658637,
        0.014707724,
        0.027903901,
        0.0065577934,
        -0.017387414,
        0.0076021818,
        -0.029519279,
        0.014982562,
        0.012039179,
        0.0059363744,
        -0.0023723308,
        -0.012915606,
        0.011921499,
        0.00559371,
        0.019654317,
        -0.008711458,
        -0.0032301622,
        -0.01391404,
        -0.014405541,
        -0.011300987,
        0.00800343,
        0.021277353,
        0.0068369918,
        -0.01805903,
        -0.038961153,
        0.0015078797,
        0.018673632,
        -0.009241079,
        -0.023659738,
        0.008264527,
        0.008122439,
        -0.009578408,
        -0.014112199,
        0.035717152,
        -0.04860676,
        0.0030760735,
        0.020977713,
        0.011855025,
        0.010709837,
        0.0059976294,
        -0.008595783,
        -0.0045684855,
        -0.008163445,
        -0.0034100893,
        -0.0051314416,
        0.015542078,
        -0.016352782,
        0.008070984,
        0.005529993,
        -0.011538042,
        0.0022084324,
        -0.0012567504,
        -0.012602528,
        0.025159,
        -0.018839974,
        -0.00875572,
        -0.010007071,
        -0.024702854,
        0.0028721862,
        -0.012775461,
        0.020035319,
        -0.022042304,
        0.02067883,
        -0.014568448,
        -0.013139389,
        -0.01373436,
        0.010358027,
        -0.002802275,
        -0.009074924,
        -0.0046824603,
        -0.019734869,
        -0.01258179,
        -0.026271334,
        -0.0047492785,
        0.0036285464,
        1.1580691e-05,
        0.00940038,
        0.006225683,
        0.021871869,
        0.01706677,
        -0.033546038,
        -0.023230972,
        -0.030423,
        0.004074207,
        -0.024428675,
        0.0075844307,
        -0.016960278,
        -5.2642048e-05,
        0.020043077,
        -0.024848858,
        0.0046874885,
        0.00081355707,
        -0.00013413178,
        0.011035037,
        0.00077686546,
        0.002509424,
        0.020334065,
        0.0009306559,
        0.012741346,
        0.035128072,
        0.0025702794,
        0.020976473,
        0.012331221,
        0.0048047313,
        -0.017071677,
        0.0018104193,
        0.0028658728,
        -0.027978161,
        0.013970792,
        0.005867608,
        0.014577262,
        -0.0072558164,
        0.0147726135,
        0.016159102,
        0.015514406,
        0.0032593461,
        0.04016389,
        0.013303599,
        0.020896383,
        0.024294186,
        -0.011310628,
        -0.0024496221,
        0.0016884027,
        0.008221846,
        0.022078337,
        0.013899652,
        -0.0054154424,
        0.0012435069,
        -0.015485723,
        0.008526193,
        0.001967123,
        -0.007930498,
        -0.005295261,
        0.0218993,
        -0.013754787,
        0.0029595555,
        -0.02493474,
        -0.0017290843,
        0.0037380762,
        -0.0026881343,
        0.004096634,
        0.008899155,
        -0.020752614,
        -0.022336736,
        -0.008413891,
        -0.011019479,
        0.00071589637,
        -0.008405036,
        -0.0074628824,
        0.020921784,
        0.017312272,
        -0.016358469,
        0.010910737,
        -0.0191798,
        0.02406887,
        -0.09033589,
        0.041134972,
        -0.026344031,
        -0.026793616,
        -0.0015770203,
        0.005685511,
        -0.0069180205,
        0.0052788937,
        -0.0040110555,
        0.024337914,
        -0.0053583914,
        -0.020086648,
        0.004124364,
        -0.010639711,
        -0.000174685,
        0.016051104,
        0.005587521,
        0.0171112,
        0.017393157,
        -0.0039687897,
        -0.0011513276,
        -0.0047641755,
        0.0045781173,
        0.005327451,
        0.024081431,
        0.016654601,
        0.019888647,
        0.023529677,
        0.007028719,
        -0.008512684,
        -0.0021384072,
        0.0063059856,
        -0.004190459,
        -0.017903961,
        -0.007441049,
        -0.002930443,
        -0.0063710385,
        -0.0014481894,
        0.0030277153,
        0.028786177,
        -0.01082907,
        -0.029220074,
        0.016816096,
        0.028298732,
        0.010788061,
        -0.01036218,
        -0.010823112,
        0.021114552,
        0.012224227,
        -0.001265625,
        -0.018754615,
        0.024855789,
        -0.0046310513,
        -0.0034337603,
        0.02282738,
        0.012995475,
        0.03763696,
        -0.0077516027,
        0.018270181,
        0.019053567,
        0.019644756,
        -0.006412573,
        -0.0063480968,
        0.010141381,
        -0.00442648,
        -0.0007986484,
        0.004285747,
        0.009418971,
        -0.017540071,
        0.0038854016,
        0.008635052,
        0.03220578,
        0.019017387,
        -0.023175403,
        -0.00460514,
        -0.0010027324,
        0.010821305,
        0.004703211,
        -0.01849378,
        0.04239236,
        0.012329246,
        0.010441686,
        0.008093646,
        0.0005024337,
        -0.000619739,
        -0.029579636,
        0.0022091437,
        -0.010523086,
        0.013574261,
        -0.0028618718,
        0.00444791,
        -0.010090562,
        -0.0011013265,
        0.027805168,
        -0.0017973321,
        -0.005447035,
        -0.014625804,
        -0.011603596,
        0.0009846772,
        0.008552289,
        -0.007819736,
        0.011649108,
        0.0020807153,
        0.008838886,
        0.0017276924,
        0.022652948,
        0.0014023389,
        0.0046223626,
        -0.02213051,
        -0.002702235,
        0.016096383,
        0.0013759236,
        -0.019516755,
        0.01605248,
        0.0037625385,
        0.00908245,
        -0.009505329,
        -0.013073422,
        -0.00017524755,
        -0.002980485,
        -0.0051166825,
        -0.0018843273,
        0.006635123,
        0.017029518,
        0.01693185,
        0.012419271,
        -0.015917884,
        0.00015913608,
        -0.001370077,
        -0.033341143,
        -0.012073432,
        0.009318005,
        -0.0011352341,
        -0.026212972,
        0.0010411558,
        -0.006937004,
        0.027988462,
        -0.0037275231,
        -0.007209841,
        0.009861188,
        -0.011485202,
        0.0054158,
        -0.00035442374,
        -0.009659746,
        0.0065834513,
        0.016955974,
        -0.034034383,
        0.019449783,
        -0.0025747535,
        -0.038033754,
        -0.011864044,
        -0.0034886322,
        0.001351754,
        -0.011069878,
        -0.0043112575,
        -0.008901392,
        -0.0055416194,
        -0.01278613,
        -0.0015477742,
        -0.019118672,
        0.0027335158,
        0.0077106133,
        -0.013818354,
        0.004855615,
        0.0027995745,
        -7.437546e-05,
        0.0076741125,
        0.008283022,
        0.00043884697,
        0.0026844034,
        0.0044117384,
        -0.008529792,
        -0.0073361755,
        0.011694984,
        0.003031613,
        -0.038993116,
        0.017014882,
        -0.014768747,
        -0.0038418926,
        0.009609794,
        0.0034989053,
        -0.0020059189,
        -0.00481992,
        -0.011297107,
        -0.006447507,
        -0.0036120666,
        0.02202795,
        0.003594725,
        0.0022995123,
        -0.012738773,
        0.0055495133,
        0.00013349432,
        -0.004413044,
        0.015978007,
        -0.011077626,
        0.003230517,
        0.0023877104,
        0.011243834,
        0.008269599,
        0.014160335,
        -0.013701478,
        0.013616105,
        0.0065775365,
        0.013270152,
        0.007537876,
        0.011673519,
        -0.023070414,
        -0.014037111,
        0.0073243673,
        0.020439293,
        -0.01086988,
        0.017429171,
        0.0029462927,
        -0.013309823,
        0.03607981,
        0.00573497,
        -0.0076682055,
        0.02187846,
        -0.014312209,
        0.00565066,
        -0.032939073,
        -0.0024873582,
        -0.008381209,
        0.0018085511,
        -0.0033907879,
        -0.009094096,
        0.0025648372,
        -0.010667453,
        -0.0065626646,
        -0.002484866,
        -0.012658551,
        -0.014550881,
        0.007975087,
        -0.0058075315,
        -0.0038860878,
        -0.0038740393,
        -0.0017455353,
        0.020170143,
        -0.032557152,
        -0.002751697,
        -0.005994991,
        -0.009645973,
        -0.028065566,
        0.004818727,
        0.014705605,
        -0.009687944,
        -0.023663111,
        0.009904709,
        -0.028793052,
        -0.016720839,
        0.008029516,
        0.0033745253,
        0.03046466,
        0.017825592,
        -0.015425174,
        0.016393539,
        0.008685705,
        -0.028980087,
        0.025852721,
        0.011740744,
        0.0019371499,
        -0.029768817,
        0.00083715044,
        0.015684042,
        0.0069892174,
        -0.005265762,
        0.009184725,
        0.00069282635,
        0.021754025,
        -0.12939939,
        0.008967387,
        -0.004807506,
        -0.024966951,
        0.007853421,
        0.00040495873,
        0.006654038,
        -0.0152547425,
        0.023814375,
        -0.011100472,
        0.00751706,
        0.016742205,
        0.0052263075,
        -0.0015810366,
        -0.005547425,
        -0.01649502,
        -0.0038173513,
        -0.018578243,
        0.009781573,
        -0.018043632,
        0.0089547625,
        0.018870322,
        0.014619767,
        0.01126326,
        -0.047593918,
        -0.022597784,
        0.0032579922,
        0.014831103,
        -0.009988932,
        -0.0032396426,
        0.0046930728,
        0.004759217,
        -0.009576541,
        0.0113635175,
        0.006766288,
        0.004487925,
        -0.0003133374,
        -0.0072588306,
        0.0024550036,
        0.0065588932,
        0.006320735,
        0.007956145,
        0.0031498591,
        -0.0087427795,
        -0.022865487,
        -0.0028829603,
        0.016254444,
        -0.02061152,
        -0.017615844,
        -0.0047240397,
        -0.034451876,
        -0.009788097,
        -0.013501928,
        -0.02977455,
        -0.022470258,
        -0.0034006434,
        -0.022720847,
        -0.006900997,
        -0.002188294,
        -0.0040360936,
        0.003728449,
        -0.011449808,
        0.0040428876,
        0.01563386,
        -0.035048492,
        0.01519657,
        0.027310748,
        0.021642083,
        0.013961389,
        -0.008043168,
        0.00069273904,
        -0.011594899,
        0.0135727115,
        0.0028734726,
        -0.012044037,
        0.012494906,
        -0.011924198,
        -0.0029363653,
        0.016920084,
        -0.00042207917,
        -0.008247504,
        -0.0059478167,
        -0.07542652,
        -0.010595143,
        0.013698993,
        -0.0045537706,
        -0.01782093,
        -0.0040070405,
        -0.03382877,
        0.0031735315,
        -0.033977743,
        -0.016182598,
        -0.0054498278,
        0.00076862634,
        0.006761791,
        -0.028136076,
        -0.00397501,
        0.008312341,
        -0.0111409575,
        -0.03700155,
        -0.016649075,
        -0.0020558764,
        0.01806423,
        0.0071400846,
        0.031046633,
        -0.004535009,
        -0.021590436,
        0.04063509,
        -0.0052729286,
        -0.017077215,
        -0.00064765016,
        0.008573193,
        0.006445624,
        -0.1510329,
        -0.0014723261,
        -0.018924419,
        0.008834824,
        0.0055744783,
        0.00654805,
        -0.017775062,
        -0.010209857,
        0.0071826223,
        -0.0028242336,
        0.010547345,
        0.019384662,
        -0.0035192606,
        -0.006188635,
        -0.0030586259,
        0.13312083,
        -0.022830749,
        0.019207692,
        -0.012769224,
        -0.021182036,
        -0.005658561,
        0.008204933,
        -0.0056679,
        0.007952189,
        -0.021709735,
        -0.02081112,
        -0.0016931369,
        0.00756888,
        0.0035108358,
        -0.0061741867,
        -0.0045108306,
        0.012753571,
        0.032343186,
        0.014344252,
        0.013402009,
        -0.018815428,
        0.016850036,
        -0.008494621,
        0.0022919106,
        0.014120423,
        -0.007211437,
        -0.010700916,
        0.003923606,
        -0.012990057,
        -0.0073033725,
        0.030140469,
        -0.01565795,
        6.164147e-05,
        0.016202416,
        0.01154441,
        -0.037127335,
        -0.07322559,
        -0.0034927996,
        -0.0061463956,
        -0.0010918637,
        -0.020310195,
        -0.01567168,
        -0.0067640757,
        -0.0003178966,
        0.015522589,
        0.017961716,
        0.0052354992,
        0.00010441034,
        0.007969236,
        0.00059483515,
        0.014521248,
        0.02344456,
        0.0006323087,
        0.0035414244,
        -0.003613946,
        -0.0106222145,
        -0.015713152,
        0.007192146,
        -0.017323343,
        -0.0049169823,
        -0.010775156,
        -0.016215283,
        0.009826098,
        0.018776918,
        0.00950358,
        0.007840707,
        0.00041161524,
        0.0018503598,
        -0.020324344,
        -0.009331671,
        0.0015722162,
        -0.02958394,
        0.018366704,
        -0.020276675,
        0.02624142,
        0.011098329,
        -0.0025897524,
        0.0027776738,
        0.025705853,
        0.007063692,
        0.018749155,
        -0.0028042884,
        -0.022366121,
        0.020088103,
        -0.0036284132,
        0.003623481,
        -0.025216194,
        0.017224165,
        -0.013726261,
        0.001919356,
        0.01947138,
        -0.017649833,
        0.017954899,
        -0.02471562,
        -0.010021097,
        -0.0008603977,
        0.016141634,
        0.0029619178,
        0.0023291688,
        0.010745366,
        0.004943367,
        -0.008500639,
        -0.017807702,
        0.0019626508,
        -0.01499688,
        0.001478204,
        -0.01408311,
        -0.010728842,
        0.007656007,
        0.0086340895,
        -0.011221735,
        0.010626665,
        0.008400325,
        -0.0036194918,
        -0.013763938,
        0.0074977446,
        0.012651045,
        0.0031211702,
        0.009502281,
        -0.012155173,
        0.004954723,
        -0.014004474,
        -0.0017616645,
        0.0059638578,
        -0.019145822,
        0.0073823654,
        -0.0008063315,
        0.009959219,
        -0.005317357,
        0.0018834237,
        0.0053455587,
        0.008304913,
        0.0046493397,
        0.0044088955,
        0.0045678713,
        -0.0021948093,
        -0.014246954,
        -0.005734953,
        -0.0038734481,
        -0.011749272,
        0.008724837,
        0.0062321136,
        0.0083171185,
        -0.0026090788,
        -0.0059519014,
        0.005137182,
        -0.0109934695,
        0.022275424,
        -0.006171929,
        0.0072145956,
        -9.5085496e-05,
        0.020036576,
        -0.010477139,
        -0.0067501166,
        0.0013406534,
        0.013916178,
        0.0132567845,
        -0.012081624,
        -0.00078120216,
        -0.005371916,
        -0.009785243,
        -0.011507293,
        -0.0032881114,
        -0.005748957,
        -0.006222768,
        0.002182202,
        -0.009388143,
        -0.0039858124,
        -0.0009800538,
        0.018331641,
        -0.0062892344,
        0.008437773,
        -0.0027449066,
        -0.021318465,
        0.0036225042,
        -0.0066473316,
        -0.020558642,
        0.017110778,
        -0.0064217616,
        -0.00180766,
        0.009294008,
        -0.0115589425,
        -0.009299221,
        0.006371152,
        -0.0015879823,
        -0.002892047,
        -0.003617808,
        -0.00020748995,
        0.0058272732,
        -0.009051284,
        0.022146014,
        0.006491043,
        0.00775355,
        -0.0010212376,
        -0.0015622127,
        0.00931318,
        0.0028261114,
        -0.01378726,
        -0.015243426,
        0.0031293982,
        -0.008007164,
        -0.00053024775,
        -0.0048005036,
        -0.008569178,
        0.005052865,
        -0.008920651,
        -0.00936861,
        0.009654944,
        -0.004508891,
        0.0011533306,
        0.0022820558,
        -0.0024552993,
        0.003325472,
        -0.0014023028,
        0.00525133,
        -0.017168913,
        -0.0023805413,
        -0.0027025877,
        -0.002259623,
        -0.0044935727,
        -0.005240654,
        -0.00021240502,
        -0.0047794515,
        0.00952601,
        0.0011652133,
        -0.0047270455,
        -0.0018844687,
        0.019865504,
        -0.006323009,
        -0.00036883782,
        0.0029407914,
        0.00047461787,
        0.0055943513,
        -0.006587911,
        0.00971772,
        -0.0050508366,
        0.017860398,
        0.018318664,
        0.0017270483,
        0.008955262,
        -0.013377128,
        -0.006802355,
        -0.0074929283,
        -0.0014752803,
        -0.015663426,
        0.009121106,
        0.01478305,
        0.011292919,
        -0.0022195037,
        0.020009689,
        0.0047935443,
        -0.0024965943,
        -0.013366062,
        -0.00681654,
        0.0002585276,
        -0.010753118,
        -0.014502633,
        0.001952663,
        0.0010854794,
        0.004078321,
        0.0035383143,
        -0.008790532,
        -0.008794187,
        -0.0100563895,
        0.011123345,
        -0.0019489338,
        -0.00041631368,
        0.0029473987,
        -0.005751232,
        -0.0021345115,
        0.01477082,
        0.0010166052,
        0.012835146,
        0.009917742,
        -0.018834954,
        -0.0059735244,
        0.011846028,
        -0.008724295,
        0.013615421,
        -0.00091901695,
        0.0015336917,
        0.024941275,
        0.00030673674,
        0.00149864,
        0.014513504,
        -0.0003729289,
        -0.0062078545,
        0.0017935297,
        -0.008724158,
        0.0013440778,
        -0.01380385,
        -0.01373753,
        -0.0008323103,
        -0.0016803101,
        -0.0043123816,
        0.110688,
        0.00665287,
        0.007158734,
        0.011771706,
        -0.004458453,
        -0.002938439,
        0.01186258,
        -0.021314794,
        0.00537543,
        0.00022674035,
        0.010862221,
        -0.0030973,
        -0.005939483,
        0.008534757,
        0.005866527,
        0.00855957,
        -0.007280876,
        0.000762585,
        -0.0006742749,
        0.007565869,
        -0.0045119864,
        0.02313202,
        0.013809461,
        -0.007656126,
        -0.0015720202,
        0.014823793,
        -0.0005921859,
        -0.0052475184,
        0.0023556405,
        0.016500188,
        0.010281364,
        -0.00799465,
        -0.010142447,
        0.014431658,
        -0.005244203,
        -0.00599752,
        -0.008311936,
        -0.0074291276,
        0.007987181,
        0.0084253065,
        -0.012835954,
        -0.0051133563,
        0.0029138422,
        -0.005461615,
        -0.0013342213,
        0.0008183976,
        0.0020003805,
        0.004491036,
        0.001850115,
        0.00022906116,
        0.0004521047,
        -0.0026502563,
        -0.01742275,
        0.0028321024,
        -0.003524274,
        0.00022202809,
        -0.00016238526,
        -0.004426072,
        -0.0066744112,
        -0.011331547,
        0.0001289467,
        0.019289088,
        -0.0028611475,
        -0.008988114,
        -0.0046647363,
        -0.008005187,
        -0.007290116,
        0.0047139907,
        -0.00937172,
        -0.013754622,
        0.007183847,
        0.002331178,
        0.0018340517,
        -0.0045899916,
        0.044451747,
        0.008321504,
        -0.005918469,
        0.011402472,
        -0.005694787,
        0.0022296628,
        -0.00013140008,
        0.0074859895,
        -0.013491502,
        0.013144266,
        0.00820552,
        -0.0045546475,
        -0.02181617,
        0.00514699,
        0.0057813968,
        -0.010531574,
        -0.002950434,
        -0.023833944,
        0.011626184,
        0.0048616445,
        0.0029180497,
        0.003476115,
        0.10328243,
        0.0016384281,
        -0.00078791974,
        -0.011856201,
        0.00061197364,
        0.0001216211,
        0.010134263,
        -0.005828203,
        0.004773311,
        -0.00021829961,
        -0.005517323,
        -0.0056470064,
        0.017649645,
        -0.017592818,
        0.001661926,
        -0.00578187,
        -0.005888575,
        0.0003150301,
        -0.00068501086,
        -0.0016718446,
        0.008537195,
        -0.0036142797,
        -0.0025156483,
        0.017934082,
        0.01050929,
        0.007331457,
        -0.00738565,
        0.014239746,
        -0.014453572,
        -0.003773255,
        -0.0047450582,
        -0.008657462,
        0.0056324606,
        0.006916355,
        -0.0073819454,
        0.009376283,
        0.018409709,
        -0.013073462,
        0.006334186,
        -0.0021654777,
        -0.0061196624,
        0.00032252015,
        0.004906033,
        -0.0025571098,
        -0.006796739,
        0.006339237,
        0.011490522,
        0.0024139562,
        -0.008255621,
        -0.019935697,
        -0.005184472,
        0.00016859593,
        0.024812294,
        0.007982519,
        0.0042726174,
        0.012634747,
        -0.00076400157,
        0.0038611852,
        0.0019986667,
        0.010370887,
        0.006772411,
        0.0046496056,
        0.006337725,
        0.010421897,
        -0.0061305114,
        0.0068160878,
        0.011544147,
        -0.000738413,
        0.00077578955,
        -0.0043791435,
        0.0044639916,
        0.016263913,
        -0.0051052943,
        0.012897671,
        0.014272759,
        0.0030172619,
        0.007841179,
        0.003848315,
        0.010367769,
        0.007927083,
        -0.013458436,
        -0.010282733,
        -0.009980375,
        0.016935213,
        -0.006517775,
        0.006384899,
        0.002398272,
        -0.0033191994,
        -0.0074785464,
        -0.009185773,
        0.013577882,
        0.010283768,
        0.015317003,
        0.002448045,
        0.011757568,
        -0.008554192,
        0.011117583,
        -0.00920255,
        -0.003028711,
        -0.0050290423,
        -0.0031414274,
        0.009410799,
        -0.013231948,
        -0.009254124,
        0.0063067926,
        0.0040816437,
        0.0055288696,
        -0.009170706,
        0.0010622316,
        0.015124373,
        -0.009006002,
        -0.0003499904,
        0.009913814,
        -0.016222322,
        -0.012426016,
        -0.009070102,
        0.010162924,
        0.02066661,
        0.0068769823,
        0.02756753,
        0.014498504,
        0.0013059587,
        0.002832799,
        -0.0058346624,
        -0.0070658326,
        -0.0046537216,
        0.0025856548,
        -0.0032453986,
        0.0065563186,
        0.002850837,
        -7.2336836e-05,
        -0.01168593,
        0.0070342086,
        0.00218388,
        0.011622577,
        -0.004661477,
        -0.007865104,
        -0.0029414513,
        -0.027602617,
        -0.0018203399,
        -0.0032812494,
        -0.004362593,
        -0.009578161,
        0.0029755612,
        -0.004755056,
        0.012332351,
        -0.003975473,
        -0.006154364,
        0.013855118,
        -0.014073293,
        -0.011863845,
        -0.0017068831,
        -0.014879743,
        -0.0022318475,
        -0.017346546,
        0.0069802087,
        0.010671397,
        0.0033336794,
        0.008659153,
        0.020584857,
        0.006730129,
        -0.07327115,
        0.00032425812,
        0.008421148,
        -0.008124639,
        -0.008051152,
        -0.007474888,
        -0.011325525,
        -0.0109763,
        0.007415072,
        0.00096549623,
        0.0013512933,
        -0.004849023,
        -0.0031186237,
        0.0016537497,
        0.0028537125,
        0.0058019585,
        0.0022188986,
        0.022251626,
        0.0002938111,
        -0.018826963,
        -0.0021587613,
        0.010778154,
        -0.010131954,
        0.006653376,
        -0.0052220942,
        0.003827855,
        5.2411022e-05,
        0.011155643,
        -0.024272269,
        0.014051154,
        0.0028047722,
        0.0025018926,
        0.017287575,
        0.010950572,
        8.3752886e-05,
        -0.018130535,
        -0.0109773725,
        -0.00037636544,
        0.002810323,
        -0.0034140893,
        0.010608645,
        -0.01631297,
        0.0026317537,
        -0.009536513,
        0.0065788245,
        -0.0073182425,
        0.01842634,
        0.0008854875,
        0.008659172,
        -0.004363538,
        -0.008705347,
        0.014745755,
        -0.0005146915,
        0.0023427121,
        0.0051244795,
        -0.014588452,
        0.009370515,
        -0.008853554,
        -0.018500106,
        0.0077343495,
        0.009399969,
        0.009928814,
        0.007738341,
        -8.739325e-05,
        -0.004064991,
        -0.007908127,
        0.0037062082,
        -0.004770574,
        -0.007555319,
        -0.00048295042,
        0.0059395963,
        0.002568376,
        -0.0073288796,
        -0.0047218553,
        -0.0021191821,
        0.0029743176,
        -0.00042805405,
        -0.009448437,
        -0.0048972405,
        0.0076240953,
        0.006270701,
        0.0010143893,
        -0.008983795,
        -0.017917939,
        0.009550106,
        0.004012137,
        -0.015026972,
        -0.0042476305,
        -0.016367432,
        0.015136364,
        -0.018067418,
        0.0075863185,
        -0.0035760903,
        -0.0068944865,
        0.0040948316,
        0.0057072025,
        0.00070803746,
        -0.0048048603,
        0.013599966,
        0.006734001,
        0.0008269926,
        -0.012732285,
        0.0028476452,
        0.0024529796,
        -0.013565214,
        0.0013670722,
        -0.0034284892,
        -0.013378252,
        0.005467282,
        0.0026476635,
        0.0045613865,
        0.0029213603,
        -0.013849818,
        -0.0040478753,
        -0.0037701936,
        -0.0074358867,
        -0.0002682002,
        -0.0070605585,
        -0.0036210124,
        -0.012325265,
        -0.0033093016,
        0.010541683,
        -0.0194658,
        -0.011109102,
        0.009253145,
        -0.015651701,
        0.006751129,
        0.0018796317,
        0.014070038,
        0.0006457525,
        0.0068485592,
        0.004998491,
        -0.0032726314,
        0.0033760106,
        -0.011249112,
        0.0038426006,
        0.0104549965,
        0.016497007,
        -0.010041399,
        -0.0070262677,
        0.008506425,
        -0.0023608399,
        0.008758925,
        -0.002366394,
        -0.0041708425,
        -0.010316538,
        0.0027513457,
        0.013353866,
        -0.00020375031,
        -0.0069787716,
        0.0027855015,
        0.008357656,
        0.000515194,
        0.00027941752,
        -0.005950019,
        -0.0018476773,
        0.006833757,
        -0.006232846,
        -0.006909852,
        -0.019004418,
        0.018372197,
        0.013312537,
        0.012637737,
        -0.014786372,
        0.010278875,
        0.00056944025,
        -0.011649555,
        -0.004725912,
        -0.014256515,
        -0.011454904,
        -0.018081514,
        0.0006224883,
        -0.003832851,
        -0.01673673,
        0.0051405006,
        -0.00019891198,
        0.01940192,
        -0.008076219,
        -0.0023206964,
        -0.0073504564,
        0.016810535,
        0.0066553797,
        -0.0044608866,
        0.0067358124,
        0.0017526082,
        -0.014683421,
        -0.007389753,
        0.0041197436,
        -0.0006592579,
        0.007189255,
        -0.0006885276,
        0.0059176194,
        0.024668833,
        -0.004509813,
        0.003857353,
        0.0007576185,
        0.012856179,
        0.0060650706,
        0.0020932117,
        -0.0011720761,
        0.0031734724,
        0.0025791428,
        0.005590014,
        0.0024921421,
        0.003849411,
        -0.121297754,
        -0.0021112666,
        -0.0026005763,
        -0.0070404573,
        -0.011808628,
        0.0026289471,
        0.026800143,
        -0.0045107035,
        0.007833524,
        0.0041114017,
        -0.018934952,
        -0.003292854,
        0.0022509645,
        -0.024462586,
        -0.0014778102,
        0.008366003,
        0.0042880224,
        -0.014283898,
        0.00509128,
        -0.009858519,
        -0.005836204,
        0.0152465105,
        -0.010843755,
        0.005291954,
        -0.008833221,
        -0.005382251,
        -0.030552205,
        -0.0073547456,
        0.003913525,
        -0.002875242,
        -0.0075636287,
        0.0049370667,
        0.0057220412,
        0.006802747,
        0.025118235,
        -0.009586192,
        0.008121331,
        0.0098025035,
        -0.14992744,
        0.0011948715,
        -0.0046519865,
        0.008078045,
        -0.012429164,
        0.002669048,
        0.011528319,
        0.0021092556,
        0.019413993,
        -0.0018000301,
        -0.01887662,
        0.0060650487,
        -0.0023920052,
        -0.013068622,
        0.01352505,
        0.009150154,
        -0.016019003,
        0.029956574,
        -0.0036511463,
        0.009843936,
        -0.0031337873,
        0.002376418,
        -0.004538653,
        0.015467604,
        -0.0060236966,
        0.015104127,
        -0.005371674,
        -0.005862085,
        0.0063016713,
        -0.006024692,
        0.0067914715,
        -0.011731089,
        -0.0034174412,
        -0.025502706,
        0.0046110004,
        -0.0018681033,
        -9.742518e-05,
        0.0073208264,
        0.012771767,
        -0.014607976,
        0.0051717707,
        -0.014098529,
        -0.00020231646,
        0.0006965938,
        0.0015893474,
        0.0044231983,
        0.013730039,
        0.012683434,
        -0.006765984,
        0.0059835697,
        0.01064143,
        -0.002347186,
        -0.0108879255,
        0.0041419524,
        0.0043611703,
        0.0072779483,
        0.0036337727,
        0.008081808,
        0.017358992,
        -0.00906825,
        0.003960662,
        -0.003736374,
        0.016347187,
        -0.015452487,
        -0.014352461,
        0.0006542963,
        -0.0047123823,
        0.025908437,
        0.011111924,
        -0.010236339,
        -0.0050352667,
        0.018441947,
        -0.007986807,
        0.0003113869,
        0.007860159,
        -0.024847656,
        0.0012184334,
        -0.0084606465,
        0.0063187946,
        -0.01481044,
        -0.0065261484,
        0.013559452,
        -0.0075162533,
        0.020410907,
        0.006366477,
        -0.012496969,
        -0.0044640647,
        -0.009855033,
        -0.0094596315,
        -0.053373173,
        -0.0048267087,
        -0.0027769236,
        -0.00050832256,
        0.014663368,
        0.016121743,
        -0.0044627544,
        -0.0076916334,
        -0.002592611,
        -0.0035193604,
        0.00082223164,
        0.018841136,
        0.008702178,
        -0.012852452,
        0.008353457,
        -0.00044592997,
        -0.009405888,
        0.0033226593,
        -0.010420167,
        -0.0072680577,
        -0.021861007,
        -0.0059895506,
        0.008448149,
        0.026288712,
        0.022068854,
        -0.020296218,
        0.0023167492,
        -0.009297088,
        0.00074476696,
        0.018562213,
        -0.018005047,
        0.0073224297,
        0.0049200524,
        -0.0017904756,
        -0.010765336,
        0.002128962,
        0.0060391566,
        0.023361389,
        -0.0074940133,
        -0.019341685,
        0.0053914925,
        0.014084786,
        0.00854987,
        0.003704116,
        0.0037539294,
        -0.0015467831,
        -0.02740097,
        -0.008398217,
        0.006888281,
        -0.009307137,
        -0.01422116,
        0.0070696753,
        -0.004520895,
        -0.011768769,
        0.003955276,
        0.023004463,
        -0.011676615,
        0.0012350633,
        0.015535851,
        -0.006182297,
        0.022383524,
        -0.0072880136,
        0.0049740383,
        -0.0006118007,
        0.010495618,
        -0.0051633795,
        0.006790474,
        0.0015511148,
        -0.015153624,
        0.010268936,
        0.0073811733,
        0.0009781937,
        0.01178416,
        -0.0011089374,
        -0.02107821,
        -0.0001564125,
        0.020317037,
        0.0066496274,
        0.0067242566,
        -0.015597361,
        -0.0045491583,
        0.0017079853,
        0.00094277743,
        0.016627384,
        0.009670779,
        -0.011171341,
        0.010954631,
        -0.0022659078,
        -0.014708458,
        0.01843715,
        0.007617961,
        -0.024753617,
        -0.0039235894,
        -0.0078902915,
        0.005745844,
        0.014380347,
        -0.0100562135,
        0.011866964,
        -0.0018162932,
        -0.008051157,
        0.022041596,
        0.01766327,
        0.008229253,
        -0.0023589474,
        -8.5670516e-05,
        -0.01334843,
        -0.0028626153,
        0.004518495,
        0.019997327,
        -0.0045658904,
        0.0034501764,
        0.0063031767,
        0.009320613,
        -0.014047962,
        -0.14648624,
        -0.006824073,
        -0.0016596966,
        -0.011411841,
        -0.015082734,
        -0.013335878,
        0.011551834,
        -0.010563982,
        0.006982176,
        0.00021983274,
        -0.012148634,
        0.008191701,
        -0.007821234,
        -0.032838333,
        0.02373918,
        -0.0010815274,
        0.0015897636,
        0.0074377107,
        -0.012479082,
        -2.8738057e-05,
        -0.0014833176,
        -0.012974388,
        -0.007973302,
        -0.0016758282,
        0.003148686,
        0.0025638358,
        0.023206575,
        -0.014269449,
        -0.014307423,
        -0.005754924,
        -0.0051003057,
        0.0072190645,
        -0.0038502892,
        0.011825166,
        -0.00910748,
        0.005015077,
        -0.014182448,
        0.0026624624,
        -0.0073124445,
        -0.007811429,
        -0.008524223,
        -0.0065209325,
        -0.01517046,
        -0.0033956042,
        -0.0018241138,
        -0.0010755508,
        -0.0028145376,
        -0.029124115,
        -0.011431108,
        -0.008952763,
        0.005596294,
        -0.007676772,
        -0.016617885,
        -0.004309976,
        7.1410705e-06,
        -0.00685617,
        0.0059458436,
        0.0065972665,
        -0.00623502,
        0.010776645,
        0.0058070733,
        -0.024633314,
        -0.0033471691,
        0.012810785,
        0.011727861,
        -0.0058485097,
        -0.008082717,
        0.18568861,
        -0.0037833068,
        0.031405855,
        0.0058803726,
        0.019156465,
        0.011062058,
        0.007262358,
        0.023455767,
        0.0045828987,
        -0.003917489,
        0.005831838,
        0.0046402244,
        -0.01515068,
        0.0030133575,
        0.0003972263,
        -0.006058487,
        -0.0031490333,
        -0.016547663,
        0.0025233866,
        -0.00021064868,
        0.012178125,
        0.004730312,
        0.025675826,
        0.0074501582,
        -0.00037532562,
        -0.0025681772,
        -0.022017429,
        -0.0033388655,
        -0.008359423,
        0.042633213,
        0.021065637,
        -0.0036932325,
        -0.01564345,
        0.0044151954,
        0.005430005,
        0.0022588146,
        0.015692776,
        -0.011276456,
        -0.018856872,
        0.0049786097,
        -0.011066668,
        0.011256917,
        0.005979955,
        -0.013485485,
        -0.00091142964,
        -0.013925857,
        -0.013831037,
        0.018961344,
        0.018166203,
        0.0062501547,
        -0.0023373307,
        -0.0031093734,
        0.0087390365,
        0.01374065,
        0.001636415,
        -0.0075751166,
        0.0173315,
        -0.01750892,
        0.0014772417,
        0.017004343,
        -0.0144999195,
        -0.00739983,
        -0.0066257915,
        0.02878988,
        0.0057194745,
        0.008635359,
        0.015976269,
        0.0038256433,
        -0.01283821,
        -0.1580003,
        0.025915647,
        -0.009930299,
        0.0032365585,
        -0.0069837826,
        -0.0011071875,
        0.020616967,
        0.024034526,
        -0.010344491,
        0.0042544743,
        -0.014355718,
        -0.023501696,
        -0.0024584287,
        0.01487159,
        0.017071873,
        0.005222706,
        0.010290435,
        0.0034755243,
        0.019284764,
        -0.00016062049,
        -0.016484646,
        -0.015094092,
        0.017877644,
        -0.020629816,
        -0.021116318,
        0.0045439424,
        -0.009144549,
        -0.018406717,
        0.017762253,
        0.008192928,
        -0.017476078,
        0.001562817,
        -0.004305497,
        0.0126657095,
        -0.014840151,
        0.011372157,
        -0.025775868,
        0.019723454,
        0.016748326,
        -0.025785085,
        0.013203365,
        -0.010608254,
        0.0079379855,
        0.014586085,
        -0.011305807,
        0.0075934757,
        -0.015627231,
        0.0035635184,
        -0.018511996,
        0.010906587,
        0.015116275,
        0.0028590485,
        -0.00443116,
        -0.01227754,
        -0.023583533,
        0.0048066615,
        0.01730808,
        -0.0216921,
        0.009354765,
        -0.00798531,
        0.020445408,
        0.018132895,
        0.012520549,
        -0.016801314,
        9.4779054e-05,
        -0.007474092,
        -0.005201382,
        0.0039640097,
        0.011631662,
        0.0057292446,
        -0.0073009,
        0.004058574,
        -8.686888e-05,
        -0.005437351,
        0.014309525,
        0.0155521175,
        -0.0027825597,
        0.015848111,
        0.012241578,
        0.017484935,
        0.004502121,
        -0.015509028,
        0.007988163,
        0.007959078,
        0.049246587,
        -0.0055177175,
        -0.015391873,
        -0.0003052154,
        0.015076743,
        -0.00033929464,
        0.0049585258,
        0.0023300897,
        0.00028332608,
        0.01120625,
        -0.021564042,
        -0.014732017,
        -0.012526575,
        0.005515836,
        0.0009166974,
        -0.022474587,
        -0.014740618,
        0.0040782862,
        0.025402917,
        -0.006254135,
        0.01009363,
        -0.0239528,
        -0.0025709493,
        -0.02412855,
        0.006550295,
        0.0016730384,
        -0.009603526,
        0.0033992422,
        0.0012092056,
        -0.006085367,
        0.008703499,
        -0.003689244,
        -0.0014076435,
        -0.009534574,
        -0.0054853354,
        -0.02066654,
        -0.010680351,
        0.013598772,
        0.0031068241,
        -0.0037121272,
        0.008072782,
        -0.003394622,
        0.0025109928,
        0.0027826724,
        0.016229061,
        0.025427112,
        -0.02198953,
        0.006238042,
        0.0015266989,
        0.007788444,
        0.010830141,
        -6.60342e-05,
        0.007627213,
        0.00016204389,
        0.01698931,
        -0.0030149345,
        -0.009655093,
        0.00019984157,
        -0.006867563,
        0.009870613,
        0.005105316,
        0.0012779349,
        0.0071327724,
        0.017466392,
        -0.0037296363,
        0.0033255361,
        0.0030833823,
        -0.00883321,
        0.0042368327,
        0.002736798,
        0.006582338,
        0.0035749418,
        0.0048294896,
        -0.0123593435,
        -0.0020034928,
        -0.017286006,
        -0.0050142664,
        0.010838562,
        -5.273929e-05,
        -0.000928733,
        0.017409956,
        -0.013367674,
        0.008117572,
        -0.0061205,
        -0.018993247,
        -0.009219725,
        0.0076219584,
        -0.008695164,
        0.021369938,
        0.0061461236,
        -0.020753767,
        -0.0048317555,
        -0.004392805,
        0.002545932,
        0.014583911,
        -0.0916195,
        -0.0039765052,
        -0.0026589138,
        0.0027497371,
        -0.0060457117,
        -0.013877818,
        -0.029172467,
        0.02089418,
        0.0028347054,
        -0.015397303,
        -0.00410825,
        -0.01662438,
        -0.017619092,
        -0.010262277,
        -0.0014270046,
        -0.012884295,
        0.012163371,
        0.017418971,
        -0.012878469,
        -0.02445126,
        0.010692117,
        0.0063889762,
        0.014484618,
        0.0040963483,
        -0.0072482466,
        0.00013649461,
        0.0072907396,
        -0.007882731,
        0.010025193,
        0.0044521717,
        -0.0016728152,
        -0.008928391,
        0.011873581,
        0.01642035,
        0.003983667,
        0.006183549,
        -0.006595571,
        -0.013323625,
        0.021635074,
        -0.04934709,
        -0.024186512,
        -0.016602408,
        -0.124441475,
        -0.011104407,
        -0.006539677,
        0.012304552,
        0.0042314716,
        -0.0043529486,
        0.006476256,
        0.015736673,
        -0.0061760806,
        0.004017224,
        -0.009259471,
        0.0017247092,
        -0.0016620724,
        -0.0004989829,
        -0.0025653262,
        0.0062084403,
        -0.015221538,
        -0.0033914673,
        -0.021644207,
        -0.014648646,
        -0.02133929,
        -0.0102555845,
        -0.009395289,
        0.0032432524,
        -0.012789208,
        0.014268083,
        0.0058866637,
        0.01634662,
        0.0059790243,
        0.007548118,
        -0.015289527,
        0.0034553194,
        0.009562817,
        -0.004334733,
        0.0014097105,
        0.0052294014,
        0.0035959422,
        0.0014639507,
        0.01168748,
        -0.003039298,
        6.840169e-05,
        0.024520623,
        0.0034481513,
        -0.019904332,
        0.008244853,
        -0.12143959,
        -0.02191296,
        0.014649597,
        -0.009611833,
        0.006271151,
        -0.00017700053,
        0.00087882427,
        0.114617154,
        -0.0011940574,
        0.007654556,
        0.008840175,
        -0.0094092665,
        -0.0028152463,
        0.0034973063,
        -0.006297694,
        0.02877233,
        -1.8610106e-05,
        0.02141105,
        0.017378982,
        0.007444074,
        0.0147992475,
        -0.0057501304,
        -0.017331824,
        -0.021406159,
        -0.0034970785,
        -0.06266787,
        0.005331466,
        -0.009293423,
        -0.009338751,
        0.0022409894,
        0.00015672352,
        -0.010486083,
        -0.01567856,
        -0.014644908,
        0.0015032913,
        -0.0007331043,
        0.010177664,
        0.018277008,
        -0.0026981349,
        -0.0039951405,
        0.011566305,
        -0.008083476,
        -0.0011381932,
        -0.011273989,
        -0.002804924,
        0.0045972485,
        -0.008335533,
        0.016989483,
        0.02289651,
        -0.0029778127,
        -0.010036067,
        -0.008870659,
        0.0013754714,
        0.0051520066,
        -0.0047030034,
        0.0036878784,
        0.00077708735,
        0.0028162675,
        -0.0023834838,
        0.0051732073,
        -0.028702738,
        -0.0072024264,
        0.015809914,
        0.011033836,
        0.0040182592,
        0.018200962,
        -0.002888368,
        -0.0020212634,
        -0.038930945,
        0.01927916,
        0.0062721567,
        0.00060184725,
        -0.005223804,
        -0.012872093,
        0.012680507,
        0.011309677,
        0.011129673,
        0.0018420434,
        -0.01382634,
        0.015565427,
        -0.018349381,
        0.01466565,
        -0.001353591,
        -0.014900259,
        0.009851331,
        0.0117140785,
        -0.00743465,
        0.025129415,
        0.016198084,
        0.0019343417,
        0.00924074,
        -0.009581441,
        -0.010399943,
        0.00978751,
        -0.0076831565,
        -0.0013594856,
        0.012791783,
        -0.0010281834,
        -0.0009442754,
        -0.0045031924,
        0.011287458,
        -0.011136374,
        -0.0013462842,
        -0.0079557,
        0.015005766,
        -0.000110964815,
        -0.0045358343,
        0.008143235,
        0.0037677065,
        0.0067877434,
        0.0016352028,
        -0.017597875,
        0.008422427,
        -0.0023164933,
        0.0071869255,
        -0.019100312,
        0.009985823,
        -0.0020570366,
        -0.026934348,
        0.003076819,
        -0.011163135,
        -0.012213635,
        0.013222991,
        -0.0101319775,
        0.010044762,
        0.010960541,
        -0.0257306,
        0.0011238749,
        0.004471614,
        0.015289434,
        -0.024679223,
        0.021054082,
        0.0008031577,
        -0.0109911505,
        0.0023224847,
        -0.005666247,
        -0.027743232,
        0.011889743,
        0.0063092886,
        -0.00726674,
        -0.007048237,
        0.00060997764,
        -0.017122535,
        -0.012625799,
        0.01242399,
        -0.0046939575,
        -0.0104039805,
        0.0018829515,
        0.0028653666,
        0.0050186487,
        0.009846843,
        0.009597321,
        -0.003612255,
        0.0037283038,
        -0.009318037,
        -0.0056000073,
        0.006410897,
        -0.013107866,
        0.009468542,
        -0.0021661893,
        0.00149705,
        -0.004347935,
        0.008857427,
        -0.0076005682,
        -0.005476933,
        0.017501764,
        0.017327258,
        0.011655478,
        0.0230992,
        -0.015241063,
        0.01810898,
        -0.0043899566,
        -0.0046896758,
        0.005219443,
        0.010208316,
        2.2112632e-05,
        0.010181631,
        0.0059708776,
        0.013570875,
        0.0029528672,
        -0.0024266161,
        -0.0044027534,
        -0.013490575,
        0.008586271,
        -0.008849467,
        -0.019049998,
        -0.010084215,
        -0.013036467,
        0.007980119,
        0.020838317,
        0.025925392,
        -0.028966328,
        0.016440079,
        0.012633723,
        0.00041015213,
        0.010853542,
        0.012103462,
        0.010435233,
        0.0028800012,
        0.029371407,
        -0.008662513,
        -0.015126937,
        0.0028284544,
        0.015156252,
        -0.010553694,
        0.008666641,
        -0.017788855,
        0.00019339174,
        0.01673696,
        0.02057671,
        0.021990398,
        -0.011024715,
        0.00390719,
        0.0077981194,
        -0.015470191,
        -0.009945877,
        0.014853266,
        -0.000117211326,
        0.0068058386,
        0.008463524,
        0.0081442315,
        0.01274591,
        0.009515698,
        -0.009244884,
        0.006183057,
        -0.0004880961,
        -0.014025054,
        -0.0018170429,
        0.009417833,
        0.008247421,
        0.0011639093,
        0.008118469,
        -0.012913112,
        -0.0006726224,
        0.003827492,
        -0.0004501262,
        -0.0151021965,
        0.004657997,
        0.014349494,
        -0.0012812655,
        -0.009239153,
        0.00015070793,
        -0.0035881158,
        0.012558791,
        0.0035480733,
        -0.008528752,
        -0.0075272843,
        -0.009152359,
        -0.0056430916,
        0.025702138,
        0.006939131,
        0.004515532,
        0.00010698207,
        0.01502567,
        0.0014772497,
        -0.00370293,
        -0.005067512,
        0.005285346,
        -0.0038558468,
        -0.010127444,
        0.005417485,
        0.011311958,
        -0.020462038,
        0.03143681,
        -0.017472161,
        -0.018055031,
        0.013967053,
        0.0038547975,
        -0.0026967924,
        0.0065782294,
        0.012526953,
        -0.02100617,
        -0.021077445,
        -0.0057640877,
        0.002596218,
        -0.007626997,
        -0.020916902,
        -0.0098007955,
        -7.83309e-05,
        0.012013214,
        -0.0024430773,
        -0.012951724,
        -0.00595156,
        -0.0042154063,
        0.018080873,
        -0.0030140071,
        -0.016490845,
        0.02185448,
        -0.012352665,
        -0.0029778637,
        0.010505468,
        -0.005698139,
        -0.015210042,
        0.0054160566,
        0.0014385834,
        0.017358497,
        0.0024925193,
        0.0018530615,
        0.015873127,
        -0.0022302072,
        -0.0005028827,
        0.0021279568,
        0.0038967044,
        0.016841127,
        -0.014042811,
        -0.0030516684,
        0.0050372616,
        -0.031957567,
        -0.019308222,
        -0.0025386028,
        -0.002679519,
        -0.0056086555,
        -0.008683474,
        0.015971964,
        0.007353149,
        -0.027007397,
        0.008924696,
        -0.0008340547,
        0.019880608,
        -0.0055025206,
        -0.01590024,
        -0.0036399763,
        0.018635087,
        -0.013036403,
        0.008815702,
        0.011861558,
        -0.01468177,
        0.00121297,
        -0.009398982,
        0.010337015,
        0.0110228965,
        -0.004992435,
        -0.005738334,
        -0.015813546,
        0.013014979,
        0.013768794,
        -0.0048942706,
        0.012043901,
        -0.0010712534,
        0.0010319346,
        -0.01611398,
        -0.016709376,
        0.019312043,
        -0.020934407,
        0.00062033866,
        -0.008133676,
        0.008957683,
        0.0023839083,
        -0.010094943,
        -0.00811617,
        -0.0029940363,
        0.001117675,
        -0.010440734,
        0.0043290155,
        -0.017887944,
        0.010158061,
        0.0006746722,
        0.00030605687,
        0.016504183,
        0.0047078663,
        -0.001644117,
        0.011201007,
        0.015554936,
        0.010223629,
        0.00036477478,
        0.0011143686,
        0.00026682302,
        -0.02338571,
        0.007436491,
        -0.0072130626,
        -0.010764247,
        0.0059921034,
        7.1646195e-05,
        0.0039738715,
        -0.015295445,
        0.016727777,
        0.0008555697,
        -0.00761532,
        0.009012112,
        -0.0043480154,
        -0.0100516295,
        0.028565217,
        -0.004737194,
        0.005854979,
        -0.013342218,
        -2.7877297e-06,
        0.014637703,
        0.010970798,
        -0.00556974,
        -0.013536306,
        0.0022598417,
        0.011963701,
        0.0044474546,
        -0.019014085,
        -0.0045170477,
        0.0041359123,
        0.008223246,
        -0.009368905,
        -0.010186132,
        -0.018779028,
        0.0068433704,
        -0.024169505,
        -0.0013979742,
        -0.000950571,
        0.011932858,
        0.010654073,
        0.0033393197,
        -0.008133018,
        0.013895245,
        -0.006771279,
        0.0059592198,
        0.0046007363,
        -0.009901792,
        0.013518554,
        -0.010640123,
        0.030683612,
        -0.004593259,
        0.0064020967,
        -0.00032915978,
        -0.01396591,
        -0.009155577,
        0.011233285,
        0.0034603106,
        0.00563457,
        0.00887876,
        -0.01009322,
        0.014306754,
        0.012489329,
        0.020376142,
        0.0033521242,
        0.027736077,
        -0.006620418,
        -0.011721168,
        -0.0075882487,
        -0.00054419966,
        0.0048041223,
        0.010346022,
        0.007299806,
        -0.014125196,
        -0.008890534,
        -0.031629153,
        -0.0025711812,
        -0.009225888,
        0.009615475,
        0.0009996102,
        -0.00092516735,
        -0.0073280293,
        -0.0022291527,
        0.0088368915,
        0.005296829,
        -0.02370823,
        -0.033806533,
        -0.014753461,
        -0.014627037,
        -0.033608373,
        -0.0076055396,
        -0.007276065,
        0.0067738704,
        0.01653296,
        -0.020108256,
        0.00415349,
        -0.005306598,
        0.007747371,
        0.012712696,
        -0.0032491782,
        0.009729047,
        0.004196501,
        0.0020489455,
        0.013599524,
        0.0034331854,
        -0.032916598,
        -0.0057196734,
        -0.05371472,
        0.0073951883,
        -0.015388794,
        -0.009371987,
        -0.014560442,
        -0.0116404835,
        -0.015051198,
        -0.02936442,
        0.017729996,
        0.0017165431,
        0.0030272766,
        0.005313956,
        -0.004989299,
        0.0046037124,
        -0.01081718,
        -0.020768141,
        -0.0041953414,
        -0.014083716,
        0.003337101,
        0.010465462,
        0.0076561696,
        -0.01294184,
        0.0017708628,
        0.0074526262,
        -0.009927837,
        -0.008924496,
        0.019676551,
        0.011515565,
        0.011263191,
        -0.008189904,
        -0.0018928152,
        -0.019628735,
        0.0065091765,
        0.012162742,
        0.0027431848,
        -0.0077134855,
        -0.0061783134,
        -0.024497608,
        -0.024195593,
        0.013701517,
        0.008105571,
        0.0075714593,
        0.012207213,
        -0.0029695663,
        0.01627858,
        -0.010049757,
        -0.008298437,
        -0.0048773503,
        -0.02464528,
        0.0035822587,
        -0.012197318,
        0.014313859,
        0.016389947,
        -0.01128234,
        0.0021636123,
        0.017820656,
        0.0030172274,
        -0.0015584748,
        0.02670911,
        -0.014194415,
        -0.013723627,
        0.013710168,
        -0.004039517,
        0.0012402287,
        0.000561743,
        -0.025302852,
        0.0026709249,
        -0.017945996,
        -0.0018008811,
        -0.006347606,
        0.014038851,
        -0.015283283,
        0.032182362,
        -0.0025191242,
        -0.011755994,
        -0.007523007,
        -0.017186662,
        -0.014898556,
        -0.00722732,
        0.0025408738,
        -0.006213105,
        -0.00023598055,
        -0.0033700983,
        0.010423045,
        0.0071767936,
        0.018666547,
        0.019532768,
        -0.014758007,
        -0.003334614,
        -0.005039353,
        -0.007967098,
        0.0028862418,
        -0.021203822,
        -0.010797992,
        -0.0021622998,
        -0.006015637,
        0.0036511025,
        0.014989732,
        -0.015609774,
        0.013743676,
        0.014679715,
        0.009965072,
        0.011522203,
        0.0015538016,
        0.00489717,
        0.012983299,
        -0.005212814,
        -0.014766995,
        -0.010789752,
        0.0032154983,
        0.010751431,
        0.013039059,
        0.017811615,
        -0.019156426,
        -0.014025598,
        0.012985389,
        -0.00027232498,
        0.015444761,
        0.010331533,
        -0.011279931,
        -0.008512632,
        0.005181569,
        -0.0039224303,
        -0.010557567,
        -0.017517334,
        -0.00620871,
        0.015093537,
        -0.0017424285,
        0.0010208939,
        0.00079204695,
        -0.0026273255,
        0.009973198,
        0.006490109,
        0.009968716,
        0.0011381607,
        0.01959072,
        0.0051467875,
        -0.0024302576,
        -0.02331086,
        0.02755953,
        -0.005980661,
        0.004818372,
        -0.0002631601,
        0.0020474286,
        -0.017047165,
        -0.0043906635,
        0.011114555,
        -0.012126833,
        -0.014264713,
        -0.0026860936,
        0.0019311702,
        -0.0036805777,
        0.013153112,
        -0.020166775,
        0.025770789,
        0.01950089,
        -0.010063315,
        -0.0047104238,
        0.0006746316,
        -0.0049613924,
        -0.010810624,
        0.016753815,
        -0.013445554,
        0.016612468,
        0.0016970533,
        0.023309922,
        0.003590286,
        0.010605335,
        0.01817799,
        0.006384871,
        0.0032461695,
        -0.00087257073,
        -0.006790621,
        0.010042906,
        0.00841939,
        0.013978074,
        -0.0054378393,
        0.013068841,
        0.0063626994,
        0.005147911,
        -0.011171199,
        -0.014286936,
        0.00917229,
        0.0013667835,
        0.0012117933,
        0.011147516,
        0.00681716,
        -0.0030325984,
        -0.0137928855,
        0.0006294049,
        -0.02786776,
        0.0058849202,
        0.006283374,
        0.00091846986,
        0.0040782425,
        0.014922349,
        -0.0019371698,
        -0.009025283,
        0.0028353164,
        0.012115809,
        -0.003750639,
        0.0011520251,
        0.0024091564,
        0.0063483026,
        -0.0051304093,
        0.0018131937,
        0.0037019101,
        -0.017387263,
        -0.029721772,
        0.005642841,
        0.009279778,
        -0.0073055457,
        0.0147376945,
        0.030366777,
        0.2010059,
        0.13352787,
        -0.014008169,
        0.015652975,
        0.0054901578,
        0.013961525,
        -0.0141178295,
        -0.0034510568,
        0.01008886,
        0.014918495,
        -0.003853932,
        -0.0015629199,
        -0.0029597543,
        -0.001987736,
        -0.0063958406,
        -0.016775418,
        -0.00043222477,
        -0.006359775,
        -0.0069299825,
        0.0002673354,
        0.013605118,
        0.017145848,
        0.0010076067,
        0.0026602321,
        -0.017253863,
        0.005693179,
        -0.013559205,
        -0.0286654,
        0.010745431,
        -0.00057814695,
        -0.025915157,
        0.002492199,
        0.009236817,
        -0.004859301,
        -0.012052171,
        0.0010926072,
        0.01998926,
        0.0051381905,
        -0.0052766954,
        -0.013996707,
        0.0016413804,
        -0.01409733,
        -0.019760977,
        -0.007584411,
        -0.0019550666,
        0.0008633331,
        -0.00818619,
        -0.0036378233,
        -0.020464804,
        -0.021995079,
        -0.0012209944,
        -0.013407473,
        -0.010768164,
        -0.003733543,
        0.0010313222,
        0.00018610389,
        -0.003056455,
        0.002297645,
        -0.010894392,
        0.0073029893,
        0.0012239392,
        0.0037775382,
        -0.025674647,
        -0.0028810303,
        0.0024238827,
        -0.008106442,
        -0.00011667741,
        -0.0067331316,
        -0.005295494,
        0.014629786,
        0.0039820336,
        0.033170167,
        0.004897789,
        -0.0054253126,
        0.010980613,
        0.0020236822,
        -0.0012850411,
        -0.0007028319,
        0.0023133669,
        0.0075345747,
        -0.019692713,
        -0.012268094,
        -0.004110287,
        0.018343601,
        -0.003985101,
        0.017828012,
        0.015871102,
        0.021738261,
        0.11893127,
        0.0019439096,
        -0.0012855564,
        -0.016268179,
        0.01955498,
        -0.0027278184,
        0.0035799325,
        0.039202493,
        0.012201291,
        0.01735868,
        -0.0021645129,
        0.0101949265,
        0.007922806,
        -0.01614391,
        0.00024402095,
        0.004406655,
        0.0062491116,
        0.051693104,
        -0.0003653363,
        -0.0026543178,
        -0.0255546,
        -0.0037092648,
        -0.00808358,
        0.0014458848,
        -0.005338212,
        0.006824182,
        -0.004715628,
        0.00514055,
        -0.0075991787,
        -0.004520641,
        -0.120110616,
        0.0031089908,
        0.004434372,
        -0.006278761,
        -0.012694825,
        0.011402337,
        -0.013207823,
        -0.03548214,
        -0.0075549423,
        0.006370858,
        -0.009396872,
        -0.01337702,
        0.005407338,
        0.018366523,
        0.013395984,
        -0.0022473016,
        0.0031267945,
        -0.0026388334,
        0.018284941,
        0.013520069,
        0.01502719,
        0.01436912,
        0.01094694,
        -8.690339e-05,
        0.011365958,
        0.005041319,
        0.0078383265,
        -0.0054744096,
        0.012732002,
        -0.008455726,
        -0.01167687,
        0.01655395,
        -0.014068539,
        0.010075537,
        0.0053761182,
        0.011580794,
        0.00040715307,
        0.0025097842,
        0.006358901,
        0.0062615583,
        0.011228248,
        -0.014947631,
        0.008819513,
        -0.013842312,
        -0.0057010725,
        -0.016869295,
        0.008906283,
        -0.004590115,
        0.0017565441,
        -0.028759165,
        0.057926845,
        -0.0014655589,
        -0.019851854,
        -0.00815287,
        -0.024201708,
        -0.011789288,
        -0.0034255306,
        0.0057501276,
        0.004364179,
        0.015550476,
        -0.012242187,
        0.024507513,
        0.0028407306,
        0.015344123,
        0.027696904,
        -0.0016738396,
        0.010777092,
        -0.0004903479,
        -0.008422215,
        0.018224815,
        0.015679376,
        0.012767944,
        -0.002046503,
        -0.025933158,
        -0.009458827,
        0.0029733405,
        -0.018893177,
        0.0051348642,
        0.009222699,
        0.003475031,
        0.003360118,
        -0.026880445,
        0.0010829068,
        0.13610461,
        0.0058715595,
        -0.0025425178,
        -0.018234858,
        -0.012815697,
        0.018507734,
        0.021038106,
        -0.008949593,
        0.0020627996,
        -0.0025946223,
        -0.006293042,
        0.0037332755,
        -0.008372949,
        -0.013802702,
        0.0032499123,
        -0.011038402,
        0.002992377,
        -0.004237861,
        0.014333801,
        -0.026500342,
        0.005192639,
        -0.009292512,
        -0.0047962684,
        -0.007851659,
        0.003129566,
        -8.4042375e-05,
        -0.012044316,
        0.00946566,
        0.0008772719,
        -0.0005716222,
        0.002185132,
        -0.008249169,
        0.00876186,
        -0.01678339,
        0.009760621,
        -0.023480972,
        0.00074121816,
        0.0018387339,
        -0.0009090987,
        0.0041693426,
        0.012757139,
        0.008562212,
        0.0001036523,
        0.02520426,
        -0.0313008,
        0.21457486,
        0.017133355,
        0.008097823,
        -0.0059026373,
        0.003070178,
        0.020574132,
        0.002429069,
        -0.004391834,
        0.014114817,
        0.011076759,
        -0.0068275356,
        0.010790023,
        -0.0070255343,
        -0.004673448,
        0.0027980602,
        0.013024338,
        0.0117459465,
        0.0074462118,
        0.014539115,
        -0.013652739,
        -0.0002490101,
        0.014086894,
        0.011271723,
        -0.008274468,
        0.001397697,
        0.00058636157,
        0.012636688,
        0.010852112,
        0.0043171044,
        0.015500225,
        0.0074967514,
        -0.0063684355,
        0.006948282,
        -0.014690671,
        0.013036861,
        -0.0019322739,
        0.012686878,
        0.0068997745,
        -0.010709624,
        -0.009372421,
        -0.012525086,
        0.025672568,
        -0.006945638,
        0.0052356226,
        -0.0029646838,
        0.02007887,
        -0.014867461,
        2.674572e-05,
        -0.014622129,
        0.0065204618,
        0.008233188,
        -0.0010436594,
        0.0088824425,
        -0.010678632,
        -0.000929463,
        -0.007720481,
        -0.0041843597,
        -0.01598309,
        -0.01350181,
        -0.018582666,
        -0.02293405,
        0.017030697,
        -0.024387695,
        -0.0033717565,
        -0.024177188,
        0.0038573944,
        -0.00642227
      ],
      "embedding_pca_2d": [],
      "embedding_pca_3d": [],
      "embedding_cluster_id": null,
      "migration_history": [],
      "metadata": {
        "compute_time": 145.33757877349854,
        "patch_type": "full",
        "api_costs": 0.2159933,
        "num_applied": 1,
        "patch_name": "mpc_inspired_phase_switching",
        "patch_description": "This approach uses a simplified Model Predictive Control (MPC) strategy that:\n1. Performs multi-step lookahead prediction using the actual nonlinear dynamics\n2. Evaluates candidate control actions over a short horizon\n3. Uses phase detection to switch between aggressive recovery and precision balancing modes\n4. Implements a novel \"momentum nullification\" strategy that explicitly targets zero angular momentum\n5. Uses trajectory-based cost evaluation rather than instantaneous state feedback\n\nThe key insight is that instead of relying purely on LQR gains (which assume linear dynamics), we can directly simulate forward trajectories for a few candidate control forces and pick the one that minimizes a multi-step cost. This naturally handles the nonlinear dynamics better, especially during large angle recovery.",
        "error_attempt": null,
        "novelty_attempt": 1,
        "resample_attempt": 3,
        "patch_attempt": 1,
        "model_name": "claude-opus-4-5",
        "temperature": 0.5,
        "max_tokens": 4096,
        "llm_result": {
          "content": "Looking at the performance metrics, the best performer achieved a score of 4967.97 with a stabilization time of 185 steps. The key differentiators seem to be:\n\n1. Phase-aware control with momentum-cancellation\n2. Predictive momentum compensation\n3. Sliding surface inspired damping in transition zones\n4. More sophisticated integral control\n\nLet me design a fundamentally different approach using **Model Predictive Control (MPC) principles** with a receding horizon optimization, combined with phase-based switching.\n\n<NAME>\nmpc_inspired_phase_switching\n</NAME>\n\n<DESCRIPTION>\nThis approach uses a simplified Model Predictive Control (MPC) strategy that:\n1. Performs multi-step lookahead prediction using the actual nonlinear dynamics\n2. Evaluates candidate control actions over a short horizon\n3. Uses phase detection to switch between aggressive recovery and precision balancing modes\n4. Implements a novel \"momentum nullification\" strategy that explicitly targets zero angular momentum\n5. Uses trajectory-based cost evaluation rather than instantaneous state feedback\n\nThe key insight is that instead of relying purely on LQR gains (which assume linear dynamics), we can directly simulate forward trajectories for a few candidate control forces and pick the one that minimizes a multi-step cost. This naturally handles the nonlinear dynamics better, especially during large angle recovery.\n</DESCRIPTION>\n\n<CODE>\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0\nM_POLE = 0.35\nL_POLE = 2.5\nL_COM = L_POLE / 2\nG = 9.81\nFRICTION_CART = 0.35\nFRICTION_JOINT = 0.25\nDT = 0.02\nMAX_STEPS = 1000\n\ndef simulate_pendulum_step(state, force, dt):\n    x, theta, dx, dtheta = state\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n    M_total = M_CART + M_POLE\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    MPC-Inspired Phase-Switching Controller with Momentum Nullification\n    Uses short-horizon trajectory evaluation combined with LQR for precision control.\n    \"\"\"\n\n    def __init__(self):\n        # Precompute LQR gains for precision phase\n        m, M, l, g = M_POLE, M_CART, L_COM, G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c, b_j = FRICTION_CART, FRICTION_JOINT\n\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n        self.K = self._solve_lqr(A, B, Q, R)\n\n        self.integral_x = 0.0\n        self.K_i = 0.8\n        self.omega_n = np.sqrt(G / L_COM)\n        \n        # MPC parameters\n        self.horizon = 3\n        self.n_candidates = 7\n\n    def _solve_lqr(self, A, B, Q, R):\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        return np.linalg.inv(R) @ B.T @ P\n\n    def _predict_trajectory(self, state, force, steps):\n        \"\"\"Simulate forward trajectory\"\"\"\n        s = state.copy()\n        cost = 0.0\n        for _ in range(steps):\n            s = simulate_pendulum_step(s, force, DT)\n            # Weighted cost emphasizing angle stability\n            cost += 44.0 * s[1]**2 + 4.5 * s[0]**2 + 0.6 * s[2]**2 + 3.2 * s[3]**2\n        return cost, s\n\n    def _mpc_search(self, state, base_force):\n        \"\"\"Search for best control action using trajectory evaluation\"\"\"\n        best_force = base_force\n        best_cost = float('inf')\n        \n        # Generate candidate forces around base_force\n        spread = min(30.0, 10.0 + 20.0 * abs(state[1]))\n        candidates = np.linspace(base_force - spread, base_force + spread, self.n_candidates)\n        candidates = np.clip(candidates, -100.0, 100.0)\n        \n        for f in candidates:\n            cost, final_state = self._predict_trajectory(state, f, self.horizon)\n            # Add control effort penalty\n            cost += 0.5 * f**2 * self.horizon\n            # Penalty for predicted failure\n            if abs(final_state[1]) > 0.95 or abs(final_state[0]) > 9.0:\n                cost += 10000.0\n            if cost < best_cost:\n                best_cost = cost\n                best_force = f\n        \n        return best_force\n\n    def get_action(self, state):\n        x, theta, dx, dtheta = state\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n        \n        state_vec = np.array([x, theta, dx, dtheta])\n        \n        # Compute base LQR force\n        base_force = float((-self.K @ state_vec)[0])\n        \n        # Phase detection\n        abs_theta = abs(theta)\n        abs_dtheta = abs(dtheta)\n        \n        # Recovery phase: large angles\n        if abs_theta > 0.5:\n            # Adaptive gain scheduling\n            pos_gain = 1.0 + 0.6 * np.tanh(5.0 * (abs_theta - 0.5))\n            vel_gain = 1.0 + 0.35 * np.tanh(4.0 * (abs_dtheta - 0.8))\n            force = base_force * pos_gain * vel_gain\n            \n            # MPC refinement for critical situations\n            if abs_theta > 0.7:\n                force = self._mpc_search(state_vec, force)\n            \n            # Momentum-aware swing assist\n            if abs_theta > 0.8:\n                swing_act = np.tanh(6.0 * (abs_theta - 0.8))\n                # Normalized momentum term\n                norm_mom = (theta * dtheta) / (self.omega_n * L_COM)\n                falling_factor = 1.0 + np.tanh(2.5 * norm_mom)\n                force += 8.0 * swing_act * np.sign(theta) * falling_factor\n        \n        # Transition phase: moderate angles\n        elif abs_theta > 0.15:\n            # Smooth gain scheduling\n            pos_gain = 1.0 + 0.4 * np.tanh(6.0 * (abs_theta - 0.15))\n            vel_gain = 1.0 + 0.25 * np.tanh(5.0 * (abs_dtheta - 0.5))\n            force = base_force * pos_gain * vel_gain\n            \n            # Sliding mode inspired damping\n            lambda_s = 2.0 * self.omega_n\n            s = dtheta + lambda_s * theta\n            force += -0.6 * np.tanh(2.0 * s)\n        \n        # Precision phase: small angles\n        else:\n            force = base_force\n            \n            # Dual-gated integral control\n            angle_gate = np.tanh(15.0 * (0.12 - abs_theta))\n            vel_gate = np.tanh(10.0 * (1.0 - abs_dtheta))\n            gate = angle_gate * vel_gate\n            \n            if gate > 0.1:\n                self.integral_x += x * DT\n            else:\n                self.integral_x *= 0.92\n            \n            force += self.K_i * gate * self.integral_x\n            \n            # Precision cart damping\n            cart_damp = -0.12 * dx * (0.15 - abs_theta) / 0.15\n            force += cart_damp\n            \n            # Angular momentum nullification\n            momentum_correction = -0.08 * dtheta * np.sign(theta + 1e-8)\n            force += momentum_correction\n\n        return float(force)\n\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n    states = [state]\n    forces = []\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        force = np.clip(force, -100.0, 100.0)\n        next_state = simulate_pendulum_step(state, force, DT)\n        states.append(next_state)\n        forces.append(force)\n        state = next_state\n    return np.array(states), np.array(forces)\n```\n</CODE>",
          "msg": "Here are the performance metrics of a set of prioviously implemented programs:\n\n# Prior programs\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Phase-Aware Momentum-Cancellation LQR Controller\n    Uses sliding-mode inspired phase blending with momentum-aware integral action.\n    \"\"\"\n\n    def __init__(self):\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Linearized dynamics matrices\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Physical constants for control\n        self.omega_n = np.sqrt(G / L_COM)\n        self.E_ref = M_POLE * G * L_COM\n        self.I_pole = M_POLE * L_COM**2 / 3.0  # Moment of inertia\n\n        # Integral states\n        self.integral_x = 0.0\n        self.integral_momentum = 0.0  # Novel: track angular momentum deficit\n        \n        # Tuned parameters\n        self.K_i = 0.8\n        self.K_mom = 0.015  # Momentum integral gain\n\n    def solve_lqr(self, A, B, Q, R):\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def compute_stability_index(self, theta, dtheta):\n        \"\"\"Compute unified stability metric for phase determination\"\"\"\n        E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n        E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n        E_current = E_kinetic + E_potential\n        energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n        \n        # Stability index: higher = more unstable\n        sigma = abs(theta) + 0.3 * energy_deficit\n        return sigma, energy_deficit\n\n    def get_action(self, state):\n        x, theta, dx, dtheta = state\n        \n        # Robust angle normalization\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n        \n        # Compute stability index for phase determination\n        sigma, energy_deficit = self.compute_stability_index(theta, dtheta)\n        \n        # Phase weights via smooth sigmoid transitions\n        recovery_weight = 1.0 / (1.0 + np.exp(-12.0 * (sigma - 0.7)))\n        stabilize_weight = 1.0 - recovery_weight\n        \n        # === BASE LQR CONTROL ===\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n        \n        # Gentle gain scheduling (proven optimal slopes)\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n        \n        force = base_force * adaptive_gain\n        \n        # === RECOVERY PHASE: Energy injection with momentum feedforward ===\n        if recovery_weight > 0.05:\n            swing_activation = np.tanh(6.0 * max(0.0, abs(theta) - 0.8))\n            \n            # Normalized momentum-based falling severity\n            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)\n            falling_factor = 1.0 + np.tanh(2.5 * norm_momentum)\n            \n            # Energy-scaled assist\n            energy_scale = 1.0 + 0.4 * energy_deficit\n            \n            # Predictive feedforward: estimate where pole will be\n            sin_t, cos_t = np.sin(theta), np.cos(theta)\n            Mtot = M_CART + M_POLE\n            denom0 = L_COM * (4.0/3.0 - M_POLE * cos_t**2 / Mtot)\n            temp_pred = (M_POLE * L_COM * dtheta**2 * sin_t) / Mtot\n            theta_acc_pred = (G * sin_t - cos_t * temp_pred) / denom0\n            \n            # One-step prediction\n            theta_pred = theta + dtheta * DT + 0.5 * theta_acc_pred * DT**2\n            predictive_boost = 1.0 + 0.15 * np.tanh(3.0 * (abs(theta_pred) - abs(theta)))\n            \n            u_recovery = 8.0 * swing_activation * np.sign(theta) * falling_factor * energy_scale * predictive_boost\n            force = force + recovery_weight * u_recovery\n        \n        # === STABILIZATION PHASE: Precision control with dual-gated integral ===\n        # Dual gating: angle AND velocity (proven superior)\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n        \n        if integral_gate > 0.1:\n            # Standard position integral\n            self.integral_x += x * DT\n            \n            # Novel: Angular momentum deficit integral\n            # Target momentum is zero; track cumulative deficit\n            momentum_error = -dtheta  # Want dtheta -> 0\n            self.integral_momentum += momentum_error * DT * stabilize_weight\n            self.integral_momentum = np.clip(self.integral_momentum, -0.5, 0.5)\n        else:\n            # Leaky decay when gates closed\n            self.integral_x *= 0.95\n            self.integral_momentum *= 0.9\n        \n        # Combined integral force\n        integral_force = self.K_i * integral_gate * self.integral_x\n        momentum_force = self.K_mom * integral_gate * self.integral_momentum * np.sign(theta + 1e-8)\n        \n        force = force + integral_force + momentum_force\n        \n        # === TRANSITION SMOOTHING: Sliding surface inspired damping ===\n        # In transition zone, add extra damping proportional to sliding variable\n        if 0.15 < abs(theta) < 0.6:\n            # Sliding surface: s = dtheta + lambda * theta\n            lambda_slide = 2.0 * self.omega_n\n            s = dtheta + lambda_slide * theta\n            \n            # Smooth reaching law\n            transition_damp = -0.5 * np.tanh(2.0 * s)\n            force = force + transition_damp\n        \n        # Near-equilibrium cart damping\n        if abs(theta) < 0.12:\n            cart_damp = -0.1 * dx * (0.12 - abs(theta)) / 0.12\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4967.97\nscore: 4967.97; base_score: 393.85; time_bonus: 2281.45; energy_bonus: 2492.67; success_bonus: 800.00; stabilization_time: 185; stabilization_ratio: 0.18; avg_energy_per_step: 0.01; total_energy: 6.55; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.01\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive Integral LQR combining best elements:\n    - Velocity-gated integral action from top performer (prevents windup)\n    - Gentler gain scheduling from crossover inspiration (smoother control)\n    - Proven Q[3]=3.2 and K_i=0.8 parameters\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.8\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with velocity-gated integral action\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi]\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Gentler gain scheduling (from crossover inspiration - proven more effective)\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        adaptive_gain = pos_gain * vel_gain\n        force = base_force * adaptive_gain\n\n        # Swing-up assist for large angles\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Velocity-gated integral action (from top performer - prevents windup during fast transients)\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4954.47\nscore: 4954.47; base_score: 393.80; time_bonus: 2267.91; energy_bonus: 2492.75; success_bonus: 800.00; stabilization_time: 187; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.51; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.04\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Adaptive LQR Controller with Velocity-Aware Gain Scheduling\n    Combines physically accurate friction modeling with intelligent gain adaptation\n    for ultra-fast stabilization while maintaining energy efficiency.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Optimized LQR weights: proven configuration from best performer (4920.14)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])  # Restored best-performing weights\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR control with optimized gain scheduling\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi] with numerical safety\n        theta = ((theta + np.pi + 1e-8) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Optimized gain scheduling: more aggressive for large errors\n        # Position-based gain: sharp response beyond 0.6 rad\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n\n        # Velocity-based gain: strong damping for high angular velocity\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain (simpler and more effective)\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Additive swing-up assist for large angles (>0.8 rad)\n        # Uses continuous falling-severity factor for smoother, more effective correction\n        if abs(theta) > 0.8:\n            # Physics-informed assist: push cart in direction to catch falling pole\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            # Continuous falling severity: scales from 1.0 to 2.0 based on how rapidly pole is falling\n            falling_severity = 1.0 + np.tanh(3.0 * theta * dtheta)\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity\n            force = force + u_swing\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4920.22\nscore: 4920.22; base_score: 393.53; time_bonus: 2233.79; energy_bonus: 2492.90; success_bonus: 800.00; stabilization_time: 192; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.44; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.03\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Advanced Adaptive LQR with Dual-Gated Integral Control and Energy-Based Swing-Up Assist\n    Combines precision LQR control with intelligent nonlinear enhancements for superior\n    performance on extreme inverted pendulum parameters.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Reference energy for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM\n\n        # Prediction horizon for proactive control\n        self.prediction_dt = DT\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Advanced adaptive LQR with enhanced integral control and energy-based assist\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Proven optimal gain scheduling\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Enhanced energy-based swing-up assist\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Calculate current mechanical energy\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n\n            # Normalized falling severity\n            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale\n            force += u_swing\n\n        # Predictive momentum compensation during mid-swing transition\n        if 0.3 < abs(theta) < 0.8:\n            # Estimate near-future angular dynamics\n            sin_theta = np.sin(theta)\n            cos_theta = np.cos(theta)\n            Mtot = M_CART + M_POLE\n            denom0 = L_COM * (4.0/3.0 - M_POLE / Mtot)\n\n            # Simplified acceleration estimate (assuming zero input)\n            temp = (M_POLE * L_COM * dtheta**2 * sin_theta) / Mtot\n            theta_acc_pred = (G * sin_theta - cos_theta * temp) / denom0\n\n            # Predict future angle\n            theta_future = theta + dtheta * self.prediction_dt + 0.5 * theta_acc_pred * self.prediction_dt**2\n\n            # Compute predictive momentum term\n            predicted_momentum = theta_future * dtheta\n            norm_predicted_momentum = predicted_momentum / (self.omega_n * L_COM)\n\n            # Proactive correction to counteract predicted instability\n            predictive_correction = -0.15 * norm_predicted_momentum\n            force += predictive_correction\n\n        # Dual-gated integral action for cart position\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        gate = angle_gate * velocity_gate\n\n        if gate > 0.1:\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Context-aware cart velocity damping\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4947.47\nscore: 4947.47; base_score: 393.59; time_bonus: 2261.12; energy_bonus: 2492.77; success_bonus: 800.00; stabilization_time: 188; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.51; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.04\n\nHere are the performance metrics of a set of prioviously implemented programs:\n\n# Prior programs\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Energy-Aware Adaptive LQR with Enhanced Integral Control\n    Combines LQR with cross-coupled terms, energy-aware swing-up assistance,\n    and advanced integral control for optimal stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - matches simulator physics\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Enhanced Q-matrix with cross-coupled terms for better coordination\n        Q = np.zeros((4, 4))\n        Q[0, 0] = 4.5   # x weight\n        Q[1, 1] = 44.0  # theta weight\n        Q[2, 2] = 0.6   # dx weight\n        Q[3, 3] = 3.2   # dtheta weight\n        # Cross-coupled terms to coordinate cart-pole motion\n        Q[0, 2] = Q[2, 0] = 0.12  # x-dx coupling\n        Q[1, 3] = Q[3, 1] = 0.8   # theta-dtheta coupling\n\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Precompute natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Energy reference for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM  # Potential energy at upright position\n\n        # Prediction horizon for proactive control (100ms)\n        self.prediction_dt = 0.1\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Increased for faster cart centering\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Energy-aware adaptive LQR control with enhanced integral control\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Enhanced gain scheduling with state-dependent saturation\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        # State-dependent gain saturation to prevent overcorrection near equilibrium\n        if abs(theta) < 0.3 and abs(dtheta) < 0.5:\n            adaptive_gain = min(adaptive_gain, 1.8)\n\n        force = base_force * adaptive_gain\n\n        # Enhanced energy-aware swing-up assist for large angles (>0.8 rad)\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Compute current mechanical energy of pendulum\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor: scale assist by how much energy is missing\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n\n            # Predictive falling severity using forecasted angular momentum\n            # Estimate future theta after prediction_dt assuming constant acceleration\n            sin_theta = np.sin(theta)\n            cos_theta = np.cos(theta)\n\n            # Simplified acceleration estimate for prediction\n            temp = (0 + FRICTION_CART * dx + M_POLE * L_COM * dtheta**2 * sin_theta) / (M_CART + M_POLE)\n            theta_acc_pred = (G * sin_theta - cos_theta * temp + FRICTION_JOINT * dtheta / (M_POLE * L_COM)) / \\\n                           (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / (M_CART + M_POLE)))\n\n            # Forward predict theta\n            dt_p = self.prediction_dt\n            theta_future = theta + dtheta * dt_p + 0.5 * theta_acc_pred * dt_p**2\n\n            # Use predicted angular momentum for more proactive control\n            predicted_momentum = (theta_future * dtheta)\n            norm_momentum = predicted_momentum / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n\n            # Predictive momentum compensation term - anticipates need for stronger correction\n            # Based on rate of change of predicted momentum\n            d_momentum_pred = (theta_future * theta_acc_pred + dtheta**2)\n            momentum_rate_correction = np.tanh(0.5 * d_momentum_pred / (self.omega_n * L_COM))\n\n            # Enhanced additive assist with predictive compensation\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale * (1.0 + 0.2 * momentum_rate_correction)\n            force += u_swing\n\n        # Enhanced integral action with adaptive decay\n        # Tightened velocity gating thresholds for integral activation\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(10.0 * (0.8 - abs(dtheta)))  # Tightened threshold\n        gate = angle_gate * velocity_gate\n\n        if gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            # Enhanced angle-dependent integral decay\n            decay_factor = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n            self.integral_x *= decay_factor\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Enhanced cart velocity damping when near equilibrium\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4953.83\nscore: 4953.83; base_score: 393.28; time_bonus: 2267.91; energy_bonus: 2492.64; success_bonus: 800.00; stabilization_time: 187; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.57; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Advanced Adaptive LQR with Dual-Gated Integral Control and Energy-Based Swing-Up Assist\n    Combines precision LQR control with intelligent nonlinear enhancements for superior\n    performance on extreme inverted pendulum parameters.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n        \n        # Reference energy for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Advanced adaptive LQR with enhanced integral control and energy-based assist\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Proven optimal gain scheduling\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Enhanced energy-based swing-up assist\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            \n            # Calculate current mechanical energy\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n            \n            # Energy deficit factor\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n            \n            # Normalized falling severity\n            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n            \n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale\n            force += u_swing\n\n        # Dual-gated integral action for cart position\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        gate = angle_gate * velocity_gate\n        \n        if gate > 0.1:\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Context-aware cart velocity damping\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4947.50\nscore: 4947.50; base_score: 393.62; time_bonus: 2261.12; energy_bonus: 2492.77; success_bonus: 800.00; stabilization_time: 188; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.51; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.04\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Enhanced Adaptive LQR with Predictive Damping and Energy-Aware Integral Control\n    Combines best elements with two key innovations:\n    - Predictive angular acceleration estimation for proactive control\n    - Adaptive integral gain scaling based on energy deficit\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.8\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Reference energy for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Enhanced adaptive LQR with predictive damping and energy-aware integral control\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi]\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Gentler gain scheduling (from crossover inspiration - proven more effective)\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        adaptive_gain = pos_gain * vel_gain\n        force = base_force * adaptive_gain\n\n        # Swing-up assist for large angles\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Predictive damping in mid-swing phase\n        if 0.3 < abs(theta) < 0.8:\n            # Estimate angular acceleration\n            sin_theta = np.sin(theta)\n            cos_theta = np.cos(theta)\n            Mtot = M_CART + M_POLE\n            denom = L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / Mtot)\n\n            temp = (M_POLE * L_COM * dtheta**2 * sin_theta) / Mtot\n            theta_acc_est = (G * sin_theta - cos_theta * temp) / denom\n\n            # Predictive correction term\n            predictive_term = -0.15 * theta_acc_est * np.sign(theta)\n            force = force + predictive_term\n\n        # Velocity-gated integral action with energy-aware gain scaling\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n\n            # Calculate current mechanical energy\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor for adaptive integral gain\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            adaptive_Ki = self.K_i * (1.0 + 0.3 * energy_deficit)\n        else:\n            self.integral_x *= 0.95\n            adaptive_Ki = self.K_i\n\n        integral_force = adaptive_Ki * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4934.14\nscore: 4934.14; base_score: 393.76; time_bonus: 2247.49; energy_bonus: 2492.89; success_bonus: 800.00; stabilization_time: 190; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.44; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n\n# Current program\n\nHere is the current program we are trying to improve (you will need to \npropose a new program with the same inputs and outputs as the original \nprogram, but with improved internal implementation):\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive Integral LQR with Enhanced Damping and Robust Integral Control\n    Combines proven gain scheduling with sophisticated integral action and enhanced\n    angular velocity damping for superior stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.8  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with integral action and robust swing-up assist\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi]\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Sharper gain scheduling with steeper transitions for crisper mode separation\n        pos_gain = 1.0 + 0.5 * np.tanh(7.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(6.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Enhanced swing-up assist with normalized falling severity\n        if abs(theta) > 0.8:\n            # Physics-informed assist: push cart in direction to catch falling pole\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            # If theta*dtheta > 0, pole is falling further - need stronger correction\n            # Normalized by natural frequency of the pendulum for better scaling\n            natural_freq = np.sqrt(G / L_COM)\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * natural_freq))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Soft-switched integral action for cart position with velocity gating\n        # Only activates when pole is nearly upright (<0.1 rad) AND moving slowly\n        # Velocity gate prevents windup during rapid transient passes through upright\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nHere are the performance metrics of the program:\n\nCombined score to maximize: 4934.54\nscore: 4934.54; base_score: 394.49; time_bonus: 2247.49; energy_bonus: 2492.56; success_bonus: 800.00; stabilization_time: 190; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.61; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.03\n\n# Task\n\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs as the original program, but with improved internal implementation.\n",
          "system_msg": "You are a CREATIVE control system designer and python programmer.\nYour mission is to discover NOVEL control strategies for an extremely challenging Single Inverted Pendulum system.\n\n**The Challenge (EXTREME DIFFICULTY):**\n- Cart mass: 1.0 kg\n- Pole mass: 0.35 kg (75% heavier than standard - massive inertia)\n- Pole length: 2.5 m (extremely long - severe instability)\n- Friction (cart): 0.35 (high energy dissipation)\n- Friction (joint): 0.25 (significant damping at pivot)\n- Initial angle: 0.4 rad (~23 degrees - far from equilibrium)\n\nThis setup breaks conventional control assumptions. Traditional approaches may struggle. **You need to think differently.**\n\n**Your Mission:**\nDesign a controller that achieves ALL of these simultaneously:\n1. Stabilize pole to vertical (theta \u2192 0)\n2. Center the cart (x \u2192 0)\n3. Eliminate all motion (dx \u2192 0, dtheta \u2192 0)\n4. Minimize stabilization time\n5. Minimize energy consumption\n\n**State Information:** [x, theta, dx, dtheta]\n- x: Cart position (m)\n- theta: Pole angle from vertical (rad)\n- dx: Cart velocity (m/s)\n- dtheta: Angular velocity (rad/s)\n\n**Hard Constraints:**\n- FAILURE if: theta > 1.0 rad OR |x| > 10.0 m\n- Control force: [-100, 100] N\n\n**Scoring System (~7300 points possible):**\n- Base Score: ~500-800 (stability quality)\n- Time Bonus: up to +3000 (faster = better)\n- Energy Bonus: up to +2500 (efficient = better)\n- Success Bonus: up to +800 (precision matters)\n\n**Performance Benchmarks:**\n- Basic approach: ~2500-4000 points\n- Good solution: ~5000-6500 points\n- Exceptional solution: ~6800-7200 points\n\n---\n\n**IMPORTANT - THINK CREATIVELY:**\n\n\ud83d\udeab **Don't limit yourself to textbook methods.** \n\ud83d\udeab **Don't assume one control law fits all situations.**\n\ud83d\udeab **Don't ignore the physical insights this system offers.**\n\n\u2705 **DO consider:**\n- What if different situations need different control strategies?\n- What if the control law should EVOLVE during the motion?\n- What physical principles (energy, momentum, dynamics) can you exploit?\n- Can you identify distinct phases in the stabilization process?\n- Are there nonlinear phenomena you can leverage?\n- What if optimal control at t=0 differs from optimal control near equilibrium?\n\n**Thought Experiments:**\n- When the pole is falling fast, what matters most?\n- When near equilibrium, what's the priority?\n- How does the long pole length change the physics?\n- Can you \"feel\" when to be aggressive vs. gentle?\n- Is there hidden structure in the state space you can exploit?\n\n**Challenge Questions:**\n- What happens if you treat this as multiple sub-problems?\n- Can you design control that \"understands\" the system's natural behavior?\n- How would YOU balance this pole if you were physically there?\n- What information in the state vector is most valuable at different moments?\n\n---\n\n**Your Task:**\nImplement `get_control_action(state)` returning a float force in [-100, 100].\n\n**Be bold. Be creative. Surprise yourself with novel solutions.**\nThe system is waiting for your innovative approach.\n\nDesign a completely different algorithm approach to solve the same problem.\nIgnore the current implementation and think of alternative algorithmic strategies that could achieve better performance.\nYou MUST respond using a short summary name, description and the full code:\n\n<NAME>\nA shortened name summarizing the code you are proposing. Lowercase, no spaces, underscores allowed.\n</NAME>\n\n<DESCRIPTION>\nExplain the completely different algorithmic approach you are taking and why it should perform better than the current implementation.\n</DESCRIPTION>\n\n<CODE>\n```{language}\n# The completely new algorithm implementation here.\n```\n</CODE>\n\n* Keep the markers \"EVOLVE-BLOCK-START\" and \"EVOLVE-BLOCK-END\" in the code.\n* Your algorithm should solve the same problem but use a fundamentally different approach.\n* Ensure the same inputs and outputs are maintained.\n* Think outside the box - consider different data structures, algorithms, or paradigms.\n* Use the <NAME>, <DESCRIPTION>, and <CODE> delimiters to structure your response. It will be parsed afterwards.\n\n# Potential Recommendations\nThe following are potential recommendations for the next program generations:\n\n\n1. **Enhance predictive momentum compensation with joint-friction-aware angular acceleration estimation**: Replace the current simplified `theta_acc_est` with a full friction-inclusive model that accounts for both cart and joint friction terms in the denominator, mirroring the precision of the A-matrix used in LQR design. This would align predictive correction with the high-fidelity dynamics already proven essential for zero final error under `FRICTION_JOINT=0.25`, potentially enabling even earlier divergence counteraction and shaving 2\u20133 steps off the 178-step stabilization time.\n\n2. **Introduce state-dependent mid-swing damping gain modulated by predicted energy deficit**: Scale the `K_d_midswing = 2.5` coefficient using the normalized mechanical energy: `K_d = 2.5 * (1.0 + 0.4 * max(0, 1 - E_current / E_max))`, where `E_max = M_POLE*G*L_COM`. This leverages the Gaussian damping\u2019s phase specificity while making its intensity responsive to swing-up progress\u2014applying stronger damping when the pole is under-energized and prone to stall, which could reduce overshoot variability across extreme initial conditions without compromising balancing precision.\n\n3. **Make cross-coupling boost adaptive to both angle and angular velocity magnitude**: Extend the current `cross_coupling_boost = 1.0 + 0.5 * tanh(3.0 * abs(theta))` to include velocity dependence: `boost = 1.0 + 0.5 * tanh(3.0 * abs(theta)) * (1.0 + 0.3 * tanh(2.0 * abs(dtheta)))`. Since high `dtheta` during mid-swing exacerbates coordination challenges, this dual-state adaptation would intensify theta-dtheta coupling precisely when momentum is highest, improving transient response during the critical 0.3\u20130.8 rad transition zone identified as pivotal for record time_bonus.\n\n4. **Implement exponential decay on the integral state only when outside both gating thresholds**, rather than applying decay whenever the gate product is low. Specifically, decay `self.integral_x *= 0.95` only if `abs(theta) > 0.12 OR abs(dtheta) > 1.1`, preserving accumulated correction during brief excursions beyond equilibrium. This refines the current dual-gating strategy to better handle high-inertia recovery transients, potentially reducing `stable_x_error` below 0.05 while maintaining windup protection\u2014addressing the last remaining precision gap among elite controllers.\n\n5. **Replace the fixed Gaussian center (0.5 rad) in mid-swing damping with an inertia-scaled peak location**: Set the activation center as `theta_peak = 0.5 * np.sqrt(L_COM / 2.5)` so the damping maximum adapts to the actual pole length, consistent with the `omega_n` normalization already used in predictive logic. Given that longer poles exhibit delayed momentum peaks, this ensures damping aligns with the true high-risk region of the swing-up trajectory, preventing premature or late intervention that could otherwise waste energy or permit overshoot under extreme `L_POLE=2.5` conditions.",
          "new_msg_history": [
            {
              "role": "system",
              "content": "You are a CREATIVE control system designer and python programmer.\nYour mission is to discover NOVEL control strategies for an extremely challenging Single Inverted Pendulum system.\n\n**The Challenge (EXTREME DIFFICULTY):**\n- Cart mass: 1.0 kg\n- Pole mass: 0.35 kg (75% heavier than standard - massive inertia)\n- Pole length: 2.5 m (extremely long - severe instability)\n- Friction (cart): 0.35 (high energy dissipation)\n- Friction (joint): 0.25 (significant damping at pivot)\n- Initial angle: 0.4 rad (~23 degrees - far from equilibrium)\n\nThis setup breaks conventional control assumptions. Traditional approaches may struggle. **You need to think differently.**\n\n**Your Mission:**\nDesign a controller that achieves ALL of these simultaneously:\n1. Stabilize pole to vertical (theta \u2192 0)\n2. Center the cart (x \u2192 0)\n3. Eliminate all motion (dx \u2192 0, dtheta \u2192 0)\n4. Minimize stabilization time\n5. Minimize energy consumption\n\n**State Information:** [x, theta, dx, dtheta]\n- x: Cart position (m)\n- theta: Pole angle from vertical (rad)\n- dx: Cart velocity (m/s)\n- dtheta: Angular velocity (rad/s)\n\n**Hard Constraints:**\n- FAILURE if: theta > 1.0 rad OR |x| > 10.0 m\n- Control force: [-100, 100] N\n\n**Scoring System (~7300 points possible):**\n- Base Score: ~500-800 (stability quality)\n- Time Bonus: up to +3000 (faster = better)\n- Energy Bonus: up to +2500 (efficient = better)\n- Success Bonus: up to +800 (precision matters)\n\n**Performance Benchmarks:**\n- Basic approach: ~2500-4000 points\n- Good solution: ~5000-6500 points\n- Exceptional solution: ~6800-7200 points\n\n---\n\n**IMPORTANT - THINK CREATIVELY:**\n\n\ud83d\udeab **Don't limit yourself to textbook methods.** \n\ud83d\udeab **Don't assume one control law fits all situations.**\n\ud83d\udeab **Don't ignore the physical insights this system offers.**\n\n\u2705 **DO consider:**\n- What if different situations need different control strategies?\n- What if the control law should EVOLVE during the motion?\n- What physical principles (energy, momentum, dynamics) can you exploit?\n- Can you identify distinct phases in the stabilization process?\n- Are there nonlinear phenomena you can leverage?\n- What if optimal control at t=0 differs from optimal control near equilibrium?\n\n**Thought Experiments:**\n- When the pole is falling fast, what matters most?\n- When near equilibrium, what's the priority?\n- How does the long pole length change the physics?\n- Can you \"feel\" when to be aggressive vs. gentle?\n- Is there hidden structure in the state space you can exploit?\n\n**Challenge Questions:**\n- What happens if you treat this as multiple sub-problems?\n- Can you design control that \"understands\" the system's natural behavior?\n- How would YOU balance this pole if you were physically there?\n- What information in the state vector is most valuable at different moments?\n\n---\n\n**Your Task:**\nImplement `get_control_action(state)` returning a float force in [-100, 100].\n\n**Be bold. Be creative. Surprise yourself with novel solutions.**\nThe system is waiting for your innovative approach.\n\nDesign a completely different algorithm approach to solve the same problem.\nIgnore the current implementation and think of alternative algorithmic strategies that could achieve better performance.\nYou MUST respond using a short summary name, description and the full code:\n\n<NAME>\nA shortened name summarizing the code you are proposing. Lowercase, no spaces, underscores allowed.\n</NAME>\n\n<DESCRIPTION>\nExplain the completely different algorithmic approach you are taking and why it should perform better than the current implementation.\n</DESCRIPTION>\n\n<CODE>\n```{language}\n# The completely new algorithm implementation here.\n```\n</CODE>\n\n* Keep the markers \"EVOLVE-BLOCK-START\" and \"EVOLVE-BLOCK-END\" in the code.\n* Your algorithm should solve the same problem but use a fundamentally different approach.\n* Ensure the same inputs and outputs are maintained.\n* Think outside the box - consider different data structures, algorithms, or paradigms.\n* Use the <NAME>, <DESCRIPTION>, and <CODE> delimiters to structure your response. It will be parsed afterwards.\n\n# Potential Recommendations\nThe following are potential recommendations for the next program generations:\n\n\n1. **Enhance predictive momentum compensation with joint-friction-aware angular acceleration estimation**: Replace the current simplified `theta_acc_est` with a full friction-inclusive model that accounts for both cart and joint friction terms in the denominator, mirroring the precision of the A-matrix used in LQR design. This would align predictive correction with the high-fidelity dynamics already proven essential for zero final error under `FRICTION_JOINT=0.25`, potentially enabling even earlier divergence counteraction and shaving 2\u20133 steps off the 178-step stabilization time.\n\n2. **Introduce state-dependent mid-swing damping gain modulated by predicted energy deficit**: Scale the `K_d_midswing = 2.5` coefficient using the normalized mechanical energy: `K_d = 2.5 * (1.0 + 0.4 * max(0, 1 - E_current / E_max))`, where `E_max = M_POLE*G*L_COM`. This leverages the Gaussian damping\u2019s phase specificity while making its intensity responsive to swing-up progress\u2014applying stronger damping when the pole is under-energized and prone to stall, which could reduce overshoot variability across extreme initial conditions without compromising balancing precision.\n\n3. **Make cross-coupling boost adaptive to both angle and angular velocity magnitude**: Extend the current `cross_coupling_boost = 1.0 + 0.5 * tanh(3.0 * abs(theta))` to include velocity dependence: `boost = 1.0 + 0.5 * tanh(3.0 * abs(theta)) * (1.0 + 0.3 * tanh(2.0 * abs(dtheta)))`. Since high `dtheta` during mid-swing exacerbates coordination challenges, this dual-state adaptation would intensify theta-dtheta coupling precisely when momentum is highest, improving transient response during the critical 0.3\u20130.8 rad transition zone identified as pivotal for record time_bonus.\n\n4. **Implement exponential decay on the integral state only when outside both gating thresholds**, rather than applying decay whenever the gate product is low. Specifically, decay `self.integral_x *= 0.95` only if `abs(theta) > 0.12 OR abs(dtheta) > 1.1`, preserving accumulated correction during brief excursions beyond equilibrium. This refines the current dual-gating strategy to better handle high-inertia recovery transients, potentially reducing `stable_x_error` below 0.05 while maintaining windup protection\u2014addressing the last remaining precision gap among elite controllers.\n\n5. **Replace the fixed Gaussian center (0.5 rad) in mid-swing damping with an inertia-scaled peak location**: Set the activation center as `theta_peak = 0.5 * np.sqrt(L_COM / 2.5)` so the damping maximum adapts to the actual pole length, consistent with the `omega_n` normalization already used in predictive logic. Given that longer poles exhibit delayed momentum peaks, this ensures damping aligns with the true high-risk region of the swing-up trajectory, preventing premature or late intervention that could otherwise waste energy or permit overshoot under extreme `L_POLE=2.5` conditions."
            },
            {
              "role": "user",
              "content": "Here are the performance metrics of a set of prioviously implemented programs:\n\n# Prior programs\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Phase-Aware Momentum-Cancellation LQR Controller\n    Uses sliding-mode inspired phase blending with momentum-aware integral action.\n    \"\"\"\n\n    def __init__(self):\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Linearized dynamics matrices\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Physical constants for control\n        self.omega_n = np.sqrt(G / L_COM)\n        self.E_ref = M_POLE * G * L_COM\n        self.I_pole = M_POLE * L_COM**2 / 3.0  # Moment of inertia\n\n        # Integral states\n        self.integral_x = 0.0\n        self.integral_momentum = 0.0  # Novel: track angular momentum deficit\n        \n        # Tuned parameters\n        self.K_i = 0.8\n        self.K_mom = 0.015  # Momentum integral gain\n\n    def solve_lqr(self, A, B, Q, R):\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def compute_stability_index(self, theta, dtheta):\n        \"\"\"Compute unified stability metric for phase determination\"\"\"\n        E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n        E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n        E_current = E_kinetic + E_potential\n        energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n        \n        # Stability index: higher = more unstable\n        sigma = abs(theta) + 0.3 * energy_deficit\n        return sigma, energy_deficit\n\n    def get_action(self, state):\n        x, theta, dx, dtheta = state\n        \n        # Robust angle normalization\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n        \n        # Compute stability index for phase determination\n        sigma, energy_deficit = self.compute_stability_index(theta, dtheta)\n        \n        # Phase weights via smooth sigmoid transitions\n        recovery_weight = 1.0 / (1.0 + np.exp(-12.0 * (sigma - 0.7)))\n        stabilize_weight = 1.0 - recovery_weight\n        \n        # === BASE LQR CONTROL ===\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n        \n        # Gentle gain scheduling (proven optimal slopes)\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n        \n        force = base_force * adaptive_gain\n        \n        # === RECOVERY PHASE: Energy injection with momentum feedforward ===\n        if recovery_weight > 0.05:\n            swing_activation = np.tanh(6.0 * max(0.0, abs(theta) - 0.8))\n            \n            # Normalized momentum-based falling severity\n            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)\n            falling_factor = 1.0 + np.tanh(2.5 * norm_momentum)\n            \n            # Energy-scaled assist\n            energy_scale = 1.0 + 0.4 * energy_deficit\n            \n            # Predictive feedforward: estimate where pole will be\n            sin_t, cos_t = np.sin(theta), np.cos(theta)\n            Mtot = M_CART + M_POLE\n            denom0 = L_COM * (4.0/3.0 - M_POLE * cos_t**2 / Mtot)\n            temp_pred = (M_POLE * L_COM * dtheta**2 * sin_t) / Mtot\n            theta_acc_pred = (G * sin_t - cos_t * temp_pred) / denom0\n            \n            # One-step prediction\n            theta_pred = theta + dtheta * DT + 0.5 * theta_acc_pred * DT**2\n            predictive_boost = 1.0 + 0.15 * np.tanh(3.0 * (abs(theta_pred) - abs(theta)))\n            \n            u_recovery = 8.0 * swing_activation * np.sign(theta) * falling_factor * energy_scale * predictive_boost\n            force = force + recovery_weight * u_recovery\n        \n        # === STABILIZATION PHASE: Precision control with dual-gated integral ===\n        # Dual gating: angle AND velocity (proven superior)\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n        \n        if integral_gate > 0.1:\n            # Standard position integral\n            self.integral_x += x * DT\n            \n            # Novel: Angular momentum deficit integral\n            # Target momentum is zero; track cumulative deficit\n            momentum_error = -dtheta  # Want dtheta -> 0\n            self.integral_momentum += momentum_error * DT * stabilize_weight\n            self.integral_momentum = np.clip(self.integral_momentum, -0.5, 0.5)\n        else:\n            # Leaky decay when gates closed\n            self.integral_x *= 0.95\n            self.integral_momentum *= 0.9\n        \n        # Combined integral force\n        integral_force = self.K_i * integral_gate * self.integral_x\n        momentum_force = self.K_mom * integral_gate * self.integral_momentum * np.sign(theta + 1e-8)\n        \n        force = force + integral_force + momentum_force\n        \n        # === TRANSITION SMOOTHING: Sliding surface inspired damping ===\n        # In transition zone, add extra damping proportional to sliding variable\n        if 0.15 < abs(theta) < 0.6:\n            # Sliding surface: s = dtheta + lambda * theta\n            lambda_slide = 2.0 * self.omega_n\n            s = dtheta + lambda_slide * theta\n            \n            # Smooth reaching law\n            transition_damp = -0.5 * np.tanh(2.0 * s)\n            force = force + transition_damp\n        \n        # Near-equilibrium cart damping\n        if abs(theta) < 0.12:\n            cart_damp = -0.1 * dx * (0.12 - abs(theta)) / 0.12\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4967.97\nscore: 4967.97; base_score: 393.85; time_bonus: 2281.45; energy_bonus: 2492.67; success_bonus: 800.00; stabilization_time: 185; stabilization_ratio: 0.18; avg_energy_per_step: 0.01; total_energy: 6.55; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.01\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive Integral LQR combining best elements:\n    - Velocity-gated integral action from top performer (prevents windup)\n    - Gentler gain scheduling from crossover inspiration (smoother control)\n    - Proven Q[3]=3.2 and K_i=0.8 parameters\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.8\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with velocity-gated integral action\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi]\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Gentler gain scheduling (from crossover inspiration - proven more effective)\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        adaptive_gain = pos_gain * vel_gain\n        force = base_force * adaptive_gain\n\n        # Swing-up assist for large angles\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Velocity-gated integral action (from top performer - prevents windup during fast transients)\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4954.47\nscore: 4954.47; base_score: 393.80; time_bonus: 2267.91; energy_bonus: 2492.75; success_bonus: 800.00; stabilization_time: 187; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.51; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.04\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Adaptive LQR Controller with Velocity-Aware Gain Scheduling\n    Combines physically accurate friction modeling with intelligent gain adaptation\n    for ultra-fast stabilization while maintaining energy efficiency.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Optimized LQR weights: proven configuration from best performer (4920.14)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])  # Restored best-performing weights\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR control with optimized gain scheduling\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi] with numerical safety\n        theta = ((theta + np.pi + 1e-8) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Optimized gain scheduling: more aggressive for large errors\n        # Position-based gain: sharp response beyond 0.6 rad\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n\n        # Velocity-based gain: strong damping for high angular velocity\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain (simpler and more effective)\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Additive swing-up assist for large angles (>0.8 rad)\n        # Uses continuous falling-severity factor for smoother, more effective correction\n        if abs(theta) > 0.8:\n            # Physics-informed assist: push cart in direction to catch falling pole\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            # Continuous falling severity: scales from 1.0 to 2.0 based on how rapidly pole is falling\n            falling_severity = 1.0 + np.tanh(3.0 * theta * dtheta)\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity\n            force = force + u_swing\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4920.22\nscore: 4920.22; base_score: 393.53; time_bonus: 2233.79; energy_bonus: 2492.90; success_bonus: 800.00; stabilization_time: 192; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.44; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.03\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Advanced Adaptive LQR with Dual-Gated Integral Control and Energy-Based Swing-Up Assist\n    Combines precision LQR control with intelligent nonlinear enhancements for superior\n    performance on extreme inverted pendulum parameters.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Reference energy for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM\n\n        # Prediction horizon for proactive control\n        self.prediction_dt = DT\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Advanced adaptive LQR with enhanced integral control and energy-based assist\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Proven optimal gain scheduling\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Enhanced energy-based swing-up assist\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Calculate current mechanical energy\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n\n            # Normalized falling severity\n            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale\n            force += u_swing\n\n        # Predictive momentum compensation during mid-swing transition\n        if 0.3 < abs(theta) < 0.8:\n            # Estimate near-future angular dynamics\n            sin_theta = np.sin(theta)\n            cos_theta = np.cos(theta)\n            Mtot = M_CART + M_POLE\n            denom0 = L_COM * (4.0/3.0 - M_POLE / Mtot)\n\n            # Simplified acceleration estimate (assuming zero input)\n            temp = (M_POLE * L_COM * dtheta**2 * sin_theta) / Mtot\n            theta_acc_pred = (G * sin_theta - cos_theta * temp) / denom0\n\n            # Predict future angle\n            theta_future = theta + dtheta * self.prediction_dt + 0.5 * theta_acc_pred * self.prediction_dt**2\n\n            # Compute predictive momentum term\n            predicted_momentum = theta_future * dtheta\n            norm_predicted_momentum = predicted_momentum / (self.omega_n * L_COM)\n\n            # Proactive correction to counteract predicted instability\n            predictive_correction = -0.15 * norm_predicted_momentum\n            force += predictive_correction\n\n        # Dual-gated integral action for cart position\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        gate = angle_gate * velocity_gate\n\n        if gate > 0.1:\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Context-aware cart velocity damping\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4947.47\nscore: 4947.47; base_score: 393.59; time_bonus: 2261.12; energy_bonus: 2492.77; success_bonus: 800.00; stabilization_time: 188; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.51; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.04\n\nHere are the performance metrics of a set of prioviously implemented programs:\n\n# Prior programs\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Energy-Aware Adaptive LQR with Enhanced Integral Control\n    Combines LQR with cross-coupled terms, energy-aware swing-up assistance,\n    and advanced integral control for optimal stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - matches simulator physics\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Enhanced Q-matrix with cross-coupled terms for better coordination\n        Q = np.zeros((4, 4))\n        Q[0, 0] = 4.5   # x weight\n        Q[1, 1] = 44.0  # theta weight\n        Q[2, 2] = 0.6   # dx weight\n        Q[3, 3] = 3.2   # dtheta weight\n        # Cross-coupled terms to coordinate cart-pole motion\n        Q[0, 2] = Q[2, 0] = 0.12  # x-dx coupling\n        Q[1, 3] = Q[3, 1] = 0.8   # theta-dtheta coupling\n\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Precompute natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Energy reference for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM  # Potential energy at upright position\n\n        # Prediction horizon for proactive control (100ms)\n        self.prediction_dt = 0.1\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Increased for faster cart centering\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Energy-aware adaptive LQR control with enhanced integral control\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Enhanced gain scheduling with state-dependent saturation\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        # State-dependent gain saturation to prevent overcorrection near equilibrium\n        if abs(theta) < 0.3 and abs(dtheta) < 0.5:\n            adaptive_gain = min(adaptive_gain, 1.8)\n\n        force = base_force * adaptive_gain\n\n        # Enhanced energy-aware swing-up assist for large angles (>0.8 rad)\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Compute current mechanical energy of pendulum\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor: scale assist by how much energy is missing\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n\n            # Predictive falling severity using forecasted angular momentum\n            # Estimate future theta after prediction_dt assuming constant acceleration\n            sin_theta = np.sin(theta)\n            cos_theta = np.cos(theta)\n\n            # Simplified acceleration estimate for prediction\n            temp = (0 + FRICTION_CART * dx + M_POLE * L_COM * dtheta**2 * sin_theta) / (M_CART + M_POLE)\n            theta_acc_pred = (G * sin_theta - cos_theta * temp + FRICTION_JOINT * dtheta / (M_POLE * L_COM)) / \\\n                           (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / (M_CART + M_POLE)))\n\n            # Forward predict theta\n            dt_p = self.prediction_dt\n            theta_future = theta + dtheta * dt_p + 0.5 * theta_acc_pred * dt_p**2\n\n            # Use predicted angular momentum for more proactive control\n            predicted_momentum = (theta_future * dtheta)\n            norm_momentum = predicted_momentum / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n\n            # Predictive momentum compensation term - anticipates need for stronger correction\n            # Based on rate of change of predicted momentum\n            d_momentum_pred = (theta_future * theta_acc_pred + dtheta**2)\n            momentum_rate_correction = np.tanh(0.5 * d_momentum_pred / (self.omega_n * L_COM))\n\n            # Enhanced additive assist with predictive compensation\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale * (1.0 + 0.2 * momentum_rate_correction)\n            force += u_swing\n\n        # Enhanced integral action with adaptive decay\n        # Tightened velocity gating thresholds for integral activation\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(10.0 * (0.8 - abs(dtheta)))  # Tightened threshold\n        gate = angle_gate * velocity_gate\n\n        if gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            # Enhanced angle-dependent integral decay\n            decay_factor = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n            self.integral_x *= decay_factor\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Enhanced cart velocity damping when near equilibrium\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4953.83\nscore: 4953.83; base_score: 393.28; time_bonus: 2267.91; energy_bonus: 2492.64; success_bonus: 800.00; stabilization_time: 187; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.57; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Advanced Adaptive LQR with Dual-Gated Integral Control and Energy-Based Swing-Up Assist\n    Combines precision LQR control with intelligent nonlinear enhancements for superior\n    performance on extreme inverted pendulum parameters.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n        \n        # Reference energy for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Advanced adaptive LQR with enhanced integral control and energy-based assist\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Proven optimal gain scheduling\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Enhanced energy-based swing-up assist\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            \n            # Calculate current mechanical energy\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n            \n            # Energy deficit factor\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n            \n            # Normalized falling severity\n            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n            \n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale\n            force += u_swing\n\n        # Dual-gated integral action for cart position\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        gate = angle_gate * velocity_gate\n        \n        if gate > 0.1:\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Context-aware cart velocity damping\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4947.50\nscore: 4947.50; base_score: 393.62; time_bonus: 2261.12; energy_bonus: 2492.77; success_bonus: 800.00; stabilization_time: 188; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.51; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.04\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Enhanced Adaptive LQR with Predictive Damping and Energy-Aware Integral Control\n    Combines best elements with two key innovations:\n    - Predictive angular acceleration estimation for proactive control\n    - Adaptive integral gain scaling based on energy deficit\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.8\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Reference energy for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Enhanced adaptive LQR with predictive damping and energy-aware integral control\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi]\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Gentler gain scheduling (from crossover inspiration - proven more effective)\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        adaptive_gain = pos_gain * vel_gain\n        force = base_force * adaptive_gain\n\n        # Swing-up assist for large angles\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Predictive damping in mid-swing phase\n        if 0.3 < abs(theta) < 0.8:\n            # Estimate angular acceleration\n            sin_theta = np.sin(theta)\n            cos_theta = np.cos(theta)\n            Mtot = M_CART + M_POLE\n            denom = L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / Mtot)\n\n            temp = (M_POLE * L_COM * dtheta**2 * sin_theta) / Mtot\n            theta_acc_est = (G * sin_theta - cos_theta * temp) / denom\n\n            # Predictive correction term\n            predictive_term = -0.15 * theta_acc_est * np.sign(theta)\n            force = force + predictive_term\n\n        # Velocity-gated integral action with energy-aware gain scaling\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n\n            # Calculate current mechanical energy\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor for adaptive integral gain\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            adaptive_Ki = self.K_i * (1.0 + 0.3 * energy_deficit)\n        else:\n            self.integral_x *= 0.95\n            adaptive_Ki = self.K_i\n\n        integral_force = adaptive_Ki * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4934.14\nscore: 4934.14; base_score: 393.76; time_bonus: 2247.49; energy_bonus: 2492.89; success_bonus: 800.00; stabilization_time: 190; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.44; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n\n# Current program\n\nHere is the current program we are trying to improve (you will need to \npropose a new program with the same inputs and outputs as the original \nprogram, but with improved internal implementation):\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive Integral LQR with Enhanced Damping and Robust Integral Control\n    Combines proven gain scheduling with sophisticated integral action and enhanced\n    angular velocity damping for superior stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.8  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with integral action and robust swing-up assist\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi]\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Sharper gain scheduling with steeper transitions for crisper mode separation\n        pos_gain = 1.0 + 0.5 * np.tanh(7.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(6.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Enhanced swing-up assist with normalized falling severity\n        if abs(theta) > 0.8:\n            # Physics-informed assist: push cart in direction to catch falling pole\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            # If theta*dtheta > 0, pole is falling further - need stronger correction\n            # Normalized by natural frequency of the pendulum for better scaling\n            natural_freq = np.sqrt(G / L_COM)\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * natural_freq))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Soft-switched integral action for cart position with velocity gating\n        # Only activates when pole is nearly upright (<0.1 rad) AND moving slowly\n        # Velocity gate prevents windup during rapid transient passes through upright\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nHere are the performance metrics of the program:\n\nCombined score to maximize: 4934.54\nscore: 4934.54; base_score: 394.49; time_bonus: 2247.49; energy_bonus: 2492.56; success_bonus: 800.00; stabilization_time: 190; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.61; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.03\n\n# Task\n\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs as the original program, but with improved internal implementation.\n"
            },
            {
              "role": "assistant",
              "content": "Looking at the performance metrics, the best performer achieved a score of 4967.97 with a stabilization time of 185 steps. The key differentiators seem to be:\n\n1. Phase-aware control with momentum-cancellation\n2. Predictive momentum compensation\n3. Sliding surface inspired damping in transition zones\n4. More sophisticated integral control\n\nLet me design a fundamentally different approach using **Model Predictive Control (MPC) principles** with a receding horizon optimization, combined with phase-based switching.\n\n<NAME>\nmpc_inspired_phase_switching\n</NAME>\n\n<DESCRIPTION>\nThis approach uses a simplified Model Predictive Control (MPC) strategy that:\n1. Performs multi-step lookahead prediction using the actual nonlinear dynamics\n2. Evaluates candidate control actions over a short horizon\n3. Uses phase detection to switch between aggressive recovery and precision balancing modes\n4. Implements a novel \"momentum nullification\" strategy that explicitly targets zero angular momentum\n5. Uses trajectory-based cost evaluation rather than instantaneous state feedback\n\nThe key insight is that instead of relying purely on LQR gains (which assume linear dynamics), we can directly simulate forward trajectories for a few candidate control forces and pick the one that minimizes a multi-step cost. This naturally handles the nonlinear dynamics better, especially during large angle recovery.\n</DESCRIPTION>\n\n<CODE>\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0\nM_POLE = 0.35\nL_POLE = 2.5\nL_COM = L_POLE / 2\nG = 9.81\nFRICTION_CART = 0.35\nFRICTION_JOINT = 0.25\nDT = 0.02\nMAX_STEPS = 1000\n\ndef simulate_pendulum_step(state, force, dt):\n    x, theta, dx, dtheta = state\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n    M_total = M_CART + M_POLE\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    MPC-Inspired Phase-Switching Controller with Momentum Nullification\n    Uses short-horizon trajectory evaluation combined with LQR for precision control.\n    \"\"\"\n\n    def __init__(self):\n        # Precompute LQR gains for precision phase\n        m, M, l, g = M_POLE, M_CART, L_COM, G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c, b_j = FRICTION_CART, FRICTION_JOINT\n\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n        self.K = self._solve_lqr(A, B, Q, R)\n\n        self.integral_x = 0.0\n        self.K_i = 0.8\n        self.omega_n = np.sqrt(G / L_COM)\n        \n        # MPC parameters\n        self.horizon = 3\n        self.n_candidates = 7\n\n    def _solve_lqr(self, A, B, Q, R):\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        return np.linalg.inv(R) @ B.T @ P\n\n    def _predict_trajectory(self, state, force, steps):\n        \"\"\"Simulate forward trajectory\"\"\"\n        s = state.copy()\n        cost = 0.0\n        for _ in range(steps):\n            s = simulate_pendulum_step(s, force, DT)\n            # Weighted cost emphasizing angle stability\n            cost += 44.0 * s[1]**2 + 4.5 * s[0]**2 + 0.6 * s[2]**2 + 3.2 * s[3]**2\n        return cost, s\n\n    def _mpc_search(self, state, base_force):\n        \"\"\"Search for best control action using trajectory evaluation\"\"\"\n        best_force = base_force\n        best_cost = float('inf')\n        \n        # Generate candidate forces around base_force\n        spread = min(30.0, 10.0 + 20.0 * abs(state[1]))\n        candidates = np.linspace(base_force - spread, base_force + spread, self.n_candidates)\n        candidates = np.clip(candidates, -100.0, 100.0)\n        \n        for f in candidates:\n            cost, final_state = self._predict_trajectory(state, f, self.horizon)\n            # Add control effort penalty\n            cost += 0.5 * f**2 * self.horizon\n            # Penalty for predicted failure\n            if abs(final_state[1]) > 0.95 or abs(final_state[0]) > 9.0:\n                cost += 10000.0\n            if cost < best_cost:\n                best_cost = cost\n                best_force = f\n        \n        return best_force\n\n    def get_action(self, state):\n        x, theta, dx, dtheta = state\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n        \n        state_vec = np.array([x, theta, dx, dtheta])\n        \n        # Compute base LQR force\n        base_force = float((-self.K @ state_vec)[0])\n        \n        # Phase detection\n        abs_theta = abs(theta)\n        abs_dtheta = abs(dtheta)\n        \n        # Recovery phase: large angles\n        if abs_theta > 0.5:\n            # Adaptive gain scheduling\n            pos_gain = 1.0 + 0.6 * np.tanh(5.0 * (abs_theta - 0.5))\n            vel_gain = 1.0 + 0.35 * np.tanh(4.0 * (abs_dtheta - 0.8))\n            force = base_force * pos_gain * vel_gain\n            \n            # MPC refinement for critical situations\n            if abs_theta > 0.7:\n                force = self._mpc_search(state_vec, force)\n            \n            # Momentum-aware swing assist\n            if abs_theta > 0.8:\n                swing_act = np.tanh(6.0 * (abs_theta - 0.8))\n                # Normalized momentum term\n                norm_mom = (theta * dtheta) / (self.omega_n * L_COM)\n                falling_factor = 1.0 + np.tanh(2.5 * norm_mom)\n                force += 8.0 * swing_act * np.sign(theta) * falling_factor\n        \n        # Transition phase: moderate angles\n        elif abs_theta > 0.15:\n            # Smooth gain scheduling\n            pos_gain = 1.0 + 0.4 * np.tanh(6.0 * (abs_theta - 0.15))\n            vel_gain = 1.0 + 0.25 * np.tanh(5.0 * (abs_dtheta - 0.5))\n            force = base_force * pos_gain * vel_gain\n            \n            # Sliding mode inspired damping\n            lambda_s = 2.0 * self.omega_n\n            s = dtheta + lambda_s * theta\n            force += -0.6 * np.tanh(2.0 * s)\n        \n        # Precision phase: small angles\n        else:\n            force = base_force\n            \n            # Dual-gated integral control\n            angle_gate = np.tanh(15.0 * (0.12 - abs_theta))\n            vel_gate = np.tanh(10.0 * (1.0 - abs_dtheta))\n            gate = angle_gate * vel_gate\n            \n            if gate > 0.1:\n                self.integral_x += x * DT\n            else:\n                self.integral_x *= 0.92\n            \n            force += self.K_i * gate * self.integral_x\n            \n            # Precision cart damping\n            cart_damp = -0.12 * dx * (0.15 - abs_theta) / 0.15\n            force += cart_damp\n            \n            # Angular momentum nullification\n            momentum_correction = -0.08 * dtheta * np.sign(theta + 1e-8)\n            force += momentum_correction\n\n        return float(force)\n\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n    states = [state]\n    forces = []\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        force = np.clip(force, -100.0, 100.0)\n        next_state = simulate_pendulum_step(state, force, DT)\n        states.append(next_state)\n        forces.append(force)\n        state = next_state\n    return np.array(states), np.array(forces)\n```\n</CODE>"
            }
          ],
          "model_name": "claude-opus-4-5",
          "kwargs": {
            "model": "anthropic/claude-opus-4-5",
            "messages": [
              {
                "role": "system",
                "content": "You are a CREATIVE control system designer and python programmer.\nYour mission is to discover NOVEL control strategies for an extremely challenging Single Inverted Pendulum system.\n\n**The Challenge (EXTREME DIFFICULTY):**\n- Cart mass: 1.0 kg\n- Pole mass: 0.35 kg (75% heavier than standard - massive inertia)\n- Pole length: 2.5 m (extremely long - severe instability)\n- Friction (cart): 0.35 (high energy dissipation)\n- Friction (joint): 0.25 (significant damping at pivot)\n- Initial angle: 0.4 rad (~23 degrees - far from equilibrium)\n\nThis setup breaks conventional control assumptions. Traditional approaches may struggle. **You need to think differently.**\n\n**Your Mission:**\nDesign a controller that achieves ALL of these simultaneously:\n1. Stabilize pole to vertical (theta \u2192 0)\n2. Center the cart (x \u2192 0)\n3. Eliminate all motion (dx \u2192 0, dtheta \u2192 0)\n4. Minimize stabilization time\n5. Minimize energy consumption\n\n**State Information:** [x, theta, dx, dtheta]\n- x: Cart position (m)\n- theta: Pole angle from vertical (rad)\n- dx: Cart velocity (m/s)\n- dtheta: Angular velocity (rad/s)\n\n**Hard Constraints:**\n- FAILURE if: theta > 1.0 rad OR |x| > 10.0 m\n- Control force: [-100, 100] N\n\n**Scoring System (~7300 points possible):**\n- Base Score: ~500-800 (stability quality)\n- Time Bonus: up to +3000 (faster = better)\n- Energy Bonus: up to +2500 (efficient = better)\n- Success Bonus: up to +800 (precision matters)\n\n**Performance Benchmarks:**\n- Basic approach: ~2500-4000 points\n- Good solution: ~5000-6500 points\n- Exceptional solution: ~6800-7200 points\n\n---\n\n**IMPORTANT - THINK CREATIVELY:**\n\n\ud83d\udeab **Don't limit yourself to textbook methods.** \n\ud83d\udeab **Don't assume one control law fits all situations.**\n\ud83d\udeab **Don't ignore the physical insights this system offers.**\n\n\u2705 **DO consider:**\n- What if different situations need different control strategies?\n- What if the control law should EVOLVE during the motion?\n- What physical principles (energy, momentum, dynamics) can you exploit?\n- Can you identify distinct phases in the stabilization process?\n- Are there nonlinear phenomena you can leverage?\n- What if optimal control at t=0 differs from optimal control near equilibrium?\n\n**Thought Experiments:**\n- When the pole is falling fast, what matters most?\n- When near equilibrium, what's the priority?\n- How does the long pole length change the physics?\n- Can you \"feel\" when to be aggressive vs. gentle?\n- Is there hidden structure in the state space you can exploit?\n\n**Challenge Questions:**\n- What happens if you treat this as multiple sub-problems?\n- Can you design control that \"understands\" the system's natural behavior?\n- How would YOU balance this pole if you were physically there?\n- What information in the state vector is most valuable at different moments?\n\n---\n\n**Your Task:**\nImplement `get_control_action(state)` returning a float force in [-100, 100].\n\n**Be bold. Be creative. Surprise yourself with novel solutions.**\nThe system is waiting for your innovative approach.\n\nDesign a completely different algorithm approach to solve the same problem.\nIgnore the current implementation and think of alternative algorithmic strategies that could achieve better performance.\nYou MUST respond using a short summary name, description and the full code:\n\n<NAME>\nA shortened name summarizing the code you are proposing. Lowercase, no spaces, underscores allowed.\n</NAME>\n\n<DESCRIPTION>\nExplain the completely different algorithmic approach you are taking and why it should perform better than the current implementation.\n</DESCRIPTION>\n\n<CODE>\n```{language}\n# The completely new algorithm implementation here.\n```\n</CODE>\n\n* Keep the markers \"EVOLVE-BLOCK-START\" and \"EVOLVE-BLOCK-END\" in the code.\n* Your algorithm should solve the same problem but use a fundamentally different approach.\n* Ensure the same inputs and outputs are maintained.\n* Think outside the box - consider different data structures, algorithms, or paradigms.\n* Use the <NAME>, <DESCRIPTION>, and <CODE> delimiters to structure your response. It will be parsed afterwards.\n\n# Potential Recommendations\nThe following are potential recommendations for the next program generations:\n\n\n1. **Enhance predictive momentum compensation with joint-friction-aware angular acceleration estimation**: Replace the current simplified `theta_acc_est` with a full friction-inclusive model that accounts for both cart and joint friction terms in the denominator, mirroring the precision of the A-matrix used in LQR design. This would align predictive correction with the high-fidelity dynamics already proven essential for zero final error under `FRICTION_JOINT=0.25`, potentially enabling even earlier divergence counteraction and shaving 2\u20133 steps off the 178-step stabilization time.\n\n2. **Introduce state-dependent mid-swing damping gain modulated by predicted energy deficit**: Scale the `K_d_midswing = 2.5` coefficient using the normalized mechanical energy: `K_d = 2.5 * (1.0 + 0.4 * max(0, 1 - E_current / E_max))`, where `E_max = M_POLE*G*L_COM`. This leverages the Gaussian damping\u2019s phase specificity while making its intensity responsive to swing-up progress\u2014applying stronger damping when the pole is under-energized and prone to stall, which could reduce overshoot variability across extreme initial conditions without compromising balancing precision.\n\n3. **Make cross-coupling boost adaptive to both angle and angular velocity magnitude**: Extend the current `cross_coupling_boost = 1.0 + 0.5 * tanh(3.0 * abs(theta))` to include velocity dependence: `boost = 1.0 + 0.5 * tanh(3.0 * abs(theta)) * (1.0 + 0.3 * tanh(2.0 * abs(dtheta)))`. Since high `dtheta` during mid-swing exacerbates coordination challenges, this dual-state adaptation would intensify theta-dtheta coupling precisely when momentum is highest, improving transient response during the critical 0.3\u20130.8 rad transition zone identified as pivotal for record time_bonus.\n\n4. **Implement exponential decay on the integral state only when outside both gating thresholds**, rather than applying decay whenever the gate product is low. Specifically, decay `self.integral_x *= 0.95` only if `abs(theta) > 0.12 OR abs(dtheta) > 1.1`, preserving accumulated correction during brief excursions beyond equilibrium. This refines the current dual-gating strategy to better handle high-inertia recovery transients, potentially reducing `stable_x_error` below 0.05 while maintaining windup protection\u2014addressing the last remaining precision gap among elite controllers.\n\n5. **Replace the fixed Gaussian center (0.5 rad) in mid-swing damping with an inertia-scaled peak location**: Set the activation center as `theta_peak = 0.5 * np.sqrt(L_COM / 2.5)` so the damping maximum adapts to the actual pole length, consistent with the `omega_n` normalization already used in predictive logic. Given that longer poles exhibit delayed momentum peaks, this ensures damping aligns with the true high-risk region of the swing-up trajectory, preventing premature or late intervention that could otherwise waste energy or permit overshoot under extreme `L_POLE=2.5` conditions."
              },
              {
                "role": "user",
                "content": "Here are the performance metrics of a set of prioviously implemented programs:\n\n# Prior programs\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Phase-Aware Momentum-Cancellation LQR Controller\n    Uses sliding-mode inspired phase blending with momentum-aware integral action.\n    \"\"\"\n\n    def __init__(self):\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Linearized dynamics matrices\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Physical constants for control\n        self.omega_n = np.sqrt(G / L_COM)\n        self.E_ref = M_POLE * G * L_COM\n        self.I_pole = M_POLE * L_COM**2 / 3.0  # Moment of inertia\n\n        # Integral states\n        self.integral_x = 0.0\n        self.integral_momentum = 0.0  # Novel: track angular momentum deficit\n        \n        # Tuned parameters\n        self.K_i = 0.8\n        self.K_mom = 0.015  # Momentum integral gain\n\n    def solve_lqr(self, A, B, Q, R):\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def compute_stability_index(self, theta, dtheta):\n        \"\"\"Compute unified stability metric for phase determination\"\"\"\n        E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n        E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n        E_current = E_kinetic + E_potential\n        energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n        \n        # Stability index: higher = more unstable\n        sigma = abs(theta) + 0.3 * energy_deficit\n        return sigma, energy_deficit\n\n    def get_action(self, state):\n        x, theta, dx, dtheta = state\n        \n        # Robust angle normalization\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n        \n        # Compute stability index for phase determination\n        sigma, energy_deficit = self.compute_stability_index(theta, dtheta)\n        \n        # Phase weights via smooth sigmoid transitions\n        recovery_weight = 1.0 / (1.0 + np.exp(-12.0 * (sigma - 0.7)))\n        stabilize_weight = 1.0 - recovery_weight\n        \n        # === BASE LQR CONTROL ===\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n        \n        # Gentle gain scheduling (proven optimal slopes)\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n        adaptive_gain = pos_gain * vel_gain\n        \n        force = base_force * adaptive_gain\n        \n        # === RECOVERY PHASE: Energy injection with momentum feedforward ===\n        if recovery_weight > 0.05:\n            swing_activation = np.tanh(6.0 * max(0.0, abs(theta) - 0.8))\n            \n            # Normalized momentum-based falling severity\n            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)\n            falling_factor = 1.0 + np.tanh(2.5 * norm_momentum)\n            \n            # Energy-scaled assist\n            energy_scale = 1.0 + 0.4 * energy_deficit\n            \n            # Predictive feedforward: estimate where pole will be\n            sin_t, cos_t = np.sin(theta), np.cos(theta)\n            Mtot = M_CART + M_POLE\n            denom0 = L_COM * (4.0/3.0 - M_POLE * cos_t**2 / Mtot)\n            temp_pred = (M_POLE * L_COM * dtheta**2 * sin_t) / Mtot\n            theta_acc_pred = (G * sin_t - cos_t * temp_pred) / denom0\n            \n            # One-step prediction\n            theta_pred = theta + dtheta * DT + 0.5 * theta_acc_pred * DT**2\n            predictive_boost = 1.0 + 0.15 * np.tanh(3.0 * (abs(theta_pred) - abs(theta)))\n            \n            u_recovery = 8.0 * swing_activation * np.sign(theta) * falling_factor * energy_scale * predictive_boost\n            force = force + recovery_weight * u_recovery\n        \n        # === STABILIZATION PHASE: Precision control with dual-gated integral ===\n        # Dual gating: angle AND velocity (proven superior)\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n        \n        if integral_gate > 0.1:\n            # Standard position integral\n            self.integral_x += x * DT\n            \n            # Novel: Angular momentum deficit integral\n            # Target momentum is zero; track cumulative deficit\n            momentum_error = -dtheta  # Want dtheta -> 0\n            self.integral_momentum += momentum_error * DT * stabilize_weight\n            self.integral_momentum = np.clip(self.integral_momentum, -0.5, 0.5)\n        else:\n            # Leaky decay when gates closed\n            self.integral_x *= 0.95\n            self.integral_momentum *= 0.9\n        \n        # Combined integral force\n        integral_force = self.K_i * integral_gate * self.integral_x\n        momentum_force = self.K_mom * integral_gate * self.integral_momentum * np.sign(theta + 1e-8)\n        \n        force = force + integral_force + momentum_force\n        \n        # === TRANSITION SMOOTHING: Sliding surface inspired damping ===\n        # In transition zone, add extra damping proportional to sliding variable\n        if 0.15 < abs(theta) < 0.6:\n            # Sliding surface: s = dtheta + lambda * theta\n            lambda_slide = 2.0 * self.omega_n\n            s = dtheta + lambda_slide * theta\n            \n            # Smooth reaching law\n            transition_damp = -0.5 * np.tanh(2.0 * s)\n            force = force + transition_damp\n        \n        # Near-equilibrium cart damping\n        if abs(theta) < 0.12:\n            cart_damp = -0.1 * dx * (0.12 - abs(theta)) / 0.12\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4967.97\nscore: 4967.97; base_score: 393.85; time_bonus: 2281.45; energy_bonus: 2492.67; success_bonus: 800.00; stabilization_time: 185; stabilization_ratio: 0.18; avg_energy_per_step: 0.01; total_energy: 6.55; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.01\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive Integral LQR combining best elements:\n    - Velocity-gated integral action from top performer (prevents windup)\n    - Gentler gain scheduling from crossover inspiration (smoother control)\n    - Proven Q[3]=3.2 and K_i=0.8 parameters\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.8\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with velocity-gated integral action\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi]\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Gentler gain scheduling (from crossover inspiration - proven more effective)\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        adaptive_gain = pos_gain * vel_gain\n        force = base_force * adaptive_gain\n\n        # Swing-up assist for large angles\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Velocity-gated integral action (from top performer - prevents windup during fast transients)\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4954.47\nscore: 4954.47; base_score: 393.80; time_bonus: 2267.91; energy_bonus: 2492.75; success_bonus: 800.00; stabilization_time: 187; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.51; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.04\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Adaptive LQR Controller with Velocity-Aware Gain Scheduling\n    Combines physically accurate friction modeling with intelligent gain adaptation\n    for ultra-fast stabilization while maintaining energy efficiency.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Optimized LQR weights: proven configuration from best performer (4920.14)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])  # Restored best-performing weights\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR control with optimized gain scheduling\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi] with numerical safety\n        theta = ((theta + np.pi + 1e-8) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Optimized gain scheduling: more aggressive for large errors\n        # Position-based gain: sharp response beyond 0.6 rad\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n\n        # Velocity-based gain: strong damping for high angular velocity\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain (simpler and more effective)\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Additive swing-up assist for large angles (>0.8 rad)\n        # Uses continuous falling-severity factor for smoother, more effective correction\n        if abs(theta) > 0.8:\n            # Physics-informed assist: push cart in direction to catch falling pole\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            # Continuous falling severity: scales from 1.0 to 2.0 based on how rapidly pole is falling\n            falling_severity = 1.0 + np.tanh(3.0 * theta * dtheta)\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity\n            force = force + u_swing\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4920.22\nscore: 4920.22; base_score: 393.53; time_bonus: 2233.79; energy_bonus: 2492.90; success_bonus: 800.00; stabilization_time: 192; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.44; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.03\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Advanced Adaptive LQR with Dual-Gated Integral Control and Energy-Based Swing-Up Assist\n    Combines precision LQR control with intelligent nonlinear enhancements for superior\n    performance on extreme inverted pendulum parameters.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Reference energy for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM\n\n        # Prediction horizon for proactive control\n        self.prediction_dt = DT\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Advanced adaptive LQR with enhanced integral control and energy-based assist\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Proven optimal gain scheduling\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Enhanced energy-based swing-up assist\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Calculate current mechanical energy\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n\n            # Normalized falling severity\n            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale\n            force += u_swing\n\n        # Predictive momentum compensation during mid-swing transition\n        if 0.3 < abs(theta) < 0.8:\n            # Estimate near-future angular dynamics\n            sin_theta = np.sin(theta)\n            cos_theta = np.cos(theta)\n            Mtot = M_CART + M_POLE\n            denom0 = L_COM * (4.0/3.0 - M_POLE / Mtot)\n\n            # Simplified acceleration estimate (assuming zero input)\n            temp = (M_POLE * L_COM * dtheta**2 * sin_theta) / Mtot\n            theta_acc_pred = (G * sin_theta - cos_theta * temp) / denom0\n\n            # Predict future angle\n            theta_future = theta + dtheta * self.prediction_dt + 0.5 * theta_acc_pred * self.prediction_dt**2\n\n            # Compute predictive momentum term\n            predicted_momentum = theta_future * dtheta\n            norm_predicted_momentum = predicted_momentum / (self.omega_n * L_COM)\n\n            # Proactive correction to counteract predicted instability\n            predictive_correction = -0.15 * norm_predicted_momentum\n            force += predictive_correction\n\n        # Dual-gated integral action for cart position\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        gate = angle_gate * velocity_gate\n\n        if gate > 0.1:\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Context-aware cart velocity damping\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4947.47\nscore: 4947.47; base_score: 393.59; time_bonus: 2261.12; energy_bonus: 2492.77; success_bonus: 800.00; stabilization_time: 188; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.51; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.04\n\nHere are the performance metrics of a set of prioviously implemented programs:\n\n# Prior programs\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Energy-Aware Adaptive LQR with Enhanced Integral Control\n    Combines LQR with cross-coupled terms, energy-aware swing-up assistance,\n    and advanced integral control for optimal stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - matches simulator physics\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Enhanced Q-matrix with cross-coupled terms for better coordination\n        Q = np.zeros((4, 4))\n        Q[0, 0] = 4.5   # x weight\n        Q[1, 1] = 44.0  # theta weight\n        Q[2, 2] = 0.6   # dx weight\n        Q[3, 3] = 3.2   # dtheta weight\n        # Cross-coupled terms to coordinate cart-pole motion\n        Q[0, 2] = Q[2, 0] = 0.12  # x-dx coupling\n        Q[1, 3] = Q[3, 1] = 0.8   # theta-dtheta coupling\n\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Precompute natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Energy reference for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM  # Potential energy at upright position\n\n        # Prediction horizon for proactive control (100ms)\n        self.prediction_dt = 0.1\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Increased for faster cart centering\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Energy-aware adaptive LQR control with enhanced integral control\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Enhanced gain scheduling with state-dependent saturation\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        # State-dependent gain saturation to prevent overcorrection near equilibrium\n        if abs(theta) < 0.3 and abs(dtheta) < 0.5:\n            adaptive_gain = min(adaptive_gain, 1.8)\n\n        force = base_force * adaptive_gain\n\n        # Enhanced energy-aware swing-up assist for large angles (>0.8 rad)\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n\n            # Compute current mechanical energy of pendulum\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor: scale assist by how much energy is missing\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n\n            # Predictive falling severity using forecasted angular momentum\n            # Estimate future theta after prediction_dt assuming constant acceleration\n            sin_theta = np.sin(theta)\n            cos_theta = np.cos(theta)\n\n            # Simplified acceleration estimate for prediction\n            temp = (0 + FRICTION_CART * dx + M_POLE * L_COM * dtheta**2 * sin_theta) / (M_CART + M_POLE)\n            theta_acc_pred = (G * sin_theta - cos_theta * temp + FRICTION_JOINT * dtheta / (M_POLE * L_COM)) / \\\n                           (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / (M_CART + M_POLE)))\n\n            # Forward predict theta\n            dt_p = self.prediction_dt\n            theta_future = theta + dtheta * dt_p + 0.5 * theta_acc_pred * dt_p**2\n\n            # Use predicted angular momentum for more proactive control\n            predicted_momentum = (theta_future * dtheta)\n            norm_momentum = predicted_momentum / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n\n            # Predictive momentum compensation term - anticipates need for stronger correction\n            # Based on rate of change of predicted momentum\n            d_momentum_pred = (theta_future * theta_acc_pred + dtheta**2)\n            momentum_rate_correction = np.tanh(0.5 * d_momentum_pred / (self.omega_n * L_COM))\n\n            # Enhanced additive assist with predictive compensation\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale * (1.0 + 0.2 * momentum_rate_correction)\n            force += u_swing\n\n        # Enhanced integral action with adaptive decay\n        # Tightened velocity gating thresholds for integral activation\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(10.0 * (0.8 - abs(dtheta)))  # Tightened threshold\n        gate = angle_gate * velocity_gate\n\n        if gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            # Enhanced angle-dependent integral decay\n            decay_factor = 0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1))\n            self.integral_x *= decay_factor\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Enhanced cart velocity damping when near equilibrium\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4953.83\nscore: 4953.83; base_score: 393.28; time_bonus: 2267.91; energy_bonus: 2492.64; success_bonus: 800.00; stabilization_time: 187; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.57; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Advanced Adaptive LQR with Dual-Gated Integral Control and Energy-Based Swing-Up Assist\n    Combines precision LQR control with intelligent nonlinear enhancements for superior\n    performance on extreme inverted pendulum parameters.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.85  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n        \n        # Reference energy for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Advanced adaptive LQR with enhanced integral control and energy-based assist\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Robust angle normalization using arctan2\n        theta = np.arctan2(np.sin(theta), np.cos(theta))\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Proven optimal gain scheduling\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Enhanced energy-based swing-up assist\n        if abs(theta) > 0.8:\n            # Smooth activation based on angle deviation\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            \n            # Calculate current mechanical energy\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n            \n            # Energy deficit factor\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            energy_scale = 1.0 + 0.5 * energy_deficit\n            \n            # Normalized falling severity\n            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)\n            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)\n            \n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity * energy_scale\n            force += u_swing\n\n        # Dual-gated integral action for cart position\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        gate = angle_gate * velocity_gate\n        \n        if gate > 0.1:\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * gate * self.integral_x\n        force = force + integral_force\n\n        # Context-aware cart velocity damping\n        if abs(theta) < 0.15:\n            cart_damp = -0.08 * dx * (0.15 - abs(theta)) / 0.15\n            force = force + cart_damp\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4947.50\nscore: 4947.50; base_score: 393.62; time_bonus: 2261.12; energy_bonus: 2492.77; success_bonus: 800.00; stabilization_time: 188; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.51; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.04\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Enhanced Adaptive LQR with Predictive Damping and Energy-Aware Integral Control\n    Combines best elements with two key innovations:\n    - Predictive angular acceleration estimation for proactive control\n    - Adaptive integral gain scaling based on energy deficit\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.8\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n        # Reference energy for swing-up scaling\n        self.E_ref = M_POLE * G * L_COM\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Enhanced adaptive LQR with predictive damping and energy-aware integral control\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi]\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Gentler gain scheduling (from crossover inspiration - proven more effective)\n        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))\n\n        adaptive_gain = pos_gain * vel_gain\n        force = base_force * adaptive_gain\n\n        # Swing-up assist for large angles\n        if abs(theta) > 0.8:\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * self.omega_n))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Predictive damping in mid-swing phase\n        if 0.3 < abs(theta) < 0.8:\n            # Estimate angular acceleration\n            sin_theta = np.sin(theta)\n            cos_theta = np.cos(theta)\n            Mtot = M_CART + M_POLE\n            denom = L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / Mtot)\n\n            temp = (M_POLE * L_COM * dtheta**2 * sin_theta) / Mtot\n            theta_acc_est = (G * sin_theta - cos_theta * temp) / denom\n\n            # Predictive correction term\n            predictive_term = -0.15 * theta_acc_est * np.sign(theta)\n            force = force + predictive_term\n\n        # Velocity-gated integral action with energy-aware gain scaling\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:\n            self.integral_x += x * DT\n\n            # Calculate current mechanical energy\n            E_kinetic = 0.5 * M_POLE * (dtheta * L_COM)**2\n            E_potential = M_POLE * G * L_COM * (1.0 - np.cos(theta))\n            E_current = E_kinetic + E_potential\n\n            # Energy deficit factor for adaptive integral gain\n            energy_deficit = max(0.0, 1.0 - E_current / self.E_ref)\n            adaptive_Ki = self.K_i * (1.0 + 0.3 * energy_deficit)\n        else:\n            self.integral_x *= 0.95\n            adaptive_Ki = self.K_i\n\n        integral_force = adaptive_Ki * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nPerformance metrics:\nCombined score to maximize: 4934.14\nscore: 4934.14; base_score: 393.76; time_bonus: 2247.49; energy_bonus: 2492.89; success_bonus: 800.00; stabilization_time: 190; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.44; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.01; stable_theta_error: 0.00; stable_x_error: 0.05\n\n\n# Current program\n\nHere is the current program we are trying to improve (you will need to \npropose a new program with the same inputs and outputs as the original \nprogram, but with improved internal implementation):\n\n```python\nimport numpy as np\n\n# --- Physics Constants ---\nM_CART = 1.0       # Mass of the cart (kg)\nM_POLE = 0.35      # Mass of the pole (kg) - \u66f4\u91cd\uff0c\u5927\u5e45\u589e\u52a0\u63a7\u5236\u96be\u5ea6\nL_POLE = 2.5       # Total length of the pole (m) - \u66f4\u957f\uff0c\u6781\u4e0d\u7a33\u5b9a\nL_COM = L_POLE / 2 # Length to center of mass (m)\nG = 9.81           # Gravity (m/s^2)\nFRICTION_CART = 0.35 # Coefficient of friction for cart - \u9ad8\u6469\u64e6\uff0c\u66f4\u591a\u80fd\u91cf\u635f\u5931\nFRICTION_JOINT = 0.25 # Coefficient of friction for joint - \u9ad8\u5173\u8282\u6469\u64e6\nDT = 0.02          # Time step (s)\nMAX_STEPS = 1000   # 20 seconds simulation\n\ndef simulate_pendulum_step(state, force, dt):\n    \"\"\"\n    Simulates one time step of the Single Inverted Pendulum.\n\n    State vector: [x, theta, dx, dtheta]\n    - x: Cart position (m)\n    - theta: Pole angle (rad), 0 is upright\n    - dx: Cart velocity (m/s)\n    - dtheta: Pole angular velocity (rad/s)\n\n    Args:\n        state: numpy array of shape (4,)\n        force: scalar float, force applied to the cart (N)\n        dt: float, time step (s)\n\n    Returns:\n        next_state: numpy array of shape (4,)\n    \"\"\"\n    x, theta, dx, dtheta = state\n\n    # Precompute trig terms\n    sin_theta = np.sin(theta)\n    cos_theta = np.cos(theta)\n\n    # Equations of Motion (Non-linear)\n    # derived from Lagrangian dynamics\n\n    # Total mass\n    M_total = M_CART + M_POLE\n\n    # Friction forces\n    f_cart = -FRICTION_CART * dx\n    f_joint = -FRICTION_JOINT * dtheta\n\n    # Denominator for solving linear system of accelerations\n    # Derived from solving the system:\n    # 1) (M+m)x_dd + (ml cos)theta_dd = F + f_cart + ml*theta_d^2*sin\n    # 2) (ml cos)x_dd + (ml^2)theta_dd = mgl sin + f_joint\n\n    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total\n\n    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \\\n                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))\n\n    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total\n\n    # Euler integration\n    next_x = x + dx * dt\n    next_theta = theta + dtheta * dt\n    next_dx = dx + x_acc * dt\n    next_dtheta = dtheta + theta_acc * dt\n\n    return np.array([next_x, next_theta, next_dx, next_dtheta])\n\n\n# EVOLVE-BLOCK-START\nclass Controller:\n    \"\"\"\n    Hybrid Adaptive Integral LQR with Enhanced Damping and Robust Integral Control\n    Combines proven gain scheduling with sophisticated integral action and enhanced\n    angular velocity damping for superior stabilization performance.\n    \"\"\"\n\n    def __init__(self):\n        # System parameters\n        m = M_POLE\n        M = M_CART\n        l = L_COM\n        g = G\n        Mtot = M + m\n        denom0 = l * (4.0 / 3.0 - m / Mtot)\n        b_c = FRICTION_CART\n        b_j = FRICTION_JOINT\n\n        # Physically accurate linearized A matrix with corrected joint friction\n        A = np.zeros((4, 4))\n        A[0, 2] = 1.0\n        A[1, 3] = 1.0\n\n        # theta_acc row (3) - using corrected joint friction term matching simulator\n        A[3, 1] = g / denom0\n        A[3, 2] = b_c / (Mtot * denom0)\n        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0\n\n        # x_acc row (2)\n        A[2, 1] = -(m * l / Mtot) * A[3, 1]\n        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]\n        A[2, 3] = b_j / (Mtot * denom0)\n\n        # B matrix\n        B = np.zeros((4, 1))\n        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)\n        B[3, 0] = -1.0 / (Mtot * denom0)\n\n        # Proven optimal LQR weights (Q[3]=3.2 achieves best performance)\n        Q = np.diag([4.5, 44.0, 0.6, 3.2])\n        R = np.array([[1.0]])\n\n        # Solve LQR gains\n        self.K = self.solve_lqr(A, B, Q, R)\n\n        # Integral control parameters\n        self.integral_x = 0.0\n        self.K_i = 0.8  # Integral gain for cart position\n\n        # Natural frequency for normalized falling severity\n        self.omega_n = np.sqrt(G / L_COM)\n\n    def solve_lqr(self, A, B, Q, R):\n        \"\"\"Solve continuous-time LQR\"\"\"\n        from scipy.linalg import solve_continuous_are\n        P = solve_continuous_are(A, B, Q, R)\n        K = np.linalg.inv(R) @ B.T @ P\n        return K\n\n    def get_action(self, state):\n        \"\"\"Adaptive LQR with integral action and robust swing-up assist\"\"\"\n        x, theta, dx, dtheta = state\n\n        # Normalize angle to [-pi, pi]\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n\n        state_vec = np.array([x, theta, dx, dtheta])\n        base_force = -self.K @ state_vec\n\n        # Sharper gain scheduling with steeper transitions for crisper mode separation\n        pos_gain = 1.0 + 0.5 * np.tanh(7.0 * max(0.0, abs(theta) - 0.6))\n        vel_gain = 1.0 + 0.3 * np.tanh(6.0 * max(0.0, abs(dtheta) - 1.0))\n\n        # Combined multiplicative gain\n        adaptive_gain = pos_gain * vel_gain\n\n        force = base_force * adaptive_gain\n\n        # Enhanced swing-up assist with normalized falling severity\n        if abs(theta) > 0.8:\n            # Physics-informed assist: push cart in direction to catch falling pole\n            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))\n            # If theta*dtheta > 0, pole is falling further - need stronger correction\n            # Normalized by natural frequency of the pendulum for better scaling\n            natural_freq = np.sqrt(G / L_COM)\n            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * natural_freq))\n            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor\n            force = force + u_swing\n\n        # Soft-switched integral action for cart position with velocity gating\n        # Only activates when pole is nearly upright (<0.1 rad) AND moving slowly\n        # Velocity gate prevents windup during rapid transient passes through upright\n        angle_gate = np.tanh(12.0 * (0.1 - abs(theta)))\n        velocity_gate = np.tanh(8.0 * (1.0 - abs(dtheta)))\n        integral_gate = angle_gate * velocity_gate\n\n        if integral_gate > 0.1:  # Only update integral when gate is significant\n            self.integral_x += x * DT\n        else:\n            self.integral_x *= 0.95  # Leaky integration when inactive\n\n        integral_force = self.K_i * integral_gate * self.integral_x\n        force = force + integral_force\n\n        return float(force[0])\n\n# Initialize controller\ncontroller = Controller()\n\ndef get_control_action(state):\n    force = controller.get_action(state)\n    # Apply force limits\n    return float(np.clip(force, -100.0, 100.0))\n# EVOLVE-BLOCK-END\n\ndef run_simulation(seed=None):\n    \"\"\"\n    Runs the simulation loop.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initial state: 0.4 rad (~23 degrees)\n    # \u66f4\u5927\u521d\u59cb\u89d2\u5ea6\u914d\u5408\u66f4\u91cd\u66f4\u957f\u7684\u6746\u5b50\uff0c\u6781\u5177\u6311\u6218\u6027\n    state = np.array([0.0, 0.9, 0.0, 0.0])\n\n    states = [state]\n    forces = []\n\n    for _ in range(MAX_STEPS):\n        force = get_control_action(state)\n        # Clip force to realistic limits\n        force = np.clip(force, -100.0, 100.0)\n\n        next_state = simulate_pendulum_step(state, force, DT)\n\n        states.append(next_state)\n        forces.append(force)\n\n        state = next_state\n\n        # # Early termination checks\n        # if np.any(np.isnan(state)):\n        #     break\n        # # Fail fast if it falls over (> 1.0 rad, matching evaluate.py)\n        # if abs(state[1]) > 1.0:\n        #     break\n\n    return np.array(states), np.array(forces)\n```\n\nHere are the performance metrics of the program:\n\nCombined score to maximize: 4934.54\nscore: 4934.54; base_score: 394.49; time_bonus: 2247.49; energy_bonus: 2492.56; success_bonus: 800.00; stabilization_time: 190; stabilization_ratio: 0.19; avg_energy_per_step: 0.01; total_energy: 6.61; mean_step_reward: 0.39; final_theta_error: 0.00; final_x_error: 0.00; stable_theta_error: 0.00; stable_x_error: 0.03\n\n# Task\n\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs as the original program, but with improved internal implementation.\n"
              }
            ],
            "max_tokens": 4096,
            "temperature": 0.5
          },
          "input_tokens": 26178,
          "output_tokens": 2863,
          "cost": 0.202465,
          "input_cost": 0.0,
          "output_cost": 0.0,
          "thought": "",
          "model_posteriors": {
            "deepseek-reasoner": 0.028571428571428574,
            "claude-opus-4-5": 0.028571428571428574,
            "gemini/gemini-2.5-pro": 0.028571428571428574,
            "gemini-3-pro-preview": 0.8,
            "xai/grok-4-1-fast-reasoning": 0.028571428571428574,
            "glm-4.6": 0.028571428571428574,
            "dashscope/qwen3-coder-plus": 0.028571428571428574,
            "dashscope/qwen-plus-2025-07-28": 0.028571428571428574
          }
        },
        "diff_summary": {
          "added": 39,
          "deleted": 0,
          "modified": 77
        },
        "meta_recommendations": "1. **Enhance predictive momentum compensation with joint-friction-aware angular acceleration estimation**: Replace the current simplified `theta_acc_est` with a full friction-inclusive model that accounts for both cart and joint friction terms in the denominator, mirroring the precision of the A-matrix used in LQR design. This would align predictive correction with the high-fidelity dynamics already proven essential for zero final error under `FRICTION_JOINT=0.25`, potentially enabling even earlier divergence counteraction and shaving 2\u20133 steps off the 178-step stabilization time.\n\n2. **Introduce state-dependent mid-swing damping gain modulated by predicted energy deficit**: Scale the `K_d_midswing = 2.5` coefficient using the normalized mechanical energy: `K_d = 2.5 * (1.0 + 0.4 * max(0, 1 - E_current / E_max))`, where `E_max = M_POLE*G*L_COM`. This leverages the Gaussian damping\u2019s phase specificity while making its intensity responsive to swing-up progress\u2014applying stronger damping when the pole is under-energized and prone to stall, which could reduce overshoot variability across extreme initial conditions without compromising balancing precision.\n\n3. **Make cross-coupling boost adaptive to both angle and angular velocity magnitude**: Extend the current `cross_coupling_boost = 1.0 + 0.5 * tanh(3.0 * abs(theta))` to include velocity dependence: `boost = 1.0 + 0.5 * tanh(3.0 * abs(theta)) * (1.0 + 0.3 * tanh(2.0 * abs(dtheta)))`. Since high `dtheta` during mid-swing exacerbates coordination challenges, this dual-state adaptation would intensify theta-dtheta coupling precisely when momentum is highest, improving transient response during the critical 0.3\u20130.8 rad transition zone identified as pivotal for record time_bonus.\n\n4. **Implement exponential decay on the integral state only when outside both gating thresholds**, rather than applying decay whenever the gate product is low. Specifically, decay `self.integral_x *= 0.95` only if `abs(theta) > 0.12 OR abs(dtheta) > 1.1`, preserving accumulated correction during brief excursions beyond equilibrium. This refines the current dual-gating strategy to better handle high-inertia recovery transients, potentially reducing `stable_x_error` below 0.05 while maintaining windup protection\u2014addressing the last remaining precision gap among elite controllers.\n\n5. **Replace the fixed Gaussian center (0.5 rad) in mid-swing damping with an inertia-scaled peak location**: Set the activation center as `theta_peak = 0.5 * np.sqrt(L_COM / 2.5)` so the damping maximum adapts to the actual pole length, consistent with the `omega_n` normalization already used in predictive logic. Given that longer poles exhibit delayed momentum peaks, this ensures damping aligns with the true high-risk region of the swing-up trajectory, preventing premature or late intervention that could otherwise waste energy or permit overshoot under extreme `L_POLE=2.5` conditions.",
        "meta_summary": "**Program Name: Suboptimal LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with conservative LQR gains (low Q, high R) to stabilize a challenging inverted pendulum system with heavy/long pole and high friction; control actions are clipped to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 3195.17, with strong energy efficiency but slow stabilization (459 steps, 46% ratio).  \n- **Feedback**: The overly conservative LQR tuning results in sluggish response and poor error correction early on, limiting time and precision bonuses despite eventual stability and low energy use.\n**Program Identifier:** Generation 0 - Patch Name initial_program - Correct Program: False\n\n**Program Name: Suboptimal LQR with Nonlinear Compensation**  \n- **Implementation**: Uses a linearized LQR controller with aggressive Q/R weights combined with gravity and damping compensation terms to stabilize a highly unstable inverted pendulum (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 0.00 due to large final position/angle errors despite running all 1000 steps.  \n- **Feedback**: The controller fails to stabilize the system\u2014final cart position diverges drastically (>4000 m) and pole angle remains far from upright (~2.94 rad), indicating poor control authority or incorrect linearization assumptions under large deviations.\n**Program Identifier:** Generation 1 - Patch Name tune_lqr_nonlinear_comp - Correct Program: False\n\n**Program Name: Sliding Mode Energy Controller**\n\n- **Implementation**: Combines energy-based swing-up with sliding mode stabilization, featuring adaptive gain scheduling, friction compensation, and smooth sign approximation to reduce chattering; uses hybrid control modes based on pole angle magnitude.\n\n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilizing the pendulum (final theta error = 2.52 rad), despite moderate energy efficiency (avg_energy_per_step = 0.35) and full episode length (stabilization_time = 1000).\n\n- **Feedback**: The controller fails to stabilize the pendulum within the required bounds (|theta| \u2264 1.0 rad), indicating insufficient robustness or incorrect tuning for the highly challenging system parameters (heavy, long pole with high friction); energy shaping may dominate over stabilization near upright, preventing convergence.\n**Program Identifier:** Generation 2 - Patch Name sliding_mode_energy_shaping_hybrid - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized continuous-time LQR controller with aggressive Q/R weights on a highly unstable pendulum (long, heavy pole; high friction), but applies it directly to the nonlinear system without accounting for large-angle deviations or proper state wrapping in dynamics.\n- **Performance**: Achieves full 1000-step simulation (stabilization_ratio=1.0) but with very poor control accuracy (final_theta_error=3.11 rad, final_x_error=4433 m) and low combined score (0.00).\n- **Feedback**: Despite running to completion, the controller fails to stabilize the pendulum near upright due to mismatch between linear LQR assumptions and highly nonlinear, challenging dynamics; large steady-state errors indicate inadequate gain tuning or linearization validity.\n**Program Identifier:** Generation 3 - Patch Name correct_linearization_tune_weights - Correct Program: False\n\n**Program Name: Hybrid Energy-LQR Inverted Pendulum Controller**  \n- **Implementation**: Combines energy-based swing-up control for large angles with a nonlinear LQR stabilizer using state-dependent gain scheduling; includes friction modeling and Euler integration.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum within limits (final theta error: 2.52 rad, exceeded 1.0 rad threshold).  \n- **Feedback**: Despite sophisticated hybrid control design, the controller fails to recover from the aggressive initial condition (0.9 rad) given the heavy, long pole and high friction; likely issues include inaccurate linearization in LQR design and insufficient robustness in transition logic.\n**Program Identifier:** Generation 4 - Patch Name nonlinear_lqr_with_energy_shaping - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized LQR controller with hand-tuned Q and R matrices on a highly unstable inverted pendulum (long, heavy pole; high friction); applies Euler integration for dynamics and clips control force to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 0.00 due to poor stabilization\u2014final angle error of 2.95 rad and position error over 4000 m.  \n- **Feedback**: Despite aggressive LQR tuning (high angle weight, low control penalty), the controller fails to stabilize the system, likely due to severe nonlinearity from large initial angle (0.9 rad) and model mismatch between linear LQR and highly nonlinear dynamics.\n**Program Identifier:** Generation 5 - Patch Name aggressive_lqr_tuning - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses two LQR controllers (conservative for large angles, aggressive near upright) with linearized dynamics around the upright equilibrium; applies state feedback with angle wrapping and force clipping.  \n- **Performance**: Scored 0.00 due to failure to stabilize, despite full simulation steps (stabilization_ratio=1.00), with high final position error (4357.35 m) and pole angle error (2.91 rad).  \n- **Feedback**: The controller fails to handle the highly unstable system (long, heavy pole; high friction); aggressive gains activate too late, and conservative gains are too weak to recover from large initial angle (0.9 rad). Linearization may be invalid for such large deviations.\n**Program Identifier:** Generation 6 - Patch Name accurate_linearization_and_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses three gain-scheduled LQR controllers selected based on angle and angular velocity magnitude, with state normalization and energy-based force scaling near equilibrium.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization (final theta error: 2.82 rad, cart position diverged to 4340 m).  \n- **Feedback**: Despite adaptive gains and proper linearization, the controller failed to stabilize the highly unstable system (long, heavy pole with high friction), suggesting insufficient robustness or incorrect dynamics modeling in the control law.\n**Program Identifier:** Generation 7 - Patch Name adaptive_lqr_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses two LQR controllers (recovery and balance modes) blended via a sigmoid-like function based on pole angle; control gains are scheduled smoothly to handle large initial deviations.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum within limits, despite high base score (14.54), indicating simulation ran full duration but ended far from upright.  \n- **Feedback**: The controller failed to correct large initial angle (0.9 rad) under challenging dynamics (heavy, long pole); final theta error (2.76 rad) and massive cart drift (x \u2248 3318 m) suggest instability or incorrect linearization assumptions in LQR design.\n**Program Identifier:** Generation 8 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a suboptimal adaptive LQR controller that switches between conservative and aggressive gain matrices based on pole angle; implemented with Euler integration and nonlinear dynamics simulation.  \n- **Performance**: Achieved a combined score of 0.00 due to high final position error (4262.33 m) and angle error (3.06 rad), despite running all 1000 steps.  \n- **Feedback**: The controller fails to stabilize the cart-pole system under challenging conditions (long, heavy pole; high friction); poor state regulation suggests inadequate gain tuning or linearization mismatch in the highly nonlinear regime.\n**Program Identifier:** Generation 9 - Patch Name adaptive_lqr_gains_near_equilibrium - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**\n\n- **Implementation**: Uses gain-scheduled LQR control blending aggressive and precision gains based on pole angle magnitude, augmented with integral action for friction compensation and additional damping terms for high velocities. Includes anti-windup, friction compensation, and smooth saturation.\n\n- **Performance**: Achieved a combined score of 0.00 despite moderate base_score (9.47) and time_bonus (1.01), due to complete failure in stabilization (final_theta_error: 3.03 rad, final_x_error: 4425 m).\n\n- **Feedback**: The controller fails to stabilize the pendulum under challenging conditions (heavy, long pole; high friction; large initial angle). Integral action and adaptive gains are insufficient to overcome model inaccuracies and aggressive dynamics, leading to divergence rather than recovery.\n**Program Identifier:** Generation 10 - Patch Name adaptive_gain_lqr_with_integral - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized model with friction-aware dynamics to compute an LQR controller; state feedback gain is derived from continuous-time algebraic Riccati equation with modest Q/R weights favoring low control effort over fast stabilization.  \n- **Performance**: Achieved a combined score of 3252.50, with strong energy efficiency (avg_energy_per_step: 0.01) but moderate stabilization ratio (0.44) and time bonus due to slow response.  \n- **Feedback**: The conservative LQR tuning successfully stabilized the challenging high-friction, long/heavy-pole system but sacrificed speed and precision\u2014evidenced by low base and time scores despite perfect final error and high success bonus.\n**Program Identifier:** Generation 11 - Patch Name correct_linear_model_friction_tune_gains - Correct Program: True\n\n**Program Name: High-Performance LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized model of an inverted pendulum with aggressive LQR cost weights (`Q=[15,35,2,4]`, `R=0.8`) and includes angle normalization; control forces are clipped to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: The LQR controller is based on a linear approximation that likely fails to capture the highly nonlinear dynamics of the heavy, long pole; early termination conditions (e.g., angle >1.0 rad) were commented out, possibly allowing unstable behavior to continue unchecked.\n**Program Identifier:** Generation 12 - Patch Name lqr_optimized_structural - Correct Program: False\n\n**Program Name: Aggressive LQR with Gain Scheduling**  \n- **Implementation**: Uses a state-dependent blend between aggressive and conservative LQR controllers based on pole angle magnitude, with high Q weights for tight angle control and lower R to allow stronger forces; designed for a heavy, long pole with high friction.  \n- **Performance**: Combined score: 0.0 \u2014 fails to stabilize the pendulum under validation tests.  \n- **Feedback**: Despite sophisticated gain scheduling and tuning for challenging dynamics, the controller fails to handle large initial angles or nonlinearities beyond the linearized model\u2019s validity; early termination due to instability suggests insufficient robustness in recovery phase.\n**Program Identifier:** Generation 13 - Patch Name aggressive_lqr_with_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses an extended state-space model with integral action for steady-state error reduction and adaptive gain scheduling based on pole angle to handle nonlinearities. The controller computes LQR gains offline using continuous-time Riccati equation and applies angle normalization and anti-windup logic for the integrator.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated control design, the linearized model used for LQR does not adequately capture the highly nonlinear dynamics of the heavy, long pole under high friction; additionally, the simulation lacks early termination on failure (e.g., pole angle >1.0 rad), leading to invalid trajectories that likely caused evaluation failures.\n**Program Identifier:** Generation 14 - Patch Name adaptive_lqr_with_integral_action - Correct Program: False\n\n**Program Name: Gain-Scheduled LQR Controller**  \n- **Implementation**: Uses a gain-scheduled LQR controller with low-gain (robust) and high-gain (precise) modes blended based on pole angle magnitude; system linearized around upright equilibrium but applied to highly nonlinear, challenging dynamics (long/heavy pole, high friction).  \n- **Performance**: Achieved only 10.40 base score with poor stabilization (final theta error 2.87 rad, large cart drift), indicating failure to control the pendulum despite full simulation duration.  \n- **Feedback**: The conservative LQR tuning (low Q, high R) and reliance on linearization in a highly nonlinear regime caused sluggish, inaccurate responses; controller failed to stabilize even within generous time limits, suggesting need for more aggressive gains or nonlinear control strategies.\n**Program Identifier:** Generation 15 - Patch Name gainscheduled_lqr - Correct Program: False\n\n**Program Name: Friction-Aware LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with friction terms in the A matrix and optimized LQR weights; includes angle normalization and semi-implicit state estimation for improved feedback.  \n- **Performance**: Achieved a combined score of 0.00 despite high base_score (9.85), failing to stabilize the pendulum within limits (final theta error: 2.82 rad, cart position diverged).  \n- **Feedback**: The controller failed to handle the highly unstable system (long, heavy pole with high friction); aggressive LQR tuning was insufficient without proper nonlinear handling or robustness to large initial angles.\n**Program Identifier:** Generation 16 - Patch Name friction_aware_lqr_optimized - Correct Program: False\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses a linearized state-space model with friction approximations to compute LQR gains, augmented with adaptive gain scheduling that increases control effort for large pole angles (>0.6 rad). Control forces are clipped to \u00b1100 N for physical realism.  \n- **Performance**: Achieved a high combined score of 3381.03, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800), stabilizing the pendulum in 422 steps.  \n- **Feedback**: The adaptive gain scheduling effectively handled the challenging initial condition (0.9 rad) and high-friction dynamics, enabling rapid stabilization despite the heavy, long pole. The controller maintained near-zero final errors, demonstrating robustness and precision.\n**Program Identifier:** Generation 17 - Patch Name advanced_friction_compensated_lqr - Correct Program: True\n\n**Program Name: Enhanced LQR with Friction Compensation**  \n- **Implementation**: Uses a linearized state-space model with friction terms to compute an LQR gain matrix, augmented with velocity-based gain scheduling and anti-windup force clipping.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilizing the pendulum within acceptable bounds despite high energy use.  \n- **Feedback**: The controller failed to stabilize the pole (final theta error = 3.04 rad), likely due to inaccurate linearization for large angles and insufficient robustness to the system's high nonlinearity, mass, and length.\n**Program Identifier:** Generation 18 - Patch Name none - Correct Program: False\n\n**Program Name: Suboptimal LQR with Friction Compensation**  \n- **Implementation**: Uses a linearized LQR controller with conservative Q/R weights and adds heuristic friction compensation for both cart and joint; state is wrapped to [\u2212\u03c0, \u03c0].  \n- **Performance**: Achieved a combined score of 3373.21, with moderate time and energy bonuses but low base accuracy due to high error tolerance.  \n- **Feedback**: The overly conservative LQR gains cause sluggish response and poor stabilization (only 42% stable steps), while friction compensation helps marginally but cannot overcome the weak control authority; evolution potential remains high.\n**Program Identifier:** Generation 19 - Patch Name nonlinear_friction_compensation_lqr - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware model to compute an LQR controller with deliberately conservative gains (low Q, high R), resulting in sluggish but stable control under challenging physics (long/heavy pole, high friction).  \n- **Performance**: Achieved a combined score of 3945.50, with strong energy efficiency and full success bonus despite slow stabilization.  \n- **Feedback**: The controller successfully stabilizes the pendulum but does so slowly (stabilization ratio 0.32), reflecting the trade-off from tuning for low control effort over responsiveness; this validates the design intent of a \"lazy\" yet functional baseline for evolutionary improvement.\n**Program Identifier:** Generation 20 - Patch Name friction_aware_lqr_model - Correct Program: True\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with LQR control, but the linearization inaccurately models friction and ignores nonlinear dynamics; Q/R weights prioritize angle over position yet are mismatched to the highly unstable physical parameters (long/heavy pole, high friction).  \n- **Performance**: Achieved near-zero combined score due to catastrophic failure in stabilization\u2014final theta error of 2.76 rad and cart drift exceeding 4000 m.  \n- **Feedback**: The controller fails because the linearized model is invalid for large initial angles (0.9 rad) and aggressive system dynamics; joint friction was incorrectly incorporated into the A matrix instead of being treated as a disturbance or nonlinear term, leading to poor real-world approximation and instability.\n**Program Identifier:** Generation 21 - Patch Name refine_lqr_model_with_physical_consistency - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware model to compute an LQR controller with deliberately conservative gains (low Q, high R) for stabilizing a challenging inverted pendulum (long, heavy pole; high friction). The controller applies clipped forces based on full-state feedback with angle normalization.\n\n- **Performance**: Achieved a combined score of 3933.91, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate time and stabilization performance (stabilization_ratio: 0.32).\n\n- **Feedback**: The conservative LQR tuning prioritizes low control effort over fast response, resulting in slow stabilization and lower time/stabilization scores despite perfect final error and high energy efficiency\u2014indicating room for improvement via gain optimization or nonlinear control.\n**Program Identifier:** Generation 22 - Patch Name fix_joint_friction_in_a_matrix - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling based on pole angle magnitude and additional angular velocity damping for small angles.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: Despite adaptive gains and damping, the controller is too conservative (high R, low Q) for the challenging environment (long/heavy pole, high friction), leading to instability and early failure; the initial angle of 0.9 rad may exceed its recovery capability.\n**Program Identifier:** Generation 23 - Patch Name smooth_adaptive_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a hybrid adaptive LQR controller with precise linearization of the nonlinear pendulum dynamics and gain scheduling based on pole angle and velocity magnitude. The controller boosts control gains when the pole deviates significantly from upright or when system velocity is high.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite accurate modeling and aggressive gain tuning, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction). Likely issues include inadequate handling of nonlinearities beyond small-angle approximations and possible instability from gain scheduling logic interacting poorly with the clipped control force.\n**Program Identifier:** Generation 24 - Patch Name adaptive_friction_compensated_lqr - Correct Program: False\n\n**Program Name: Friction-Compensated LQR Controller**  \n- **Implementation**: Uses a linearized state-space model with corrected friction terms and smooth gain scheduling based on pole angle; applies LQR control with optimized weights and clips force output.  \n- **Performance**: Achieved base score of 14.15 but failed to stabilize (final theta error = 2.62 rad, large position drift), resulting in zero success bonus and low combined score (0.00).  \n- **Feedback**: Despite improved modeling (e.g., correct joint friction term) and gain scheduling, the controller cannot stabilize the highly unstable system (heavy, long pole with high friction); poor handling of nonlinearities and insufficient robustness lead to failure.\n**Program Identifier:** Generation 25 - Patch Name smooth_adaptive_gains_with_corrected_friction_model - Correct Program: False\n\n**Program Name: Suboptimal LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware dynamics model to compute LQR gains with low state penalties (Q) and high control cost (R), plus gain scheduling based on pole angle.  \n- **Performance**: Achieved a combined score of 4431.51 with moderate time and energy bonuses but slow stabilization (259 steps, 26% ratio).  \n- **Feedback**: The conservative LQR tuning prioritizes low energy use over fast response, resulting in delayed stabilization; gain scheduling helps recover from large angles but doesn't fully compensate for the intentionally sluggish baseline policy.\n**Program Identifier:** Generation 26 - Patch Name tune_lqr_weights_angular_priority - Correct Program: True\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses an LQR controller with linearized dynamics including friction approximations, enhanced with nonlinear friction compensation via tanh-smoothed sign functions and adaptive gain scheduling for large angle deviations.  \n- **Performance**: Achieved a high combined score of 3447.47, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus due to stable final state.  \n- **Feedback**: The friction compensation and gain scheduling effectively stabilized a challenging high-friction, long-pole system; early termination was avoided by maintaining stability beyond 400 steps despite aggressive initial conditions.\n**Program Identifier:** Generation 27 - Patch Name nonlinear_friction_compensation - Correct Program: True\n\n**Program Name: Suboptimal LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains with intentionally conservative Q/R weights; applies gain scheduling based on pole angle.  \n- **Performance**: Achieves low initial score (~3000) due to sluggish response and high error tolerance, failing validation tests.  \n- **Feedback**: The controller\u2019s \"lazy\" tuning (low state penalties, high control cost) and lack of robustness to the challenging physics (long/heavy pole, high friction) prevent effective stabilization; early termination logic is commented out, risking wasted computation after failure.\n**Program Identifier:** Generation 28 - Patch Name switch_to_discrete_lqr - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model with corrected friction terms to compute LQR gains, enhanced by velocity- and angle-aware adaptive gain scheduling for aggressive stabilization.  \n- **Performance**: Achieved a high combined score of 4491.63 with fast stabilization (251 steps), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The adaptive gain scheduling significantly improved control authority during large deviations while maintaining efficiency near equilibrium, enabling robust handling of the challenging heavy/long pole configuration.\n**Program Identifier:** Generation 29 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Swing-Up Assistance**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with gain scheduling and energy-based swing-up logic for large angles; state feedback is blended during transition from swing-up to stabilization.  \n- **Performance**: Achieves low initial score (~3000) due to conservative LQR weights and challenging physics (long/heavy pole, high friction).  \n- **Feedback**: The controller stabilizes the pendulum but responds slowly and tolerates large errors; evaluation indicates room for improvement in both control aggressiveness and handling of extreme initial conditions.\n**Program Identifier:** Generation 30 - Patch Name enhanced_lqr_with_swing_up_trigger - Correct Program: False\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses a linearized state-space model at equilibrium to compute LQR gains, augmented with nonlinear friction compensation and smooth gain scheduling based on pole angle magnitude. However, the `Controller.__init__` references undefined variable `cos_theta` when constructing matrix `A`, causing runtime errors.  \n- **Performance**: Combined score: 0.0 \u2014 fails validation due to incorrect dynamics linearization and implementation bugs.  \n- **Feedback**: The controller logic is conceptually sound but flawed in execution; the undefined `cos_theta` breaks initialization, and the linearized `A` and `B` matrices do not correctly reflect the true system dynamics around \u03b8=0, undermining stability guarantees.\n**Program Identifier:** Generation 31 - Patch Name refine_lqr_gain_scheduling - Correct Program: False\n\n**Program Name: Hybrid Energy-LQR with Friction Compensation**  \n- **Implementation**: Combines energy-based control for large pole angles and LQR for small angles, with adaptive friction compensation and smooth blending between modes. Uses nonlinear dynamics simulation with Euler integration and high-friction-aware linearization for LQR design.  \n- **Performance**: Scored 0.00 due to failure to stabilize; achieved full 1000 steps but with large final errors (theta: 1.24 rad, x: 85.26 m) and no success bonus.  \n- **Feedback**: Despite sophisticated hybrid control and friction handling, the controller failed to stabilize the pendulum within acceptable bounds, indicating insufficient robustness for the extreme parameters (heavy, long pole, high friction). The system remained unstable throughout, suggesting poor tuning or inadequate energy management during transitions.\n**Program Identifier:** Generation 32 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with velocity- and angle-based adaptive gain scheduling to handle large initial deviations and high system instability from a heavy, long pole and high friction.  \n- **Performance**: Achieved a high combined score of 4469.91 with fast stabilization (254 steps), low energy use (total_energy: 6.18), and full success bonus.  \n- **Feedback**: The adaptive gains significantly improved robustness and speed of stabilization under challenging dynamics, while the precise friction modeling in the LQR design contributed to energy efficiency and stable final error near zero.\n**Program Identifier:** Generation 33 - Patch Name None - Correct Program: True\n\n**Program Name: Nonlinear Adaptive Energy-Shaping Controller**  \n- **Implementation**: Uses a composite control law combining nonlinear PD terms for angle/position, energy-shaping feedback, adaptive gain scheduling, and friction compensation to stabilize a challenging inverted pendulum (heavy, long pole; high friction).  \n- **Performance**: Achieved a combined score of 0.00 despite full simulation duration (1000 steps), with poor final stabilization (final_theta_error=1.44 rad, final_x_error=31.17 m) and no success bonus.  \n- **Feedback**: The controller fails to stabilize the pendulum upright\u2014likely due to aggressive energy shaping or insufficient coupling between cart position and pole dynamics\u2014resulting in divergence rather than recovery from large initial angles.\n**Program Identifier:** Generation 34 - Patch Name nonlinear_adaptive_controller - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must include functional code to address the problem requirements.\n**Program Identifier:** Generation 35 - Patch Name None - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform its intended task; implementation must include functional code to address the problem requirements.\n**Program Identifier:** Generation 36 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physics-matched continuous-time LQR controller with smooth, differentiable gain scheduling via tanh-based adaptive gains on position and velocity errors. The linearized dynamics matrix precisely aligns with the nonlinear simulator\u2019s structure, including consistent friction and denominator terms.  \n- **Performance**: Achieved a high combined score of 4878.27, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 198), and full success bonus.  \n- **Feedback**: The precise alignment between the controller\u2019s linear model and the simulator\u2019s physics enabled robust control despite challenging parameters (heavy/long pole, high friction). Adaptive gain scheduling improved transient response without introducing discontinuities, contributing to both stability and low energy use.\n**Program Identifier:** Generation 37 - Patch Name adaptive_lqr_friction_aware - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, incorporating friction terms matching the simulator, and applies smooth gain scheduling via tanh-based adaptation on angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4831.64 with fast stabilization (205 steps), low energy use (total_energy=6.26), and full success bonus.  \n- **Feedback**: The adaptive gains significantly improved robustness to large initial angles and high friction, enabling stable control of an otherwise highly unstable system; precise friction modeling in the LQR design was key to performance.\n**Program Identifier:** Generation 38 - Patch Name corrected_friction_and_aggressive_tuning - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model matching the simulator\u2019s physics to compute LQR gains, augmented with smooth tanh-based gain scheduling for large-angle recovery and swing-up logic when |\u03b8| > 0.85 rad.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization (final \u03b8 error = 1.59 rad, x error = 1408 m), despite full simulation runtime (1000 steps).  \n- **Feedback**: The controller fails to stabilize the pendulum\u2014likely because the swing-up logic applies incorrect force direction (based on sign(\u03b8\u00b7d\u03b8)) and the linear LQR is insufficient for highly nonlinear regimes; energy usage is high but ineffective.\n**Program Identifier:** Generation 39 - Patch Name add_swingup_trigger - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with smooth, multiplicative gain scheduling based on pole angle, angular velocity, and falling direction to enable faster stabilization under high-friction, heavy-pole conditions.  \n- **Performance**: Achieved a high combined score of 4666.63 with rapid stabilization (228 steps), low energy use (7.01 total), and perfect final error metrics.  \n- **Feedback**: The adaptive gains significantly improved response to large initial angles and instability while maintaining energy efficiency; the controller robustly handled challenging dynamics from increased mass, length, and friction.\n**Program Identifier:** Generation 40 - Patch Name aggressive_angular_damping_tuned_gains - Correct Program: True\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based modulation on angle and angular velocity errors to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a combined score of 4878.27 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but modest stabilization ratio (0.20) and base score (392.34).  \n- **Feedback**: The conservative LQR weights and high-friction dynamics limited responsiveness and precision, resulting in slower stabilization; however, the adaptive gains improved robustness enough to maintain balance for nearly the full duration despite challenging initial conditions.\n**Program Identifier:** Generation 41 - Patch Name optimized_lqr_weights_with_adaptive_gain - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control**  \n- **Implementation**: Uses an augmented-state LQR controller with integral action for zero steady-state error and adaptive gain scheduling based on angle/velocity error dominance; includes friction modeling and nonlinear dynamics.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum (final theta error: 2.95 rad, x error: >4000 m).  \n- **Feedback**: Despite sophisticated control design, the system diverges\u2014likely due to inaccurate linearization for large initial angles (0.9 rad) combined with aggressive or mis-tuned gains that destabilize rather than correct motion.\n**Program Identifier:** Generation 42 - Patch Name adaptive_lqr_integral - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling based on pole angle and angular velocity via smooth tanh transitions; system parameters reflect a challenging environment (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 4893.84 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) and base score (393.45).  \n- **Feedback**: The conservative LQR weights and adaptive gains enabled stability under difficult dynamics, yielding high time and energy bonuses, but limited responsiveness resulted in slower stabilization and lower precision.\n**Program Identifier:** Generation 43 - Patch Name optimized_adaptive_lqr_with_tanh_scheduling - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 44 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, enhanced with smooth gain scheduling via tanh-based adaptive terms that increase control effort during large deviations or falling-away dynamics.  \n- **Performance**: Achieved a combined score of 4657.90 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization was slow (stabilization_ratio: 0.23).  \n- **Feedback**: The adaptive gains improved robustness in a challenging high-friction, long-pole environment, but conservative base LQR weights limited responsiveness\u2014evident in low time bonus despite eventual stability.\n**Program Identifier:** Generation 45 - Patch Name optimized_q_weights_and_earlier_gain_activation - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with smooth, state-dependent adaptive gains based on angle and angular velocity to handle the highly unstable, high-friction pendulum dynamics.  \n- **Performance**: Achieved a combined score of 4839.16, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (204 steps).  \n- **Feedback**: The refined gain scheduling significantly improved robustness against large initial angles and high friction, enabling stable control despite aggressive system parameters; precise friction modeling in the LQR design was critical for real-world alignment.\n**Program Identifier:** Generation 46 - Patch Name adaptive_lqr_optimized_qr - Correct Program: True\n\n**Program Name: Discrete LQR with Integral Control**  \n- **Implementation**: Uses a discrete-time LQR controller derived from linearized dynamics via matrix exponential, augmented with integral action on cart position and adaptive gain based on angle error. The controller matches the simulator\u2019s physics in its linearization and applies smooth error-weighted blending for robustness.  \n- **Performance**: Achieved a combined score of 1116.13 with perfect stabilization ratio (1.00) over 1000 steps but failed to balance the pole (final_theta_error: 0.91), resulting in zero success bonus.  \n- **Feedback**: Despite strong energy efficiency and sustained simulation time, the controller fails to stabilize the pole upright\u2014likely due to excessive reliance on integral action causing drift and insufficient handling of nonlinearities in the highly unstable system (long, heavy pole with high friction).\n**Program Identifier:** Generation 47 - Patch Name discrete_lqr_integral_control - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a 5-state linearized model with integral control for position regulation, combined with nonlinear gain scheduling based on angle and angular velocity to adaptively scale control effort. Friction terms are accurately modeled in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4284.15 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (278 steps), and full success bonus.  \n- **Feedback**: The integral action effectively eliminated steady-state position error, while adaptive gains improved robustness during large deviations; accurate friction modeling and tuned LQR weights enabled stable control despite the challenging pole parameters.\n**Program Identifier:** Generation 48 - Patch Name lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control**  \n- **Implementation**: Uses a discrete-time LQR controller with integral action on cart position, augmented by sigmoid-based gain scheduling that blends angle- and velocity-dependent adaptive gains for smoother control.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly challenging pendulum (long, heavy pole; high friction; large initial angle), likely due to inaccuracies in linearization, improper integral handling, or insufficient robustness to nonlinear dynamics.\n**Program Identifier:** Generation 49 - Patch Name friction_aware_lqr_v2 - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model of the inverted pendulum to compute LQR gains, enhanced with smooth, differentiable gain scheduling via tanh-based adaptive scaling on position and velocity errors. The controller enforces physics-consistent linearization and applies force clipping for actuator limits.  \n- **Performance**: Achieved a high combined score of 4920.14, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192), and full success bonus.  \n- **Feedback**: The adaptive gain scheduling significantly improved stabilization speed and robustness under challenging conditions (heavy, long pole; high friction; large initial angle). Precise alignment between simulation dynamics and LQR linearization ensured stable control without divergence.\n**Program Identifier:** Generation 50 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Gains**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, then applies adaptive scaling via tanh-based gain scheduling and adds swing-up assistance for large angles.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and fails validation tests.  \n- **Feedback**: The controller\u2019s intentionally lazy response (high R, low Q) and aggressive physical parameters cause instability; adaptive gains and swing-up logic are insufficient to compensate for poor baseline LQR performance under high difficulty.\n**Program Identifier:** Generation 51 - Patch Name discrete_lqr_zoh - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a friction-aware linearized model for LQR gain computation with adaptive, differentiable gain scheduling based on angle and angular velocity errors via sigmoid blending.  \n- **Performance**: Achieved a combined score of 3917.04 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite challenging pole parameters.  \n- **Feedback**: The controller stabilizes the pendulum reliably but is intentionally \"lazy\" (low Q/R tuning), resulting in slower response (stabilization_ratio: 0.33) and moderate base score; adaptive gains help maintain stability without excessive force.\n**Program Identifier:** Generation 52 - Patch Name smooth_error_mode_blending - Correct Program: True\n\n**Program Name: Adaptive LQR with Synergy Gain**  \n- **Implementation**: Uses a high-precision linearized model matching the simulator's physics to compute LQR gains, enhanced with adaptive multiplicative gains based on angle, angular velocity, and falling-away detection via smooth tanh-based scheduling.  \n- **Performance**: Achieved a combined score of 4864.32 with full success bonus, fast stabilization (200 steps), and low energy use (avg 0.01 per step).  \n- **Feedback**: The controller\u2019s accurate system modeling and adaptive gains enabled robust stabilization of an extremely challenging (long, heavy, high-friction) pendulum, balancing speed and efficiency while preventing divergence during large initial angles.\n**Program Identifier:** Generation 53 - Patch Name enhanced_adaptive_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**\n\n- **Implementation**: Uses a linear-quadratic regulator (LQR) controller on an extended state space that includes an integral term for cart position, combined with smooth gain scheduling via tanh-based adaptive gains to handle nonlinearities and large initial errors.\n- **Performance**: Achieved a high combined score of 4502.04, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 248), and full success bonus.\n- **Feedback**: The integral action effectively eliminated steady-state position error, while adaptive gain scheduling improved robustness to the system's challenging dynamics (heavy, long pole with high friction). The controller maintained stability even under aggressive initial conditions.\n**Program Identifier:** Generation 54 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a discrete-time LQR controller extended with integral action for cart position regulation, enhanced by adaptive gain scheduling based on pole angle, angular velocity, and falling direction. The linearized dynamics model matches the simulator\u2019s physics, including consistent treatment of friction and denominator terms.  \n- **Performance**: Achieved a high combined score of 4383.78, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, stabilizing within 264 steps.  \n- **Feedback**: The adaptive gains and integral action effectively handled the challenging setup (heavy, long pole; high friction; large initial angle), enabling stable control while minimizing energy use. Precise alignment between the controller\u2019s linear model and the nonlinear simulator was critical to performance.\n**Program Identifier:** Generation 55 - Patch Name improved_lqr_with_discrete_time_and_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with suboptimal gains and smooth gain scheduling via tanh-based adaptive scaling on angle and angular velocity errors.  \n- **Performance**: Achieved a combined score of 4878.24 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20).  \n- **Feedback**: The conservative LQR tuning prioritizes stability over responsiveness, resulting in low energy use and high success rate but slower convergence (low time bonus relative to max), indicating room for improved transient performance through more aggressive control near instability thresholds.\n**Program Identifier:** Generation 56 - Patch Name increase_angular_velocity_weight_in_q_matrix - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced by smooth tanh-based gain scheduling that increases control effort for large angle/velocity errors.  \n- **Performance**: Achieves a combined score of 0.0 and fails validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly unstable pendulum (long, heavy pole with high friction), likely due to inaccuracies in linearization or insufficient robustness to nonlinear dynamics at large initial angles.\n**Program Identifier:** Generation 57 - Patch Name refine_q_weights - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced by smooth tanh-based gain scheduling that increases control effort for large angle/velocity errors.  \n- **Performance**: Achieved a high combined score of 4920.14, with strong time, energy, and success bonuses indicating stable, efficient balancing.  \n- **Feedback**: The adaptive gain scheduling significantly improved robustness on the challenging high-friction, long/heavy-pole setup, enabling fast stabilization (192 steps) and near-zero final errors while using minimal energy.\n**Program Identifier:** Generation 58 - Patch Name optimized_q_and_multiplicative_gains - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physics-consistent linearized model for LQR control with optimized state weights and smooth tanh-based gain scheduling that adapts to angle and angular velocity errors.  \n- **Performance**: Achieved a high combined score of 4920.13, with strong stabilization (success bonus = 800), low energy use (avg 0.01/step), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains improved transient response without introducing discontinuities, while precise alignment between the controller\u2019s A matrix and simulator dynamics was critical for stability under challenging conditions (heavy, long pole; high friction).\n**Program Identifier:** Generation 59 - Patch Name optimized_lqr_smooth_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a friction-aware linearized model with discrete-time LQR and smooth tanh-based gain scheduling to adaptively adjust control effort based on angle and angular velocity errors.  \n- **Performance**: Achieved a combined score of 4872.34 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20).  \n- **Feedback**: The adaptive gains improved responsiveness to large initial angles while maintaining stability; however, the conservative Q/R tuning limited time efficiency despite low energy use.\n**Program Identifier:** Generation 60 - Patch Name discrete_lqr_with_refined_gains - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based modulation on angle and angular velocity errors to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a combined score of 4878.25 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) indicating slow convergence.  \n- **Feedback**: The conservative LQR weights and high friction tolerance enabled stability under challenging dynamics, yet the \"lazy\" control response limited time-based rewards; adaptive gains improved large-error handling without introducing discontinuities.\n**Program Identifier:** Generation 61 - Patch Name increase_angular_velocity_penalty - Correct Program: True\n\n**Program Name: Phase-Based Adaptive Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-detection mechanism to switch between four specialized control strategies (Recovery, Swing-Back, Capture, Centering), combining LQR near equilibrium with nonlinear adaptive gains and integral cart-position control. The controller dynamically blends base LQR actions with phase-specific forces based on real-time state analysis.  \n- **Performance**: Achieved a high combined score of 4739.13, with strong time, energy, and success bonuses, stabilizing the pole in 218 steps and maintaining low final errors.  \n- **Feedback**: The phase-based adaptation effectively handles the challenging dynamics of a heavy, long pole with high friction, enabling both aggressive recovery and precise stabilization. Energy efficiency and smooth phase transitions contributed significantly to the high performance.\n**Program Identifier:** Generation 62 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a discretized LQR controller with friction-aware linearization, integral action activated near upright via tanh scheduling, and adaptive gain blending based on angle/velocity error dominance.  \n- **Performance**: Achieved a combined score of 3219.32 with moderate time and energy bonuses but low stabilization ratio (0.33).  \n- **Feedback**: The conservative LQR weights and high friction tolerance led to sluggish response and poor stabilization; integral action helped reduce steady-state position error but wasn't sufficient to maintain balance under challenging initial conditions (0.9 rad).\n**Program Identifier:** Generation 63 - Patch Name add_integral_action_with_tanh_activation - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, then applies smooth tanh-based gain scheduling on angle and angular velocity to adaptively scale control effort.  \n- **Performance**: Achieves a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction); likely due to inaccurate linearization or insufficient robustness to nonlinearities and large initial angles.\n**Program Identifier:** Generation 64 - Patch Name fine_tune_q_matrix - Correct Program: False\n\n**Program Name: Adaptive Discrete-Time LQR Controller**  \n- **Implementation**: Uses a discrete-time LQR controller derived from linearized continuous dynamics with Zero-Order Hold discretization, augmented with adaptive gain scheduling based on pole angle and angular velocity. The physics simulation includes high friction, a long/heavy pole, and large initial angle for increased difficulty.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite accurate modeling and adaptive gains, the controller fails to stabilize the highly unstable system\u2014likely due to excessive nonlinearity beyond the LQR\u2019s linear approximation range and insufficient robustness to large initial deviations.\n**Program Identifier:** Generation 65 - Patch Name adaptive_lqr_with_discrete_tuning - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linearized model for LQR control with optimized Q weights and smooth tanh-based gain scheduling that adapts to pole angle and angular velocity errors.  \n- **Performance**: Achieves a combined score of 4920.14 with full success bonus, low energy use (6.45 total), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains significantly improve robustness under challenging conditions (heavy/long pole, high friction), while the accurate linearization and tuned LQR weights enable precise stabilization with minimal control effort.\n**Program Identifier:** Generation 66 - Patch Name adaptive_lqr_tuned_q - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, incorporating friction terms matching the simulator, and applies adaptive gain scheduling based on angle and angular velocity errors to boost responsiveness during large deviations.  \n- **Performance**: Achieved a high combined score of 4920.14 with fast stabilization (192 steps), low energy use (avg 0.01 per step), and perfect final error metrics.  \n- **Feedback**: The controller\u2019s precise modeling of friction and intelligent gain adaptation enabled robust handling of a challenging, heavy, and long pole while maintaining energy efficiency and rapid stabilization.\n**Program Identifier:** Generation 67 - Patch Name optimized_q_matrix_and_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linear quadratic regulator (LQR) with physically accurate system matrices and proven optimal state weights; enhances robustness via multiplicative gain scheduling using tanh-based adaptation on angle and angular velocity.  \n- **Performance**: Achieved a combined score of 4920.13, including full success bonus, low energy usage (total_energy=6.44), and fast stabilization (192 steps).  \n- **Feedback**: The exact LQR tuning and smooth gain scheduling enabled stable control despite challenging parameters (heavy/long pole, high friction); minor Q-matrix adjustment (Q[3]=3.28) improved damping without sacrificing performance.\n**Program Identifier:** Generation 68 - Patch Name energy_based_sliding_mode_control - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based blending of position and velocity gains, tailored to a challenging high-friction, long/heavy-pole environment.  \n- **Performance**: Achieved a combined score of 4893.84 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) indicating slow convergence.  \n- **Feedback**: The conservative LQR weights and adaptive gains enable stability under difficult dynamics but result in sluggish response (low time bonus relative to max), suggesting room for tuning aggressiveness without sacrificing robustness.\n**Program Identifier:** Generation 69 - Patch Name optimize_lqr_weights_and_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive Swing-Up LQR with Integral Control**  \n- **Implementation**: Combines discrete-time LQR on an extended state (including integral of position), physics-informed swing-up assistance using hyperbolic tangent switching, and soft-switched integral action that deactivates far from equilibrium. Uses accurate linearized dynamics and zero-order hold discretization.  \n- **Performance**: Achieved a combined score of 0.00 due to failure to stabilize the pendulum within limits (final theta error: 2.81 rad, final x error: 3498 m).  \n- **Feedback**: Despite aggressive swing-up and integral terms, the controller failed to stabilize the highly challenging system (heavy, long pole with high friction); large steady-state errors suggest insufficient control authority or poor tuning for this extreme configuration.\n**Program Identifier:** Generation 70 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model to compute LQR gains with optimized state weights, augmented by smooth gain scheduling via tanh and physics-informed swing-up feedforward based on angle and angular velocity.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests despite design efforts for high difficulty conditions.  \n- **Feedback**: The controller\u2019s linearization may be inaccurate for large angles (e.g., 0.9 rad initial condition), and the swing-up logic might introduce destabilizing forces; friction modeling in the linearized A matrix appears inconsistent with the nonlinear simulator dynamics.\n**Program Identifier:** Generation 71 - Patch Name additive_swing_assist_feedforward - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with gain scheduling based on pole angle and angular velocity, plus an additive swing-up term that activates for large angles to counteract falling dynamics.  \n- **Performance**: Achieved a high combined score of 4920.15 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The adaptive gains and physics-informed swing-up assist effectively handle the challenging heavy/long pole configuration, enabling robust stabilization while minimizing energy use; precise friction modeling in both simulation and controller contributes to real-world fidelity and performance.\n**Program Identifier:** Generation 72 - Patch Name additive_swing_assist_with_proven_base - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized model to compute LQR gains with suboptimal Q/R weights, enhanced by smooth tanh-based gain scheduling that increases control effort only when angle or angular velocity errors exceed thresholds.  \n- **Performance**: Achieved a combined score of 4920.14 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The conservative LQR tuning ensures stability under challenging physics (long/heavy pole, high friction), while adaptive gains improve transient response without sacrificing smoothness; however, delayed aggressive correction limits time-based rewards.\n**Program Identifier:** Generation 73 - Patch Name none - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR Controller**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation combined with hybrid gain scheduling that blends linear and nonlinear responses based on angle and angular velocity magnitude; includes robust angle normalization and force clipping.  \n- **Performance**: Achieved a high combined score of 4865.75 with strong energy efficiency, full success bonus, and excellent stabilization.  \n- **Feedback**: The adaptive gain scheduling significantly improved control responsiveness for large deviations while maintaining precision near equilibrium, enabling stable performance even under challenging conditions (heavy, long pole with high friction and large initial angle).\n**Program Identifier:** Generation 74 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**\n\n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based smooth transitions and additive physics-informed swing-up assistance for large angles. The system parameters reflect a highly unstable pendulum (long, heavy pole; high friction).\n\n- **Performance**: Achieves an initial score near 3000 but fails to stabilize consistently under validation tests, resulting in a combined score of 0.0.\n\n- **Feedback**: Despite sophisticated control logic, the controller is too \"lazy\" due to conservative LQR weights (low Q, high R), leading to insufficient correction for the challenging dynamics. Validation likely fails because the pendulum exceeds the 1.0 rad failure threshold before stabilization.\n**Program Identifier:** Generation 75 - Patch Name optimized_q_with_swing_assist - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with suboptimal gains, enhanced by physics-informed swing-up logic and adaptive gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 4724.67, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite slow stabilization (stabilization_ratio: 0.22).  \n- **Feedback**: The conservative LQR tuning prioritizes low energy use over fast response, resulting in high energy and success bonuses but modest time and base scores; the swing-up assist effectively handles large initial angles in a challenging high-friction, long-pole environment.\n**Program Identifier:** Generation 76 - Patch Name enhanced_swing_up_assist_with_adaptive_gain - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with tanh-based gain scheduling and additive swing-up control for large angles (>0.8 rad) to handle highly unstable configurations (long, heavy pole).  \n- **Performance**: Achieved a high combined score of 4920.15, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (192 steps).  \n- **Feedback**: The hybrid approach effectively balances optimal linear control near equilibrium with non-linear corrective actions during large deviations, enabling robust performance under extreme physical parameters; angle normalization and friction-aware dynamics modeling contributed to numerical stability and realism.\n**Program Identifier:** Generation 77 - Patch Name enhanced_swingup_assist_and_q_optimization - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in no functional logic or algorithm implementation.  \n- **Performance**: Combined score is 0.0, indicating complete failure to meet validation requirements.  \n- **Feedback**: The absence of any implementation means the program cannot process inputs, produce outputs, or satisfy basic correctness criteria. A valid solution must include core logic aligned with the problem specification.\n**Program Identifier:** Generation 78 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assistance**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based activation and continuous swing-up assistance for large angles, tailored to a highly unstable pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a combined score of 4920.19, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, despite slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The \"lazy\" LQR weights (low Q, high R) prioritize energy over speed, limiting time bonus but enabling robust stability; the smooth, differentiable swing-up logic effectively handles large initial deviations without abrupt control switches.\n**Program Identifier:** Generation 79 - Patch Name continuous_falling_factor_swing_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based smooth transitions and an additive swing-up term for large angles (>0.8 rad), tailored to a challenging pendulum (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 4920.22, with strong energy efficiency (avg_energy_per_step=0.01) and full success bonus, but slow stabilization (stabilization_ratio=0.19).  \n- **Feedback**: The conservative LQR weights (low Q, high R) prioritize low energy use over fast response, resulting in delayed stabilization; however, the swing-up assist effectively recovers from large initial deviations, enabling consistent success despite difficult dynamics.\n**Program Identifier:** Generation 80 - Patch Name falling_severity_swing_assist - Correct Program: True\n\n**Program Name: Adaptive Momentum Compensation Controller**  \n- **Implementation**: Uses phase-aware switching between swing-up, transition, and fine-tuning control modes with momentum compensation, energy shaping, nonlinear damping, and friction compensation; includes integral action near equilibrium and smooth force limiting.  \n- **Performance**: Achieved a combined score of 0.00 due to failure to stabilize the pendulum (final theta error: 1.81 rad, x error: 20.07 m), despite high energy usage (total_energy: 123.20).  \n- **Feedback**: The controller fails to balance the highly unstable system (long, heavy pole with high friction); aggressive swing-up terms may destabilize early dynamics, and LQR-like gains appear insufficient for stabilization under these extreme parameters.\n**Program Identifier:** Generation 81 - Patch Name adaptive_momentum_compensation_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-consistent linearized LQR controller with adaptive gain scheduling and a momentum-based swing-up assist triggered for large angles (>0.8 rad), using normalized angular momentum and smooth tanh activations for differentiable control adjustments.  \n- **Performance**: Achieved a high combined score of 4920.12, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00), stabilizing in 192 steps.  \n- **Feedback**: The swing-up assist effectively handles the challenging initial condition (0.9 rad) with a heavy, long pole, while precise friction modeling and consistent linearization between simulation and LQR design ensure robust stabilization and minimal steady-state error.\n**Program Identifier:** Generation 82 - Patch Name none - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Action**  \n- **Implementation**: Combines a physically accurate linearized LQR controller with conditional integral action (activated near equilibrium), robust angle normalization, smooth gain scheduling via tanh, and physics-informed swing-up assist for large angles.  \n- **Performance**: Achieved a high combined score of 4691.82 with full success bonus, low energy use (avg 0.01/step), and rapid stabilization (224 steps).  \n- **Feedback**: The hybrid approach effectively balances optimal linear control near equilibrium with non-linear corrections during large deviations, enabling stable control despite challenging parameters (heavy/long pole, high friction). Conditional integration prevents windup while improving steady-state cart position accuracy.\n**Program Identifier:** Generation 83 - Patch Name hybrid_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Gated Integral Control**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, augmented with a gated integral term on cart position that activates only near upright equilibrium, and applies smooth gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 1278.20, with perfect stabilization ratio (1.00) over 1000 steps but zero success bonus due to failure to meet terminal position/angle thresholds.  \n- **Feedback**: Despite excellent energy efficiency and stability maintenance, the controller fails to center the cart (final_x_error = 25.87), indicating the integral action is insufficiently aggressive or misaligned with the true control objective; the lack of success bonus suggests the final state does not satisfy evaluation criteria for \"solved\" episodes.\n**Program Identifier:** Generation 84 - Patch Name adaptive_lqr_with_integrated_correction - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model to compute LQR gains, augmented with tanh-based gain scheduling, soft integral control, and swing-up assistance for large angles. The controller adapts force output based on pole angle and angular velocity deviations.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests despite sophisticated design.  \n- **Feedback**: Overly aggressive gain scheduling and incorrect swing-up logic likely destabilize the system under high-difficulty conditions (heavy/long pole, large initial angle). The integral term and friction modeling may also introduce instability rather than robustness.\n**Program Identifier:** Generation 85 - Patch Name soft_integral_cart_centering - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with adaptive gain scheduling based on angle and angular velocity, plus swing-up assistance for large deviations. Friction terms in the dynamics are carefully matched between simulation and controller.  \n- **Performance**: Achieved a high combined score of 4920.22 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The controller excels in balancing aggressive correction for large errors with minimal energy use near equilibrium; precise modeling of friction and well-tuned LQR weights were critical to success.\n**Program Identifier:** Generation 86 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-informed linearized LQR controller with adaptive gain scheduling and phase-aware swing-up assistance based on normalized angular momentum and pole dynamics. Uses precise friction modeling, consistent linearization around upright equilibrium, and smooth exponential transitions for gain adaptation.  \n- **Performance**: Achieved a high combined score of 4635.10 with full success bonus, low energy usage (avg 0.01/step), and fast stabilization (232 steps).  \n- **Feedback**: The swing-up assist effectively handles large initial deviations (0.9 rad) despite challenging pole parameters (heavy, long, high friction), while adaptive gains improve robustness without destabilizing the LQR core. Energy efficiency and stability were maintained through careful physics-consistent design.\n**Program Identifier:** Generation 87 - Patch Name phase_aware_swing_assist - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Action**  \n- **Implementation**: Combines a physically accurate linearized LQR controller with adaptive gain scheduling, swing-up assist for large angles, and soft-switched integral action to eliminate steady-state cart position error. Uses robust angle normalization and Euler integration matching the simulator dynamics exactly.  \n- **Performance**: Achieved a combined score of 1308.26 with perfect stabilization (1000 steps), low average energy per step (0.01), and high energy efficiency (energy_bonus: 2492.28).  \n- **Feedback**: The hybrid approach effectively stabilizes an extremely challenging system (heavy, long pole with high friction and large initial angle). Integral action and swing-up logic prevent drift and aid recovery, while adaptive gains enhance responsiveness without destabilizing the system near equilibrium.\n**Program Identifier:** Generation 88 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Combines energy-shaping swing-up control with LQR-based stabilization, using phase-based switching and friction compensation; includes adaptive gains and predictive terms for improved response.  \n- **Performance**: Achieved high energy usage (2499.93) but failed to stabilize the pendulum (final theta error: 2.93 rad), resulting in zero success bonus and low combined score (0.00).  \n- **Feedback**: Despite sophisticated control logic, the controller could not stabilize the highly challenging system (heavy, long pole with high friction); likely due to inaccurate dynamics modeling or insufficient robustness in transition between control phases.\n**Program Identifier:** Generation 89 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a suboptimal LQR controller with friction-aware linearization, adaptive gain scheduling via smooth tanh-based modulation, and soft-switched integral action for cart centering.  \n- **Performance**: Achieved a combined score of 1196.58 with perfect stabilization ratio (1.00) over 1000 steps but low base accuracy due to high position errors.  \n- **Feedback**: The conservative LQR tuning (low Q, high R) prioritizes stability over precision, resulting in high energy efficiency but poor final/state error metrics; adaptive gains help maintain balance under challenging dynamics (long/heavy pole, high friction).\n**Program Identifier:** Generation 90 - Patch Name add_soft_cart_integral - Correct Program: True\n\n**Program Name: Adaptive LQR with Synergy Gain**  \n- **Implementation**: Uses a high-precision discrete-time LQR controller derived from physically consistent linearized dynamics, enhanced with smooth adaptive gains based on pole angle, angular velocity, and falling-away detection via tanh-based scheduling.  \n- **Performance**: Achieved a combined score of 4842.54 with full success bonus, low energy use (5.86 total), and fast stabilization (203 steps).  \n- **Feedback**: The synergy gain effectively prevents divergence during large-angle excursions, while accurate friction-aware linearization ensures robustness; the adaptive gains significantly improve damping without destabilizing the system.\n**Program Identifier:** Generation 91 - Patch Name discrete_time_lqr_with_tuned_gains - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linearized LQR controller derived from consistent physics equations, enhanced with smooth tanh-based gain scheduling that adaptively increases control effort for large angle or angular velocity errors.  \n- **Performance**: Achieved a combined score of 4920.13 with full success bonus, fast stabilization (192 steps), and very low energy use (total_energy=6.44).  \n- **Feedback**: The adaptive gains significantly improved stabilization speed without sacrificing energy efficiency, while precise alignment between the controller\u2019s linear model and the nonlinear simulator ensured robustness under challenging conditions (heavy, long pole; high friction; large initial angle).\n**Program Identifier:** Generation 92 - Patch Name increase_angular_velocity_weight - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with gain scheduling based on pole angle and angular velocity, plus an additive swing-up assist for large deviations (>0.8 rad) to recover from near-failure states.  \n- **Performance**: Achieved a combined score of 4920.15, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization was slow (stabilization_ratio: 0.19).  \n- **Feedback**: The adaptive gains and swing-up logic effectively prevented failure and enabled eventual stabilization despite aggressive initial conditions and challenging physics; however, conservative base LQR weights limited responsiveness, resulting in low time bonus and delayed convergence.\n**Program Identifier:** Generation 93 - Patch Name multiplicative_gain_with_swing_assist - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Energy Assist**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance and adaptive gain scheduling, using physically accurate system linearization and friction-aware dynamics.  \n- **Performance**: Achieved a high combined score of 4920.21 with strong stabilization (success_bonus: 800), low energy use (avg_energy_per_step: 0.01), and fast stabilization (stabilization_time: 192).  \n- **Feedback**: The controller effectively handles large initial angles and high system instability due to a heavy, long pole; energy-efficient swing-up logic and precise LQR tuning enable both rapid recovery and stable balance.\n**Program Identifier:** Generation 94 - Patch Name robust_swingup_lqr - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a high-precision linearized LQR controller with optimized Q weights, adaptive gain scheduling via tanh-based activation, swing-up assistance for large angles, and soft integral action on cart position with anti-windup.  \n- **Performance**: Achieved a combined score of 4913.35 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (193 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging heavy/long pole and high friction, while integral control improved steady-state positioning without causing windup during transients.\n**Program Identifier:** Generation 95 - Patch Name swing_up_assist_with_integral - Correct Program: True\n\n**Program Name: Phase-Based Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-based control architecture with smooth transitions between recovery, transition, and precision phases; integrates LQR gains computed from friction-aware linearization, adaptive swing-up assistance for large deviations, and gated integral action for cart centering.  \n- **Performance**: Achieved a high combined score of 4845.21 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (203 steps), and perfect final pole angle (0.00 rad).  \n- **Feedback**: The phase-blending strategy effectively handles the challenging dynamics of a heavy, long pole with high friction, while anti-windup and upright-gated integral action prevent instability and ensure precise steady-state positioning.\n**Program Identifier:** Generation 96 - Patch Name phase_based_controller - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control derived from physically accurate linearized dynamics with adaptive gain scheduling and a physics-informed swing-up assist for large angles, using smooth tanh-based activation and falling severity estimation.  \n- **Performance**: Achieved a high combined score of 4920.19 with full success bonus, low energy usage (total_energy=6.48), and rapid stabilization (stabilization_time=192).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the highly unstable configuration (heavy, long pole; high friction); precise angle normalization and divergence-aware assistance enabled robust recovery and stable balancing.\n**Program Identifier:** Generation 97 - Patch Name adaptive_lqr_with_swing_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model with physically accurate friction terms to compute LQR gains, augmented with tanh-based adaptive gain scheduling and soft-switched integral action on cart position.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated control design, the controller fails to stabilize the highly unstable pendulum (long, heavy pole with high friction); likely due to inaccurate linearization, poor handling of nonlinear dynamics, or inadequate robustness to large initial angles.\n**Program Identifier:** Generation 98 - Patch Name robust_arctan2_angle_norm - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a suboptimal LQR controller with gain scheduling, integral action gated near upright, and physics-informed swing-up assistance for large angles. The linearized model accounts for friction, and control gains are adaptively scaled based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 1313.02 with perfect stabilization ratio (1.0) over 1000 steps but low base accuracy due to conservative tuning.  \n- **Feedback**: High energy efficiency (avg 0.01 per step) and full time utilization indicate stable balancing, yet poor position/angle precision (final_x_error=7.98, stable_x_error=2.79) reveals intentionally lazy control from low Q/high R weights, limiting base score despite robustness.\n**Program Identifier:** Generation 99 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a suboptimal LQR controller with friction-aware linearization and smooth gain scheduling via tanh-based adaptive gains to handle large pole angles and high friction.  \n- **Performance**: Achieves a combined score of 4920.14 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite challenging physics (long, heavy pole; high friction).  \n- **Feedback**: The conservative LQR weights result in slower response (low time bonus relative to max), but adaptive gains improve stability at large errors without discontinuities, enabling consistent stabilization within 192 steps.\n**Program Identifier:** Generation 100 - Patch Name update_to_best_lqr_weights - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a friction-aware linearized model for LQR gain computation, enhanced with smooth gain scheduling via tanh-based adaptive gains, swing-up assistance for large angles, and soft-switched integral action on cart position to improve steady-state accuracy.  \n- **Performance**: Achieved a combined score of 4920.10 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization ratio was low (0.19).  \n- **Feedback**: The controller effectively balances the challenging pendulum (long, heavy pole; high friction) and achieves stability, but slow response due to conservative LQR weights limits time bonus; integral action and adaptive gains help maintain precision once stabilized.\n**Program Identifier:** Generation 101 - Patch Name integral_action_for_zero_drift - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with adaptive gain scheduling via tanh-based blending, swing-up assist for large angles, and gated integral action for position error correction.  \n- **Performance**: Achieved a combined score of 1308.25 with perfect stabilization (1000 steps, stabilization ratio 1.0) and very low energy usage (avg 0.01 per step).  \n- **Feedback**: The controller successfully stabilizes an extremely challenging inverted pendulum (long, heavy pole; high friction; large initial angle), demonstrating robustness through hybrid nonlinear adaptations while maintaining energy efficiency\u2014though it incurs a positional drift (final x_error = 7.01), indicating room for improved steady-state accuracy.\n**Program Identifier:** Generation 102 - Patch Name increase_angular_velocity_damping - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**  \n- **Implementation**: Uses a friction-aware linearized model for LQR control with adaptive gain scheduling, integral action gated by pole angle, and physics-informed swing-up assistance for large deviations.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and environmental difficulty, failing validation tests with a combined score of 0.0.  \n- **Feedback**: The controller\u2019s deliberate \"laziness\" (low Q/high R weights) causes slow response and poor precision; despite advanced features like gain scheduling and swing-up logic, it cannot stabilize the highly unstable system within required limits.\n**Program Identifier:** Generation 103 - Patch Name robust_angle_norm - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute discrete-time LQR gains, enhanced by adaptive gain scheduling based on pole angle and angular velocity, plus swing-up assistance for large deviations.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive control logic, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction), likely due to inaccuracies in linearization or insufficient robustness in the gain adaptation under extreme initial conditions.\n**Program Identifier:** Generation 104 - Patch Name discrete_lqr_zoh - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Uses a high-precision linearized LQR controller with optimized Q weights, adaptive gain scheduling via tanh-based activation, swing-up assistance for large angles, and soft integral action on cart position with anti-windup and gating near equilibrium.  \n- **Performance**: Achieved a combined score of 4912.79 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (193 steps), and full success bonus.  \n- **Feedback**: The combination of physically consistent linearization, adaptive gains, and targeted swing-up logic effectively handles the challenging heavy/long pole and high-friction dynamics; integral action improves steady-state positioning without destabilizing transients.\n**Program Identifier:** Generation 105 - Patch Name integral_action_refinement - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 106 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a friction-aware linearized model for LQR control, enhanced with smooth gain scheduling via tanh, swing-up assist for large angles, and anti-windup integral action on cart position.  \n- **Performance**: Achieved a combined score of 4920.09 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle large initial angles and prevent failure, while conservative LQR weights and integral action improve steady-state accuracy without destabilizing the system. However, the controller\u2019s sluggish response limits time-based rewards.\n**Program Identifier:** Generation 107 - Patch Name integral_anti_windup - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with friction-aware dynamics, combined with gain scheduling based on pole angle/velocity and an additive swing-up term for large deviations.  \n- **Performance**: Achieved a high combined score of 4920.20 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The adaptive gains and physics-informed swing-up assist effectively handle the challenging heavy/long pole configuration, enabling robust recovery from large initial angles while minimizing energy use.\n**Program Identifier:** Generation 108 - Patch Name none - Correct Program: True\n\n**Program Name: Phase-Predictive LQR Controller**  \n- **Implementation**: Uses a discretized LQR controller with phase-aware switching between recovery, transition, and precision modes; includes predictive correction, swing-up assistance, and integral action for steady-state accuracy.  \n- **Performance**: Achieved a high combined score of 4782.29 with strong energy efficiency, fast stabilization (212 steps), and perfect final error metrics.  \n- **Feedback**: The adaptive phase logic and predictive corrections effectively handled the challenging high-friction, long-pole dynamics, enabling robust swing-up and precise stabilization while minimizing control effort.\n**Program Identifier:** Generation 109 - Patch Name phase_predictive_lqr - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a linearized friction-aware model for LQR control with adaptive gain scheduling via tanh-based smooth switching, integral action for position regulation (with anti-windup), and physics-informed swing-up assistance for large angles.  \n- **Performance**: Achieved a combined score of 4920.10 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The controller effectively balances the challenging high-friction, long/heavy-pole system and avoids failure, but its conservative gains and delayed response limit time-based rewards despite excellent steady-state accuracy.\n**Program Identifier:** Generation 110 - Patch Name integral_action_with_soft_switching - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Gains**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, augmented with smooth tanh-based gain scheduling and soft-switched integral action for cart position.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and fails validation tests under challenging physics (long/heavy pole, high friction).  \n- **Feedback**: The controller\u2019s \"lazy\" Q/R weights and insufficient robustness to large initial angles cause poor stabilization; adaptive gains help but don\u2019t compensate for fundamental underperformance in hard conditions.\n**Program Identifier:** Generation 111 - Patch Name increase_dtheta_weight - Correct Program: False\n\n**Program Name: Energy-Phase Adaptive LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based adaptive gains, phase-specific strategies (recovery, precision, predictive damping), and integral action with anti-windup for steady-state accuracy. The controller dynamically adjusts force output based on normalized mechanical energy, angular velocity, and state magnitude to handle a highly unstable, heavy, and long inverted pendulum.  \n- **Performance**: Achieved a high combined score of 4755.01 with strong bonuses for success, time, and energy efficiency, stabilizing the system in 215 steps with near-zero final errors.  \n- **Feedback**: The adaptive gain scheduling and phase-aware logic effectively managed the challenging dynamics (high mass, length, friction), while predictive damping and integral control ensured precise stabilization without overshoot or windup.\n**Program Identifier:** Generation 112 - Patch Name energy_phase_controller - Correct Program: True\n\n**Program Name: Adaptive Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-based hybrid controller with energy-based swing-up for large angles, blended transition control, and LQR with integral action for stabilization; includes nonlinear modifications like velocity damping and control authority scaling.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization despite high energy usage (total_energy=8.22) and full simulation duration (1000 steps).  \n- **Feedback**: The controller failed to stabilize the pendulum (final_theta_error=2.77 rad), indicating ineffective mode transitions or insufficient robustness for the highly unstable system (long, heavy pole with high friction); integral windup or aggressive swing-up may have hindered convergence.\n**Program Identifier:** Generation 113 - Patch Name none - Correct Program: False\n\n**Program Name: Feedback Linearization with Adaptive LQR**  \n- **Implementation**: Combines discrete LQR control on a linearized model with exact nonlinear inverse dynamics to track a desired angular acceleration, augmented by adaptive gain scheduling and gated integral action for position regulation.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: The controller struggles with the highly unstable, high-friction, and heavy/long pole configuration; likely due to singularities in inverse dynamics near \u00b1\u03c0/2 and insufficient robustness in the feedback linearization under large initial angles (0.9 rad). The adaptive gains and integral term may not compensate adequately for model inaccuracies or aggressive nonlinearities.\n**Program Identifier:** Generation 114 - Patch Name fbl_discretelqr_integral - Correct Program: False\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with optimized Q-matrix weights, enhanced by adaptive gain scheduling and a continuous swing-up assist that activates based on angle magnitude and falling severity.  \n- **Performance**: Achieved a combined score of 4920.22 with perfect stabilization (final theta error = 0.00) and high energy efficiency (avg_energy_per_step = 0.01).  \n- **Feedback**: The controller effectively balances aggressive correction for large deviations with fine-tuned stabilization near upright, while the swing-up logic ensures recovery from highly unstable initial conditions; the exact match between simulation dynamics and LQR model was critical to success.\n**Program Identifier:** Generation 115 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, then applies adaptive gain scheduling via tanh-based scaling and adds swing-up assistance for large angles (>0.8 rad).  \n- **Performance**: Achieves low initial score (~3000) due to conservative Q/R tuning and insufficient robustness in highly unstable conditions.  \n- **Feedback**: The controller fails validation tests because the linearized LQR is inadequate for the highly nonlinear, high-friction, long-pole regime; swing-up logic lacks smooth integration and doesn\u2019t prevent early failure beyond 1.0 rad.\n**Program Identifier:** Generation 116 - Patch Name fix_duplicate_init_remove_integral - Correct Program: False\n\n**Program Name: Phase-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Uses a three-phase control strategy (Recovery, Transition, Precision) with physics-informed LQR gains, adaptive swing-up logic, and anti-windup integral action for steady-state error correction.  \n- **Performance**: Achieved a high combined score of 4760.24 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps.  \n- **Feedback**: The aggressive recovery phase effectively handles large initial angles, while the gated integral term ensures precise stabilization without windup; high friction and pole mass make this a robust solution under challenging dynamics.\n**Program Identifier:** Generation 117 - Patch Name adaptive_lqr_with_integral_correction - Correct Program: True\n\n**Program Name: Adaptive Energy-Phase Inverted Pendulum Controller**  \n- **Implementation**: Combines LQR-based linear control with energy-aware swing-up logic, phase-specific gains, integral action for precision stabilization, and predictive damping to handle a highly unstable pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a high combined score of 4755.01 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps with minimal final errors.  \n- **Feedback**: The adaptive gain scheduling and phase-aware switching effectively managed the challenging dynamics, while integral control and predictive damping significantly improved steady-state accuracy and reduced overshoot near equilibrium.\n**Program Identifier:** Generation 118 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Uses an enhanced adaptive LQR controller with integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, and a swing-up assist activated during large deviations. The dynamics include high-friction, heavy/long pole parameters for increased difficulty.  \n- **Performance**: Achieved a combined score of 4927.10 with full success bonus, low energy usage (6.46 total), and stabilization after 191 steps.  \n- **Feedback**: The integral control effectively eliminated steady-state cart error when upright, while the swing-up logic and adaptive gains enabled recovery from highly unstable initial conditions (0.9 rad). Energy efficiency and stability were maintained despite aggressive physical challenges.\n**Program Identifier:** Generation 119 - Patch Name adaptive_lqr_integral_swingup - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-informed linear quadratic regulator (LQR) with adaptive gain scheduling and a smooth, momentum-based swing-up assist for large angles; includes soft-switched integral action to eliminate steady-state cart position error.  \n- **Performance**: Achieved a high combined score of 4920.25, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and perfect upright maintenance (final_theta_error: 0.00).  \n- **Feedback**: The swing-up assist effectively handles the challenging initial condition (0.9 rad) with a heavy, long pole, while adaptive gains and integral action improve robustness and precision without destabilizing the system.\n**Program Identifier:** Generation 120 - Patch Name increase_dtheta_q_weight - Correct Program: True\n\n**Program Name: Phase-Adaptive Discrete LQR Controller**  \n- **Implementation**: Uses discrete-time LQR with zero-order hold discretization, enhanced by phase-based swing-up assistance, adaptive gain scheduling, filtered state estimation, and integral control activated only during fine-tuning.  \n- **Performance**: Achieved a combined score of 0.0 and failed validation tests.  \n- **Feedback**: The controller\u2019s complex phase logic and energy-based swing-up may be misaligned with the highly unstable system dynamics (heavy, long pole); discretization via an incorrect matrix exponential approximation likely introduced instability, and aggressive clipping or filtering may degrade responsiveness.\n**Program Identifier:** Generation 121 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physics-informed linearized model for LQR control with adaptive gain scheduling based on pole angle and velocity, plus swing-up assistance for large deviations and leaky integral action for cart position regulation.  \n- **Performance**: Achieved a high combined score of 4920.25 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The controller effectively balances aggressive stabilization for large angles with precise upright maintenance, while friction-aware modeling and smooth gain transitions prevent instability and energy waste.\n**Program Identifier:** Generation 122 - Patch Name add_swing_up_assist_and_integral_control - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state error and a physics-informed swing-up assist that activates for large pole angles, using smooth gain scheduling via tanh-based activation functions.  \n- **Performance**: Achieved a high combined score of 4920.25, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging dynamics (heavy, long pole; high friction), while integral control ensured precise cart positioning once upright\u2014demonstrating robustness and efficiency in both transient and steady-state phases.\n**Program Identifier:** Generation 123 - Patch Name Add integral control for cart position and swing-up assist for large angles - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with adaptive gain scheduling based on angle and angular velocity, plus physics-informed swing-up assistance for large deviations.  \n- **Performance**: Achieved a high combined score of 4898.88 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 195 steps).  \n- **Feedback**: The controller effectively balances aggressive correction for large errors with energy-efficient fine control near equilibrium; the swing-up assist and multiplicative gain adaptation were key to handling the challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 124 - Patch Name discrete_lqr_design - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance using predictive angle estimation, adaptive gain scheduling via tanh activation, and soft-switched integral action for steady-state error correction. The controller uses accurate linearized dynamics matching the nonlinear simulator and includes friction-aware modeling.  \n- **Performance**: Achieved a high combined score of 4872.08 with strong time, energy, and success bonuses, stabilizing the pendulum in 199 steps with minimal final errors.  \n- **Feedback**: The swing-up assist effectively handles large initial angles (0.9 rad), while adaptive gains and integral action improve robustness and precision; low average energy per step (0.01) indicates efficient control despite challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 125 - Patch Name predictive_feedforward_swing - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Energy Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart positioning; uses physically accurate linearization and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4913.06 with full success bonus, low energy use (avg 0.01/step), and fast stabilization (193 steps).  \n- **Feedback**: The controller effectively handles the challenging pole parameters (heavy, long) by intelligently switching between swing-up and stabilization modes; integral control and adaptive gains ensure precise cart centering and robustness without excessive energy expenditure.\n**Program Identifier:** Generation 126 - Patch Name add_cart_vel_cross_coupling - Correct Program: True\n\n**Program Name: Discrete LQR with Energy-Aware Swing-Up**\n\n- **Implementation**: Uses a discrete-time LQR controller derived via zero-order hold to match Euler integration dynamics, enhanced with energy-based swing-up logic and adaptive integral control that decays aggressively when the pole is far from upright. The controller applies gain scheduling based on angle and angular velocity, and only activates swing-up assistance when energy falls below 85% of the upright potential energy.\n\n- **Performance**: Achieved a high combined score of 4892.07, with strong contributions from time bonus (2206.23), energy efficiency (2493.47), and full success bonus (800.00), stabilizing in 196 steps with near-zero final errors.\n\n- **Feedback**: The energy-aware swing-up prevents unnecessary force application while effectively recovering from large initial angles; adaptive integral decay and gain scheduling significantly improve robustness under challenging conditions (heavy, long pole with high friction).\n**Program Identifier:** Generation 127 - Patch Name discrete_lqr_energy_adaptive - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action for cart position, and physics-informed swing-up assistance triggered at large angles (>0.8 rad). Friction is accurately modeled in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4933.93 with excellent energy efficiency (avg_energy_per_step: 0.01) and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up assist effectively stabilized the highly challenging pendulum (long, heavy pole; large initial angle), while integral action ensured precise long-term positioning without compromising stability.\n**Program Identifier:** Generation 128 - Patch Name none - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) stabilization around the upright position with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart position error elimination; uses physically accurate linearization and directional momentum awareness to enhance swing-up efficiency.  \n- **Performance**: Achieved a high combined score of 4927.07 with full success bonus, low energy usage (total_energy=6.50), and fast stabilization (191 steps).  \n- **Feedback**: The controller effectively balances aggressive swing-up and precise stabilization, with integral action and adaptive gains contributing to zero final errors and robustness under challenging conditions (heavy/long pole, high friction).\n**Program Identifier:** Generation 129 - Patch Name directional_momentum_integral_control - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed gain scheduling, swing-up assistance for large angles, and soft-switched integral action to eliminate steady-state cart position error. The controller uses accurate linearized dynamics matching the simulator and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4920.25 with full success bonus, low energy use (total_energy=6.47), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging pole parameters (heavy, long, high friction), while integral control ensured precise upright stabilization without windup due to leaky integration and gating.\n**Program Identifier:** Generation 130 - Patch Name enhanced_swing_up_with_integral_control - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action gated by angle/velocity thresholds, and physics-informed swing-up assistance for large deviations. Friction-aware system modeling ensures accurate controller synthesis.  \n- **Performance**: Achieved a high combined score of 4926.67 with excellent energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up assist effectively handle the challenging heavy/long pole configuration, while gated integral action prevents destabilization during transients\u2014key to balancing speed, stability, and energy use.\n**Program Identifier:** Generation 131 - Patch Name joint_integral_gate_and_tuned_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed adaptive gain scheduling, swing-up assistance for large angles, and gated integral action for zero steady-state cart position error. The controller uses robust angle normalization, smooth activation functions (tanh), and leaky integration with angle-dependent decay.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and near-perfect stabilization.  \n- **Feedback**: The enhanced swing-up logic and adaptive gains effectively handled the challenging pole parameters (longer, heavier), while the gated integral term improved long-term positioning without destabilizing the system during large swings.\n**Program Identifier:** Generation 132 - Patch Name integral_windup_prevention_with_angle_velocity_gate - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines gain-scheduled LQR with robust swing-up assist, enhanced angular damping, and gated integral action that activates only near upright equilibrium to minimize drift.  \n- **Performance**: Achieved a high combined score of 4926.67 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00).  \n- **Feedback**: The controller effectively stabilizes a highly challenging pendulum (long, heavy, high friction) by blending aggressive swing-up logic with precise integral correction, though stabilization ratio remains low (0.19), indicating delayed convergence.\n**Program Identifier:** Generation 133 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model to compute LQR gains, enhanced with adaptive gain scheduling, robust angle normalization via `arctan2`, soft-switched integral control gated by pole angle and velocity, and a swing-up assist that activates based on normalized falling severity.  \n- **Performance**: Achieved a high combined score of 4926.64, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (stabilization_time: 191).  \n- **Feedback**: The integral action effectively eliminated steady-state cart position error without destabilizing transients, while the swing-up logic and adaptive gains enabled reliable recovery from large initial angles (0.9 rad) despite challenging pole parameters (longer and heavier than standard).\n**Program Identifier:** Generation 134 - Patch Name refined_integral_gating_with_velocity - Correct Program: True\n\n**Program Name: Phase-Based Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Uses a hybrid control strategy with energy-based swing-up for large angles, smooth transition blending, and LQR with integral action near equilibrium; includes friction modeling and Euler integration.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization despite high energy usage (total_energy: 101.95) and full simulation duration (stabilization_time: 1000).  \n- **Feedback**: The controller failed to stabilize the pendulum (final_theta_error: 2.95 rad), indicating ineffective phase transitions or insufficient robustness for the challenging pole parameters (longer, heavier, high friction).\n**Program Identifier:** Generation 135 - Patch Name none - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments and whitespace.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass any validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 136 - Patch Name None - Correct Program: False\n\n**Program Name: Energy-Shaping Hybrid Controller**  \n- **Implementation**: Combines energy-based swing-up control with LQR stabilization and smooth phase transitions based on angle, velocity, and energy thresholds; includes integral action with anti-windup and centrifugal compensation.  \n- **Performance**: Achieved a combined score of 34.79 but failed to stabilize the pendulum (success_bonus: 0.00), despite maintaining full simulation duration (stabilization_ratio: 1.00).  \n- **Feedback**: The controller could not balance the pole (final_theta_error: 3.12 rad), indicating ineffective transition or stabilization logic; high friction and aggressive pole parameters likely exposed weaknesses in the LQR design or phase-switching criteria.\n**Program Identifier:** Generation 137 - Patch Name energy_shaping_hybrid_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and soft-switched integral action to eliminate steady-state cart position error, using gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and perfect final pole alignment (final_theta_error: 0.00).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging high-mass, long-pole configuration, while integral control with angle/velocity gating prevented windup and improved precision without destabilizing the system.\n**Program Identifier:** Generation 138 - Patch Name enhanced_integral_suppression - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up**\n\n- **Implementation**: Uses an enhanced adaptive LQR controller featuring integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, and a swing-up assist activated during large deviations. Friction and nonlinear dynamics are explicitly modeled in simulation.\n\n- **Performance**: Achieved a high combined score of 4926.69 with full success bonus, low energy usage (total_energy=6.50), and perfect final stabilization (final_theta_error=0.00).\n\n- **Feedback**: The soft-switched integral control effectively eliminated steady-state cart drift without destabilizing transients, while the normalized swing-up logic provided reliable recovery from extreme angles. Optimized angular velocity damping (Q[3]=3.28) reduced oscillations and contributed to early stabilization (stabilization_time=191).\n**Program Identifier:** Generation 139 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up Assist**  \n- **Implementation**: Uses an enhanced adaptive Linear Quadratic Regulator (LQR) controller featuring integral action for steady-state cart position correction, robust angle normalization via `arctan2`, gain scheduling based on pole angle and angular velocity, and energy-aware swing-up assistance triggered when the pole is far from upright.  \n- **Performance**: Achieved a high combined score of 4906.68, with strong contributions from time bonus (2220.04), energy efficiency (2492.62), and full success bonus (800.00), stabilizing in 194 steps with near-zero final errors.  \n- **Feedback**: The integral control effectively eliminated steady-state cart drift without destabilizing the system due to intelligent gating by angle and velocity; the swing-up logic efficiently managed the challenging initial condition (0.9 rad) with a heavy, long pole, while adaptive gains improved phase-specific responsiveness.\n**Program Identifier:** Generation 140 - Patch Name energy_aware_swingup_assist - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in a complete lack of functionality for any intended algorithm or task.  \n- **Performance**: Combined score is 0.0, indicating failure to pass any validation tests.  \n- **Feedback**: An empty implementation cannot process inputs or produce outputs, leading to total failure in evaluation; actual logic must be implemented to address the problem requirements.\n**Program Identifier:** Generation 141 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced with adaptive gain scheduling, predictive correction using angular acceleration, physics-informed swing-up assistance for large angles, and gated integral control for cart position.  \n- **Performance**: Achieved a high combined score of 4787.89 with strong time, energy, and success bonuses, stabilizing in 211 steps with near-zero final errors.  \n- **Feedback**: The controller effectively balances aggressive early-phase control with energy-efficient stabilization; the swing-up assist and adaptive gains are critical for handling the challenging pole parameters (heavy, long, high friction), while integral action with gating prevents windup and improves steady-state accuracy.\n**Program Identifier:** Generation 142 - Patch Name aggressive_early_phase_control - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines gain-scheduled LQR with robust integral action, enhanced angular damping, and physics-informed swing-up assistance; uses precise linearized dynamics matching the simulator\u2019s friction model and adaptive gains based on pole angle/velocity.  \n- **Performance**: Achieved a high combined score of 4926.94 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large angles with fine stabilization near upright, while leaky integral action and soft-switched damping reduce overshoot and improve cart centering without destabilizing the pole.\n**Program Identifier:** Generation 143 - Patch Name optimized_integral_and_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and integral action for cart position regulation, using gain scheduling and energy-based scaling to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a high combined score of 4907.13 with full success bonus, low energy usage, and stable final state.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large deviations with precise LQR stabilization near upright, while integral action eliminates steady-state position error without windup due to soft gating and leaky integration.\n**Program Identifier:** Generation 144 - Patch Name energy_aware_swing_up_assist - Correct Program: True\n\n**Program Name: Phase-Based Adaptive Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-switching strategy with four LQR controllers (swing-up, transition, balancing, fine-tune), energy shaping for swing-up, integral action with windup protection, predictive compensation, and nonlinear damping.  \n- **Performance**: Combined score of 0.0; fails to stabilize the pendulum under challenging initial conditions and system parameters.  \n- **Feedback**: Despite sophisticated control logic, the controller struggles with the highly unstable configuration (long, heavy pole; high friction; large initial angle), likely due to inadequate swing-up energy injection or poor phase transition timing.\n**Program Identifier:** Generation 145 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with adaptive gain scheduling based on angle and angular velocity, enhanced by soft-switched integral action and energy-aware swing-up assistance.  \n- **Performance**: Achieved a high combined score of 4907.12 with strong time, energy, and success bonuses, stabilizing in 194 steps with minimal final errors.  \n- **Feedback**: The controller effectively balances aggressive stabilization for large deviations with energy efficiency near equilibrium; the inclusion of friction in the linearized model and intelligent gain adaptation were key to handling the challenging heavy/long pole configuration.\n**Program Identifier:** Generation 146 - Patch Name sharpen_gain_transitions - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, robust swing-up assistance based on mechanical energy deficit, and gated integral action for cart position stabilization. The controller uses a physically accurate linearized model and includes enhanced damping near equilibrium.  \n- **Performance**: Achieved a high combined score of 4926.90, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 191), and full success bonus.  \n- **Feedback**: The hybrid approach effectively handles the challenging dynamics of a heavy, long pole with high friction, demonstrating that adaptive gains and energy-based swing-up significantly improve robustness and stabilization speed, while leaky integral action prevents windup without degrading performance.\n**Program Identifier:** Generation 147 - Patch Name energy_based_swing_up - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, robust swing-up assistance based on falling severity, and soft-switched integral action for cart position regulation. The controller uses a physically accurate linearized model and dynamically adjusts gains based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4933.93, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (190 steps), and full success bonus.  \n- **Feedback**: The hybrid approach effectively stabilizes a highly challenging inverted pendulum (long, heavy pole; high friction; large initial angle). Integral action and swing-up logic are key to maintaining stability and achieving near-zero final errors, while adaptive gains prevent overreaction during large deviations.\n**Program Identifier:** Generation 148 - Patch Name simplified_integral_gating_with_optimal_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control derived from physically accurate linearized dynamics with adaptive gain scheduling, swing-up assistance based on normalized angular momentum, and dual-gated integral action that activates only near upright and low-velocity conditions.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192 steps), and full success bonus.  \n- **Feedback**: The hybrid approach effectively handles the challenging pole parameters (heavy, long, high friction), with adaptive gains and swing-up logic enabling robust recovery from large initial angles, while gated integral control prevents windup and improves steady-state accuracy.\n**Program Identifier:** Generation 149 - Patch Name adaptive_lqr_integral_swingup_enhanced - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and gated integral action to eliminate steady-state cart position error; uses adaptive gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and full success bonus.  \n- **Feedback**: The enhanced swing-up logic effectively handles the challenging initial conditions (heavy, long pole with large initial angle), while the soft-gated integral term prevents windup and improves final positioning without destabilizing the system.\n**Program Identifier:** Generation 150 - Patch Name adaptive_energy_aware_swingup - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Uses a refined LQR controller with gain scheduling, angle-only integral gating, and swing-up assistance for large deviations; includes physically accurate linearization and friction-aware dynamics.  \n- **Performance**: Achieved a high combined score of 4920.84 with full success bonus, low energy use (6.58 total), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains and simplified integral control effectively balanced stability and efficiency, while the swing-up logic enabled recovery from large initial angles (0.9 rad); tight anti-windup and decay prevented integral windup without sacrificing responsiveness.\n**Program Identifier:** Generation 151 - Patch Name refined_adaptive_lqr_optimized - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control and Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with gain scheduling, integral action for cart position regulation, and physics-informed swing-up assistance triggered by pole angle and falling dynamics. Uses accurate friction modeling and Euler integration for simulation.  \n- **Performance**: Achieved a high combined score of 4934.54 with full success bonus, low energy use (6.61 total), and fast stabilization (190 steps).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle large initial angles and system instability from heavy/long poles, while integral control ensures precise long-term positioning without windup due to velocity-gated activation.\n**Program Identifier:** Generation 152 - Patch Name sharper_gain_transitions_and_velocity_gated_integral - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Integral Action**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart position stabilization; uses physically accurate dynamics and directional momentum awareness to enhance recovery during large-angle excursions.  \n- **Performance**: Achieved a high combined score of 4933.59 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 190 steps), and perfect final state accuracy.  \n- **Feedback**: The controller effectively balances aggressive swing-up and precise stabilization, with integral action eliminating steady-state cart error without windup; directional momentum logic prevents overshoot during recovery, contributing to robustness under challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 153 - Patch Name enhanced_integral_control_with_dynamic_gating - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state cart position error, adaptive gain scheduling via tanh-based angle/velocity thresholds, and physics-informed swing-up assist using energy deficit and angular momentum cues.  \n- **Performance**: Achieved a high combined score of 4926.95 with strong stability (final theta error \u2248 0), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The integration of swing-up logic enabled recovery from large initial angles (0.9 rad), while adaptive gains and integral control ensured precise stabilization; energy-efficient actuation and leaky integration prevented windup and maintained robustness under high-friction, heavy-pole conditions.\n**Program Identifier:** Generation 154 - Patch Name energy_scaled_swing_up_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state error and a physics-informed swing-up assist that activates for large pole angles (>0.8 rad), using smooth tanh-based gain scheduling and dual-gated integral feedback.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00).  \n- **Feedback**: The adaptive gains and swing-up logic enabled rapid recovery from large initial angles (0.9 rad), while integral control ensured precise stabilization; the design effectively balanced aggressive correction with energy conservation under high-friction, unstable dynamics.\n**Program Identifier:** Generation 155 - Patch Name enhanced_integral_control_with_velocity_gating_and_adaptive_decay - Correct Program: True\n\n**Program Name: Phase-Aware Adaptive LQR Controller**  \n- **Implementation**: Uses three phase-specific LQR controllers (swing-up, transition, stabilize) combined with energy-based swing-up logic and adaptive integral action with anti-windup. Angle normalization and momentum-aware control enhance robustness under high-friction, heavy-pole conditions.  \n- **Performance**: Achieved a combined score of 4478.10 with full success bonus, low final errors, and efficient energy use (avg 0.01 per step).  \n- **Feedback**: The phase-adaptive strategy effectively handles the challenging dynamics (long, heavy pole; high friction), enabling rapid stabilization (253 steps) and maintaining balance with minimal energy expenditure. Integral action and smooth phase transitions prevent windup and ensure precision.\n**Program Identifier:** Generation 156 - Patch Name none - Correct Program: True\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with cross-coupled state weighting, adaptive gain scheduling, energy-based swing-up assistance for large angles, and enhanced integral action with angle/velocity gating and decay.  \n- **Performance**: Achieved a high combined score of 4953.84, with strong stabilization (final theta error \u2248 0), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The energy-aware swing-up and adaptive gains effectively handled the challenging heavy/long pole, while integral control with tight gating improved cart centering without destabilizing the system near equilibrium.\n**Program Identifier:** Generation 157 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Energy-Based Swing-Up**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, dual-gated integral action for cart position, and energy-based swing-up assistance triggered by large pole angles. The controller uses physically accurate linearization and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4947.50, with strong time, energy, and success bonuses, stabilizing the pendulum in 188 steps despite extreme parameters (heavy, long pole; high friction).  \n- **Feedback**: The energy-based swing-up and adaptive gains effectively handled large initial deviations, while dual-gated integral control improved steady-state cart positioning without destabilizing the pole. Evaluation confirms robustness and efficiency under challenging dynamics.\n**Program Identifier:** Generation 158 - Patch Name adaptive_lqr_integral_v2 - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in a complete lack of functionality for any intended algorithm or task.  \n- **Performance**: Combined score is 0.0, indicating failure to pass any validation tests.  \n- **Feedback**: An empty implementation cannot perform computation or meet problem requirements; adding core logic is essential for correctness and scoring.\n**Program Identifier:** Generation 159 - Patch Name energy_coupled_lqr - Correct Program: False\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, predictive falling severity estimation, and enhanced integral action with state-dependent gating and decay. The controller uses a physically accurate linearized model and cross-coupled Q-matrix terms for coordinated cart-pole dynamics.  \n- **Performance**: Achieved a high combined score of 4953.84, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 187 steps), and full success bonus.  \n- **Feedback**: The energy-aware swing-up logic and adaptive gains effectively handled large initial angles and system instability from the heavy, long pole. Integral control with tight velocity/angle gating prevented windup while improving steady-state cart positioning, contributing to low final errors and high time/energy bonuses.\n**Program Identifier:** Generation 160 - Patch Name enhanced_swingup_with_momentum_prediction - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines gain-scheduled LQR control with adaptive integral action, enhanced angular damping, and physics-informed swing-up assistance to stabilize a challenging inverted pendulum (heavy, long pole with high friction).  \n- **Performance**: Achieved a high combined score of 4933.93, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00), stabilizing in 190 steps.  \n- **Feedback**: The adaptive gains and integral clamping effectively handled large initial angles and high system instability, while the swing-up logic provided critical recovery during aggressive falls\u2014key to robust performance under extreme parameters.\n**Program Identifier:** Generation 161 - Patch Name adaptive_integral_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines physically accurate linearized dynamics with gain-scheduled LQR, swing-up assistance for large angles, and gated integral action with anti-windup to handle a challenging high-friction, long/heavy-pole inverted pendulum.  \n- **Performance**: Achieved a high combined score of 4920.84 with full success bonus, low energy use (6.58 total), and fast stabilization (192 steps).  \n- **Feedback**: The sharpened gain scheduling and angle-gated integral control effectively balanced aggressive swing-up and precise stabilization, while the tuned LQR weights (notably Q[3]=3.2) and friction-aware modeling were critical to robustness under difficult physical parameters.\n**Program Identifier:** Generation 162 - Patch Name adaptive_integral_lqr_hybrid - Correct Program: True\n\n**Program Name: Adaptive Energy-Aware LQR with Swing-Up Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, directional momentum-aware swing-up logic, and soft-switched integral action for cart position, all tuned for extreme pendulum parameters (heavy, long pole; high friction).  \n- **Performance**: Achieved a high combined score of 4927.07, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and stable final state.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large deviations with precise LQR stabilization near upright, while integral action eliminates steady-state position error without causing overshoot\u2014key to handling the challenging system dynamics.\n**Program Identifier:** Generation 163 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Energy-Based Swing-Up**\n\n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, dual-gated integral action for cart position, predictive momentum compensation, and energy-based swing-up assistance tailored to extreme pendulum parameters (heavy, long pole with high friction).\n\n- **Performance**: Achieved a high combined score of 4947.47, with strong bonuses for success (800), time (2261), and energy efficiency (2492), stabilizing in 188 steps with near-zero final errors.\n\n- **Feedback**: The controller effectively handles challenging initial conditions and system dynamics through intelligent nonlinear enhancements; the energy-based swing-up and adaptive gains were critical for rapid stabilization and robustness under high friction and instability.\n**Program Identifier:** Generation 164 - Patch Name predictive_momentum_compensation - Correct Program: True\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance, predictive falling severity estimation, and adaptive integral action that activates only near equilibrium; includes cross-coupled terms in the Q-matrix and state-dependent gain scheduling for robustness.  \n- **Performance**: Achieved a high combined score of 4953.83 with strong time, energy, and success bonuses, stabilizing the pendulum in 187 steps with minimal final errors.  \n- **Feedback**: The controller effectively handles the challenging pole parameters (longer, heavier) by blending aggressive swing-up logic with precise stabilization; integral control and predictive momentum compensation significantly improve centering and robustness without excessive energy use.\n**Program Identifier:** Generation 165 - Patch Name predictive_momentum_compensation - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action gated by pole angle/velocity, and energy-based swing-up assistance for large deviations. Uses physically accurate friction modeling in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4926.68 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle the challenging heavy/long pole configuration, while gated integral action prevents destabilization during transients\u2014key to balancing speed and stability.\n**Program Identifier:** Generation 166 - Patch Name enhanced_swing_up_with_energy_awareness - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines physically accurate linearized dynamics with LQR control (Q[3]=3.2), gain scheduling based on angle/velocity thresholds, energy-aware swing-up logic, and gated integral action with anti-windup for cart centering.  \n- **Performance**: Achieved a high combined score of 4920.85 with full success bonus, low energy use (avg 0.01/step), and rapid stabilization (192 steps).  \n- **Feedback**: The sharpened gain scheduling and energy-aware swing-up effectively handled the challenging heavy/long pole, while the simplified integral gating proved robust for steady-state accuracy without windup issues.\n**Program Identifier:** Generation 167 - Patch Name enhanced_swing_up_with_energy_awareness - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control and Swing-Up**\n\n- **Implementation**: Uses an enhanced adaptive LQR controller with integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, swing-up assistance triggered at large angles, and mid-swing angular damping. The dynamics include high-friction terms and a heavy, long pole to increase difficulty.\n\n- **Performance**: Achieved a combined score of 0.00 despite a base score of 7.16, failing to stabilize the pendulum (final theta error: 2.94 rad, final x error: 4432 m) over 1000 steps.\n\n- **Feedback**: The controller fails to stabilize the system due to excessive drift in cart position and inability to recover from large initial angles; integral action may be too weak or improperly gated, and swing-up logic might interfere with stabilization rather than assist it.\n**Program Identifier:** Generation 168 - Patch Name sharper_gain_scheduling - Correct Program: False\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines LQR with velocity-gated integral action, gentle gain scheduling, and swing-up assistance for large angles; uses physically accurate linearized dynamics and proven Q/R weights.  \n- **Performance**: Achieved a high combined score of 4954.47 with full success bonus, low energy use (6.51 total), and fast stabilization (187 steps).  \n- **Feedback**: The hybrid approach effectively balances aggressive swing-up for large deviations with precise LQR control near upright, while velocity-gated integration prevents windup and improves stability.\n**Program Identifier:** Generation 169 - Patch Name adaptive_integral_lqr_hybrid - Correct Program: True\n\n**Program Name: Adaptive Cross-Coupled LQR Controller**  \n- **Implementation**: Uses an adaptive LQR with state-dependent cross-coupling in the Q-matrix, mid-swing angular damping, predictive momentum compensation, and dual-gated integral control to stabilize a highly unstable inverted pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a combined score of 5015.73 with full success bonus, low energy use (7.54 total), and fast stabilization (178 steps).  \n- **Feedback**: The controller effectively handles large initial angles and aggressive dynamics through adaptive gains and predictive corrections; the dual-gated integral term prevents windup while aiding precision near upright equilibrium.\n**Program Identifier:** Generation 170 - Patch Name adaptive_cross_coupled_lqr_with_midswing_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with gain scheduling, robust swing-up logic based on energy deficit and momentum prediction, velocity-gated integral action for cart centering, and enhanced damping near equilibrium. The controller uses a physically accurate linearized model and adaptive gains that increase responsiveness during large deviations.  \n- **Performance**: Achieved a high combined score of 4756.64 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps with minimal final errors.  \n- **Feedback**: The hybrid approach effectively handles the challenging dynamics (heavy, long pole; high friction), with integral action and swing-up assist crucial for recovery and stabilization. Energy-efficient control and precise gain scheduling contributed to low average energy use and fast settling.\n**Program Identifier:** Generation 171 - Patch Name none - Correct Program: True\n\n**Program Name: Phase-Aware Momentum-Cancellation LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with phase-based adaptive gains, momentum-aware integral action, and energy-injection recovery logic tailored to a highly unstable inverted pendulum (heavy, long pole with high friction). Uses stability-index-driven phase blending, predictive feedforward, and dual-gated integrals for precision stabilization.  \n- **Performance**: Achieved a high combined score of 4967.97, with strong time, energy, and success bonuses, stabilizing the system in 185 steps with near-zero final errors.  \n- **Feedback**: The controller effectively handles extreme initial conditions and physical challenges through its hybrid strategy; evaluation confirms robustness, efficient energy use, and precise stabilization, validating the novel momentum-integral and phase-aware design choices.\n**Program Identifier:** Generation 172 - Patch Name phase_aware_momentum_lqr - Correct Program: True\n\n**Program Name: Hybrid Adaptive Cross-Coupled LQR Controller**  \n- **Implementation**: Combines cross-coupled LQR with mid-swing damping, sharpened gain scheduling, predictive momentum compensation, and angle-gated integral control to stabilize a challenging inverted pendulum (heavy, long pole; high friction; large initial angle).  \n- **Performance**: Achieved a high combined score of 4983.31 with full success bonus, low energy use (7.54 total), and fast stabilization (183 steps).  \n- **Feedback**: The controller effectively balances aggressive swing-up for large angles with precise linear regulation near upright, while adaptive gains and predictive terms prevent overshoot and divergence; integral gating avoids windup during transients.\n**Program Identifier:** Generation 173 - Patch Name adaptive_crosscoupled_lqr_with_midswing_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Predictive Damping**\n\n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, predictive angular acceleration estimation for mid-swing damping, energy-aware integral action, and swing-up assistance for large angles. Uses physically accurate linearized dynamics and friction-aware modeling.\n\n- **Performance**: Achieved a high combined score of 4934.14, with strong time, energy, and success bonuses, stabilizing the pendulum in 190 steps (19% of max time) and maintaining near-zero final errors.\n\n- **Feedback**: The integration of predictive damping and energy-based integral gain adaptation significantly improved robustness under challenging conditions (heavy, long pole; high friction). Early stabilization and low energy usage indicate efficient, proactive control that balances swing-up and balancing phases effectively.\n**Program Identifier:** Generation 174 - Patch Name enhanced_adaptive_lqr_with_predictive_damping - Correct Program: True",
        "meta_scratch_pad": "## Successful Algorithmic Patterns\n\n- **State-dependent cross-coupling in the LQR Q-matrix significantly improves coordination between angle and angular velocity**, enabling faster stabilization under high-inertia conditions. The current best program (*Adaptive Cross-Coupled LQR Controller*, **5015.73**) introduces off-diagonal terms `Q[1,3] = Q[3,1] = 0.8`, dynamically boosted by `cross_coupling_boost = 1.0 + 0.5 * tanh(3.0 * abs(theta))`. This yields the fastest stabilization time (**178 steps**) among all evaluated programs\u2014beating even the previously top-performing Generation 165 (**187 steps**)\u2014and contributes directly to its record **time_bonus: 2328.30**.\n\n- **Gaussian-shaped mid-swing damping provides targeted momentum suppression without destabilizing near-equilibrium control**. The current best uses `mid_swing_activation = exp(-4.0 * (abs(theta) - 0.5)**2)` to apply damping only in the critical **0.25\u20130.85 rad** range, avoiding interference with swing-up or balancing phases. This contrasts with blunt constant damping and explains why this program achieves **stable_x_error: 0.05** despite aggressive swing-up, outperforming *Hybrid Adaptive Integral LQR* (**4756.64**) which lacks such phase-specific damping.\n\n- **Predictive momentum compensation using estimated angular acceleration enables earlier counteraction of divergence**, especially crucial for the heavy, long pole (`M_POLE=0.35`, `L_POLE=2.5`). The current best estimates `theta_acc_est` from gravity and joint friction, then computes `pred_divergence = theta * dtheta_pred / (omega_n * L_COM)` to generate a corrective term. This proactive strategy\u2014absent in Generations 171 and 174\u2014contributes to its near-zero final errors (**final_theta_error: 0.00**, **final_x_error: 0.00**) and supports its elite energy efficiency (**total_energy: 7.54**) by reducing corrective overactuation.\n\n- **Dual-gated integral control with exponential decay during inactivity maintains precision without windup**, building on prior successful patterns but refined for extreme dynamics. The current best uses identical gating (`angle_gate = tanh(12.0*(0.1\u2212|\u03b8|))`, `velocity_gate = tanh(8.0*(1.0\u2212|d\u03b8|))`) as the previous best (**4954.47**), but couples it with **integral decay (`*= 0.95`)** when inactive\u2014a technique validated in Generation 167. This synergy ensures rapid reactivation near equilibrium, achieving **stable_x_error: 0.05**, matching the precision of top predecessors while operating under more challenging physical parameters.\n\n## Ineffective Approaches\n\n- **Omission of predictive or mid-swing damping mechanisms leads to momentum overshoot and slower stabilization**, even with otherwise strong LQR foundations. *Hybrid Adaptive Integral LQR Controller* (**4756.64**)\u2014despite excellent integral gating and gain scheduling\u2014stabilizes in **215 steps**, **37 steps slower** than the current best, due to lack of explicit mid-phase damping or predictive correction. This confirms that reactive LQR alone is insufficient for the heaviest/longest poles tested.\n\n- **Swing-up logic that lacks falling-severity normalization wastes energy and delays transition to balancing**. While all top programs use swing-up, the current best\u2019s use of `falling_factor = 1.0 + tanh(2.0 * theta * dtheta / (L_COM * omega_n))` makes assistance directionally intelligent. In contrast, *Adaptive LQR with Predictive Damping* (**4934.14**) uses less refined swing-up, resulting in **190-step stabilization**\u20147 steps slower than the current best\u2014indicating that unnormalized swing-up is suboptimal under high inertia.\n\n- **Static or weakly adaptive cross-coupling fails to exploit state-dependent coordination benefits**. Earlier correct programs like Generation 171 (**4756.64**) and 172 (**4967.97**) omit cross-coupled Q-terms entirely, relying solely on diagonal weights and gain scheduling. Their scores\u2014while high\u2014are consistently **>48 points below** the current best (**5015.73**), demonstrating that cross-coupling is not merely optional but a key differentiator at the performance frontier.\n\n## Implementation Insights\n\n- **The current best program integrates four synergistic layers\u2014adaptive cross-coupled LQR, Gaussian mid-swing damping, predictive momentum compensation, and dual-gated integral control\u2014into a single coherent architecture**, each layer addressing a distinct phase of the control challenge. Unlike piecemeal approaches, this holistic design ensures smooth transitions: swing-up activates only beyond **0.8 rad**, mid-swing damping peaks at **0.5 rad**, predictive correction scales with divergence, and integral action engages only within **0.1 rad** and low velocity. This layered phase-awareness is absent in all lower-scoring programs.\n\n- **Precise friction-aware linearized dynamics in the A-matrix are non-negotiable for zero final angle error under high joint friction (`FRICTION_JOINT=0.25`)**. The current best explicitly models joint friction in `A[3,3] = -b_j / (m*l*denom0)`, matching the structure used in all correct programs. This fidelity\u2014combined with the proven Q-matrix `diag([4.5, 44.0, 0.6, 3.2])`\u2014enables perfect upright stabilization (**final_theta_error: 0.00**), whereas even minor model inaccuracies would cause residual oscillation under these extreme parameters.\n\n- **Force clipping redundancy (inside controller and simulation loop) ensures robustness during aggressive swing-up maneuvers**, such as the current best\u2019s `u_swing = 8.0 * ...` term. This dual safeguard prevents numerical overflow and guarantees actuator compliance, directly enabling its full **success_bonus: 800.00** across all test conditions\u2014a pattern consistently present in all correct programs but missing in failed ones like Generation 168.\n\n- **Natural frequency normalization (`omega_n = sqrt(G / L_COM)`) enables scalable, physics-informed decision-making across varying pole lengths and masses**. The current best uses `omega_n` to normalize both predictive divergence (`theta * dtheta_pred / (omega_n * L_COM)`) and falling severity, ensuring control intensity adapts to the actual dynamic timescale. This explains its ability to maintain **avg_energy_per_step: 0.01**\u2014matching saturated elite efficiency\u2014while stabilizing **9 steps faster** than Generation 174, which uses similar but less normalized predictive logic.\n\n## Performance Analysis\n\n- **The current best program (**5015.73**) sets a new performance ceiling, surpassing all predecessors by \u226532 points**, primarily through unprecedented speed (**178 steps**) rather than energy savings (which remain saturated at ~7.5 total). Its **time_bonus: 2328.30** exceeds the previous best (**2267.91**) by **60.39 points**, confirming that **swing-up and mid-phase efficiency\u2014not just balancing precision\u2014is now the dominant scoring factor** under extreme physical parameters.\n\n- **Cross-coupled LQR with predictive damping outperforms hybrid integral-LQR and phase-aware variants**, despite their sophistication. *Phase-Aware Momentum-Cancellation LQR* (**4967.97**) and *Hybrid Adaptive Cross-Coupled LQR* (**4983.31**) both score **>32 points below** the current best, indicating that **explicit cross-coupling in the cost function combined with Gaussian mid-swing damping provides superior transient control** compared to phase-blending or sharpened gain scheduling alone.\n\n- **Energy efficiency has plateaued across all correct implementations (total_energy \u2248 6.5\u20137.54)**, making time-to-stabilization the sole meaningful differentiator above **4920**. The current best\u2019s **178-step stabilization**\u2014**7\u201337 steps faster** than peers\u2014accounts for nearly all of its score advantage, as all top programs earn the same **success_bonus: 800.00** and achieve near-identical final errors.\n\n- **The combination of predictive compensation and mid-swing damping uniquely enables both fast swing-up and minimal overshoot**, resolving a key trade-off that limited earlier controllers. Programs without both features\u2014e.g., Generation 171 (no predictive/mid-swing) or Generation 174 (predictive but no Gaussian damping)\u2014show measurably slower convergence, proving that **proactive momentum management across the entire swing-up trajectory is essential for record performance**.",
        "embed_cost": 0.0,
        "novelty_cost": 0.0,
        "stdout_log": "Evaluating program: results_pendulum_liter/gen_178/main.py\nSaving results to: results_pendulum_liter/gen_178/results\nRun 1/1 completed in 0.12 seconds\nCorrectness and error status saved to results_pendulum_liter/gen_178/results/correct.json\nMetrics saved to results_pendulum_liter/gen_178/results/metrics.json\nEvaluation or Validation failed: Validation failed: Cart moved out of bounds (>5.0m)\nMetrics:\n  combined_score: 0.0\n  public: {'score': 0.0, 'base_score': 12.71844072470201, 'time_bonus': 1.0063878837075355, 'energy_bonus': 9.423786028745421e-06, 'success_bonus': 0.0, 'stabilization_time': 1000, 'stabilization_ratio': 1.0, 'avg_energy_per_step': 0.8684929373063232, 'total_energy': 868.4929373063231, 'mean_step_reward': 0.01271844072470201, 'final_theta_error': 1.679098929090209, 'final_x_error': 3838.716002358118, 'stable_theta_error': 1.6159599267658076, 'stable_x_error': 1430.486532346738}\n  private: {'oscillation_penalty': -1.0, 'angle_oscillation_penalty': -374.0, 'boundary_penalty': -2000.0, 'unstable_penalty': -500.0, 'max_theta': 3.133639422775996, 'max_x': 3838.716002358118, 'max_force': 100.0, 'force_std': 37.20535644121505}\n  execution_time_mean: 0.1198946500007878\n  execution_time_std: 0.0\n  num_valid_runs: 0\n  num_invalid_runs: 1\n  all_validation_errors: ['Cart moved out of bounds (>5.0m)']\n",
        "stderr_log": "",
        "code_analysis_metrics": {
          "cyclomatic_complexity": 24,
          "average_cyclomatic_complexity": 2.6666666666666665,
          "halstead_volume": 3962.4062518028904,
          "halstead_difficulty": 7.799568965517241,
          "halstead_effort": 30905.060830333317,
          "lines_of_code": 257,
          "logical_lines_of_code": 140,
          "comments": 51,
          "maintainability_index": -26.362355349358197,
          "max_nesting_depth": 4,
          "complexity_score": 1.0
        }
      },
      "in_archive": false
    }
  ],
  "meta_summary": "**Program Name: Suboptimal LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with conservative LQR gains (low Q, high R) to stabilize a challenging inverted pendulum system with heavy/long pole and high friction; control actions are clipped to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 3195.17, with strong energy efficiency but slow stabilization (459 steps, 46% ratio).  \n- **Feedback**: The overly conservative LQR tuning results in sluggish response and poor error correction early on, limiting time and precision bonuses despite eventual stability and low energy use.\n**Program Identifier:** Generation 0 - Patch Name initial_program - Correct Program: False\n\n**Program Name: Suboptimal LQR with Nonlinear Compensation**  \n- **Implementation**: Uses a linearized LQR controller with aggressive Q/R weights combined with gravity and damping compensation terms to stabilize a highly unstable inverted pendulum (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 0.00 due to large final position/angle errors despite running all 1000 steps.  \n- **Feedback**: The controller fails to stabilize the system\u2014final cart position diverges drastically (>4000 m) and pole angle remains far from upright (~2.94 rad), indicating poor control authority or incorrect linearization assumptions under large deviations.\n**Program Identifier:** Generation 1 - Patch Name tune_lqr_nonlinear_comp - Correct Program: False\n\n**Program Name: Sliding Mode Energy Controller**\n\n- **Implementation**: Combines energy-based swing-up with sliding mode stabilization, featuring adaptive gain scheduling, friction compensation, and smooth sign approximation to reduce chattering; uses hybrid control modes based on pole angle magnitude.\n\n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilizing the pendulum (final theta error = 2.52 rad), despite moderate energy efficiency (avg_energy_per_step = 0.35) and full episode length (stabilization_time = 1000).\n\n- **Feedback**: The controller fails to stabilize the pendulum within the required bounds (|theta| \u2264 1.0 rad), indicating insufficient robustness or incorrect tuning for the highly challenging system parameters (heavy, long pole with high friction); energy shaping may dominate over stabilization near upright, preventing convergence.\n**Program Identifier:** Generation 2 - Patch Name sliding_mode_energy_shaping_hybrid - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized continuous-time LQR controller with aggressive Q/R weights on a highly unstable pendulum (long, heavy pole; high friction), but applies it directly to the nonlinear system without accounting for large-angle deviations or proper state wrapping in dynamics.\n- **Performance**: Achieves full 1000-step simulation (stabilization_ratio=1.0) but with very poor control accuracy (final_theta_error=3.11 rad, final_x_error=4433 m) and low combined score (0.00).\n- **Feedback**: Despite running to completion, the controller fails to stabilize the pendulum near upright due to mismatch between linear LQR assumptions and highly nonlinear, challenging dynamics; large steady-state errors indicate inadequate gain tuning or linearization validity.\n**Program Identifier:** Generation 3 - Patch Name correct_linearization_tune_weights - Correct Program: False\n\n**Program Name: Hybrid Energy-LQR Inverted Pendulum Controller**  \n- **Implementation**: Combines energy-based swing-up control for large angles with a nonlinear LQR stabilizer using state-dependent gain scheduling; includes friction modeling and Euler integration.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum within limits (final theta error: 2.52 rad, exceeded 1.0 rad threshold).  \n- **Feedback**: Despite sophisticated hybrid control design, the controller fails to recover from the aggressive initial condition (0.9 rad) given the heavy, long pole and high friction; likely issues include inaccurate linearization in LQR design and insufficient robustness in transition logic.\n**Program Identifier:** Generation 4 - Patch Name nonlinear_lqr_with_energy_shaping - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized LQR controller with hand-tuned Q and R matrices on a highly unstable inverted pendulum (long, heavy pole; high friction); applies Euler integration for dynamics and clips control force to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 0.00 due to poor stabilization\u2014final angle error of 2.95 rad and position error over 4000 m.  \n- **Feedback**: Despite aggressive LQR tuning (high angle weight, low control penalty), the controller fails to stabilize the system, likely due to severe nonlinearity from large initial angle (0.9 rad) and model mismatch between linear LQR and highly nonlinear dynamics.\n**Program Identifier:** Generation 5 - Patch Name aggressive_lqr_tuning - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses two LQR controllers (conservative for large angles, aggressive near upright) with linearized dynamics around the upright equilibrium; applies state feedback with angle wrapping and force clipping.  \n- **Performance**: Scored 0.00 due to failure to stabilize, despite full simulation steps (stabilization_ratio=1.00), with high final position error (4357.35 m) and pole angle error (2.91 rad).  \n- **Feedback**: The controller fails to handle the highly unstable system (long, heavy pole; high friction); aggressive gains activate too late, and conservative gains are too weak to recover from large initial angle (0.9 rad). Linearization may be invalid for such large deviations.\n**Program Identifier:** Generation 6 - Patch Name accurate_linearization_and_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses three gain-scheduled LQR controllers selected based on angle and angular velocity magnitude, with state normalization and energy-based force scaling near equilibrium.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization (final theta error: 2.82 rad, cart position diverged to 4340 m).  \n- **Feedback**: Despite adaptive gains and proper linearization, the controller failed to stabilize the highly unstable system (long, heavy pole with high friction), suggesting insufficient robustness or incorrect dynamics modeling in the control law.\n**Program Identifier:** Generation 7 - Patch Name adaptive_lqr_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses two LQR controllers (recovery and balance modes) blended via a sigmoid-like function based on pole angle; control gains are scheduled smoothly to handle large initial deviations.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum within limits, despite high base score (14.54), indicating simulation ran full duration but ended far from upright.  \n- **Feedback**: The controller failed to correct large initial angle (0.9 rad) under challenging dynamics (heavy, long pole); final theta error (2.76 rad) and massive cart drift (x \u2248 3318 m) suggest instability or incorrect linearization assumptions in LQR design.\n**Program Identifier:** Generation 8 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a suboptimal adaptive LQR controller that switches between conservative and aggressive gain matrices based on pole angle; implemented with Euler integration and nonlinear dynamics simulation.  \n- **Performance**: Achieved a combined score of 0.00 due to high final position error (4262.33 m) and angle error (3.06 rad), despite running all 1000 steps.  \n- **Feedback**: The controller fails to stabilize the cart-pole system under challenging conditions (long, heavy pole; high friction); poor state regulation suggests inadequate gain tuning or linearization mismatch in the highly nonlinear regime.\n**Program Identifier:** Generation 9 - Patch Name adaptive_lqr_gains_near_equilibrium - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**\n\n- **Implementation**: Uses gain-scheduled LQR control blending aggressive and precision gains based on pole angle magnitude, augmented with integral action for friction compensation and additional damping terms for high velocities. Includes anti-windup, friction compensation, and smooth saturation.\n\n- **Performance**: Achieved a combined score of 0.00 despite moderate base_score (9.47) and time_bonus (1.01), due to complete failure in stabilization (final_theta_error: 3.03 rad, final_x_error: 4425 m).\n\n- **Feedback**: The controller fails to stabilize the pendulum under challenging conditions (heavy, long pole; high friction; large initial angle). Integral action and adaptive gains are insufficient to overcome model inaccuracies and aggressive dynamics, leading to divergence rather than recovery.\n**Program Identifier:** Generation 10 - Patch Name adaptive_gain_lqr_with_integral - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized model with friction-aware dynamics to compute an LQR controller; state feedback gain is derived from continuous-time algebraic Riccati equation with modest Q/R weights favoring low control effort over fast stabilization.  \n- **Performance**: Achieved a combined score of 3252.50, with strong energy efficiency (avg_energy_per_step: 0.01) but moderate stabilization ratio (0.44) and time bonus due to slow response.  \n- **Feedback**: The conservative LQR tuning successfully stabilized the challenging high-friction, long/heavy-pole system but sacrificed speed and precision\u2014evidenced by low base and time scores despite perfect final error and high success bonus.\n**Program Identifier:** Generation 11 - Patch Name correct_linear_model_friction_tune_gains - Correct Program: True\n\n**Program Name: High-Performance LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized model of an inverted pendulum with aggressive LQR cost weights (`Q=[15,35,2,4]`, `R=0.8`) and includes angle normalization; control forces are clipped to \u00b1100 N.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: The LQR controller is based on a linear approximation that likely fails to capture the highly nonlinear dynamics of the heavy, long pole; early termination conditions (e.g., angle >1.0 rad) were commented out, possibly allowing unstable behavior to continue unchecked.\n**Program Identifier:** Generation 12 - Patch Name lqr_optimized_structural - Correct Program: False\n\n**Program Name: Aggressive LQR with Gain Scheduling**  \n- **Implementation**: Uses a state-dependent blend between aggressive and conservative LQR controllers based on pole angle magnitude, with high Q weights for tight angle control and lower R to allow stronger forces; designed for a heavy, long pole with high friction.  \n- **Performance**: Combined score: 0.0 \u2014 fails to stabilize the pendulum under validation tests.  \n- **Feedback**: Despite sophisticated gain scheduling and tuning for challenging dynamics, the controller fails to handle large initial angles or nonlinearities beyond the linearized model\u2019s validity; early termination due to instability suggests insufficient robustness in recovery phase.\n**Program Identifier:** Generation 13 - Patch Name aggressive_lqr_with_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses an extended state-space model with integral action for steady-state error reduction and adaptive gain scheduling based on pole angle to handle nonlinearities. The controller computes LQR gains offline using continuous-time Riccati equation and applies angle normalization and anti-windup logic for the integrator.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated control design, the linearized model used for LQR does not adequately capture the highly nonlinear dynamics of the heavy, long pole under high friction; additionally, the simulation lacks early termination on failure (e.g., pole angle >1.0 rad), leading to invalid trajectories that likely caused evaluation failures.\n**Program Identifier:** Generation 14 - Patch Name adaptive_lqr_with_integral_action - Correct Program: False\n\n**Program Name: Gain-Scheduled LQR Controller**  \n- **Implementation**: Uses a gain-scheduled LQR controller with low-gain (robust) and high-gain (precise) modes blended based on pole angle magnitude; system linearized around upright equilibrium but applied to highly nonlinear, challenging dynamics (long/heavy pole, high friction).  \n- **Performance**: Achieved only 10.40 base score with poor stabilization (final theta error 2.87 rad, large cart drift), indicating failure to control the pendulum despite full simulation duration.  \n- **Feedback**: The conservative LQR tuning (low Q, high R) and reliance on linearization in a highly nonlinear regime caused sluggish, inaccurate responses; controller failed to stabilize even within generous time limits, suggesting need for more aggressive gains or nonlinear control strategies.\n**Program Identifier:** Generation 15 - Patch Name gainscheduled_lqr - Correct Program: False\n\n**Program Name: Friction-Aware LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with friction terms in the A matrix and optimized LQR weights; includes angle normalization and semi-implicit state estimation for improved feedback.  \n- **Performance**: Achieved a combined score of 0.00 despite high base_score (9.85), failing to stabilize the pendulum within limits (final theta error: 2.82 rad, cart position diverged).  \n- **Feedback**: The controller failed to handle the highly unstable system (long, heavy pole with high friction); aggressive LQR tuning was insufficient without proper nonlinear handling or robustness to large initial angles.\n**Program Identifier:** Generation 16 - Patch Name friction_aware_lqr_optimized - Correct Program: False\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses a linearized state-space model with friction approximations to compute LQR gains, augmented with adaptive gain scheduling that increases control effort for large pole angles (>0.6 rad). Control forces are clipped to \u00b1100 N for physical realism.  \n- **Performance**: Achieved a high combined score of 3381.03, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800), stabilizing the pendulum in 422 steps.  \n- **Feedback**: The adaptive gain scheduling effectively handled the challenging initial condition (0.9 rad) and high-friction dynamics, enabling rapid stabilization despite the heavy, long pole. The controller maintained near-zero final errors, demonstrating robustness and precision.\n**Program Identifier:** Generation 17 - Patch Name advanced_friction_compensated_lqr - Correct Program: True\n\n**Program Name: Enhanced LQR with Friction Compensation**  \n- **Implementation**: Uses a linearized state-space model with friction terms to compute an LQR gain matrix, augmented with velocity-based gain scheduling and anti-windup force clipping.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilizing the pendulum within acceptable bounds despite high energy use.  \n- **Feedback**: The controller failed to stabilize the pole (final theta error = 3.04 rad), likely due to inaccurate linearization for large angles and insufficient robustness to the system's high nonlinearity, mass, and length.\n**Program Identifier:** Generation 18 - Patch Name none - Correct Program: False\n\n**Program Name: Suboptimal LQR with Friction Compensation**  \n- **Implementation**: Uses a linearized LQR controller with conservative Q/R weights and adds heuristic friction compensation for both cart and joint; state is wrapped to [\u2212\u03c0, \u03c0].  \n- **Performance**: Achieved a combined score of 3373.21, with moderate time and energy bonuses but low base accuracy due to high error tolerance.  \n- **Feedback**: The overly conservative LQR gains cause sluggish response and poor stabilization (only 42% stable steps), while friction compensation helps marginally but cannot overcome the weak control authority; evolution potential remains high.\n**Program Identifier:** Generation 19 - Patch Name nonlinear_friction_compensation_lqr - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware model to compute an LQR controller with deliberately conservative gains (low Q, high R), resulting in sluggish but stable control under challenging physics (long/heavy pole, high friction).  \n- **Performance**: Achieved a combined score of 3945.50, with strong energy efficiency and full success bonus despite slow stabilization.  \n- **Feedback**: The controller successfully stabilizes the pendulum but does so slowly (stabilization ratio 0.32), reflecting the trade-off from tuning for low control effort over responsiveness; this validates the design intent of a \"lazy\" yet functional baseline for evolutionary improvement.\n**Program Identifier:** Generation 20 - Patch Name friction_aware_lqr_model - Correct Program: True\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized state-space model with LQR control, but the linearization inaccurately models friction and ignores nonlinear dynamics; Q/R weights prioritize angle over position yet are mismatched to the highly unstable physical parameters (long/heavy pole, high friction).  \n- **Performance**: Achieved near-zero combined score due to catastrophic failure in stabilization\u2014final theta error of 2.76 rad and cart drift exceeding 4000 m.  \n- **Feedback**: The controller fails because the linearized model is invalid for large initial angles (0.9 rad) and aggressive system dynamics; joint friction was incorrectly incorporated into the A matrix instead of being treated as a disturbance or nonlinear term, leading to poor real-world approximation and instability.\n**Program Identifier:** Generation 21 - Patch Name refine_lqr_model_with_physical_consistency - Correct Program: False\n\n**Program Name: Suboptimal LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware model to compute an LQR controller with deliberately conservative gains (low Q, high R) for stabilizing a challenging inverted pendulum (long, heavy pole; high friction). The controller applies clipped forces based on full-state feedback with angle normalization.\n\n- **Performance**: Achieved a combined score of 3933.91, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate time and stabilization performance (stabilization_ratio: 0.32).\n\n- **Feedback**: The conservative LQR tuning prioritizes low control effort over fast response, resulting in slow stabilization and lower time/stabilization scores despite perfect final error and high energy efficiency\u2014indicating room for improvement via gain optimization or nonlinear control.\n**Program Identifier:** Generation 22 - Patch Name fix_joint_friction_in_a_matrix - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling based on pole angle magnitude and additional angular velocity damping for small angles.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: Despite adaptive gains and damping, the controller is too conservative (high R, low Q) for the challenging environment (long/heavy pole, high friction), leading to instability and early failure; the initial angle of 0.9 rad may exceed its recovery capability.\n**Program Identifier:** Generation 23 - Patch Name smooth_adaptive_gain_scheduling - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**  \n- **Implementation**: Uses a hybrid adaptive LQR controller with precise linearization of the nonlinear pendulum dynamics and gain scheduling based on pole angle and velocity magnitude. The controller boosts control gains when the pole deviates significantly from upright or when system velocity is high.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite accurate modeling and aggressive gain tuning, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction). Likely issues include inadequate handling of nonlinearities beyond small-angle approximations and possible instability from gain scheduling logic interacting poorly with the clipped control force.\n**Program Identifier:** Generation 24 - Patch Name adaptive_friction_compensated_lqr - Correct Program: False\n\n**Program Name: Friction-Compensated LQR Controller**  \n- **Implementation**: Uses a linearized state-space model with corrected friction terms and smooth gain scheduling based on pole angle; applies LQR control with optimized weights and clips force output.  \n- **Performance**: Achieved base score of 14.15 but failed to stabilize (final theta error = 2.62 rad, large position drift), resulting in zero success bonus and low combined score (0.00).  \n- **Feedback**: Despite improved modeling (e.g., correct joint friction term) and gain scheduling, the controller cannot stabilize the highly unstable system (heavy, long pole with high friction); poor handling of nonlinearities and insufficient robustness lead to failure.\n**Program Identifier:** Generation 25 - Patch Name smooth_adaptive_gains_with_corrected_friction_model - Correct Program: False\n\n**Program Name: Suboptimal LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware dynamics model to compute LQR gains with low state penalties (Q) and high control cost (R), plus gain scheduling based on pole angle.  \n- **Performance**: Achieved a combined score of 4431.51 with moderate time and energy bonuses but slow stabilization (259 steps, 26% ratio).  \n- **Feedback**: The conservative LQR tuning prioritizes low energy use over fast response, resulting in delayed stabilization; gain scheduling helps recover from large angles but doesn't fully compensate for the intentionally sluggish baseline policy.\n**Program Identifier:** Generation 26 - Patch Name tune_lqr_weights_angular_priority - Correct Program: True\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses an LQR controller with linearized dynamics including friction approximations, enhanced with nonlinear friction compensation via tanh-smoothed sign functions and adaptive gain scheduling for large angle deviations.  \n- **Performance**: Achieved a high combined score of 3447.47, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus due to stable final state.  \n- **Feedback**: The friction compensation and gain scheduling effectively stabilized a challenging high-friction, long-pole system; early termination was avoided by maintaining stability beyond 400 steps despite aggressive initial conditions.\n**Program Identifier:** Generation 27 - Patch Name nonlinear_friction_compensation - Correct Program: True\n\n**Program Name: Suboptimal LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains with intentionally conservative Q/R weights; applies gain scheduling based on pole angle.  \n- **Performance**: Achieves low initial score (~3000) due to sluggish response and high error tolerance, failing validation tests.  \n- **Feedback**: The controller\u2019s \"lazy\" tuning (low state penalties, high control cost) and lack of robustness to the challenging physics (long/heavy pole, high friction) prevent effective stabilization; early termination logic is commented out, risking wasted computation after failure.\n**Program Identifier:** Generation 28 - Patch Name switch_to_discrete_lqr - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model with corrected friction terms to compute LQR gains, enhanced by velocity- and angle-aware adaptive gain scheduling for aggressive stabilization.  \n- **Performance**: Achieved a high combined score of 4491.63 with fast stabilization (251 steps), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The adaptive gain scheduling significantly improved control authority during large deviations while maintaining efficiency near equilibrium, enabling robust handling of the challenging heavy/long pole configuration.\n**Program Identifier:** Generation 29 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Swing-Up Assistance**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with gain scheduling and energy-based swing-up logic for large angles; state feedback is blended during transition from swing-up to stabilization.  \n- **Performance**: Achieves low initial score (~3000) due to conservative LQR weights and challenging physics (long/heavy pole, high friction).  \n- **Feedback**: The controller stabilizes the pendulum but responds slowly and tolerates large errors; evaluation indicates room for improvement in both control aggressiveness and handling of extreme initial conditions.\n**Program Identifier:** Generation 30 - Patch Name enhanced_lqr_with_swing_up_trigger - Correct Program: False\n\n**Program Name: Friction-Compensated Adaptive LQR Controller**  \n- **Implementation**: Uses a linearized state-space model at equilibrium to compute LQR gains, augmented with nonlinear friction compensation and smooth gain scheduling based on pole angle magnitude. However, the `Controller.__init__` references undefined variable `cos_theta` when constructing matrix `A`, causing runtime errors.  \n- **Performance**: Combined score: 0.0 \u2014 fails validation due to incorrect dynamics linearization and implementation bugs.  \n- **Feedback**: The controller logic is conceptually sound but flawed in execution; the undefined `cos_theta` breaks initialization, and the linearized `A` and `B` matrices do not correctly reflect the true system dynamics around \u03b8=0, undermining stability guarantees.\n**Program Identifier:** Generation 31 - Patch Name refine_lqr_gain_scheduling - Correct Program: False\n\n**Program Name: Hybrid Energy-LQR with Friction Compensation**  \n- **Implementation**: Combines energy-based control for large pole angles and LQR for small angles, with adaptive friction compensation and smooth blending between modes. Uses nonlinear dynamics simulation with Euler integration and high-friction-aware linearization for LQR design.  \n- **Performance**: Scored 0.00 due to failure to stabilize; achieved full 1000 steps but with large final errors (theta: 1.24 rad, x: 85.26 m) and no success bonus.  \n- **Feedback**: Despite sophisticated hybrid control and friction handling, the controller failed to stabilize the pendulum within acceptable bounds, indicating insufficient robustness for the extreme parameters (heavy, long pole, high friction). The system remained unstable throughout, suggesting poor tuning or inadequate energy management during transitions.\n**Program Identifier:** Generation 32 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with velocity- and angle-based adaptive gain scheduling to handle large initial deviations and high system instability from a heavy, long pole and high friction.  \n- **Performance**: Achieved a high combined score of 4469.91 with fast stabilization (254 steps), low energy use (total_energy: 6.18), and full success bonus.  \n- **Feedback**: The adaptive gains significantly improved robustness and speed of stabilization under challenging dynamics, while the precise friction modeling in the LQR design contributed to energy efficiency and stable final error near zero.\n**Program Identifier:** Generation 33 - Patch Name None - Correct Program: True\n\n**Program Name: Nonlinear Adaptive Energy-Shaping Controller**  \n- **Implementation**: Uses a composite control law combining nonlinear PD terms for angle/position, energy-shaping feedback, adaptive gain scheduling, and friction compensation to stabilize a challenging inverted pendulum (heavy, long pole; high friction).  \n- **Performance**: Achieved a combined score of 0.00 despite full simulation duration (1000 steps), with poor final stabilization (final_theta_error=1.44 rad, final_x_error=31.17 m) and no success bonus.  \n- **Feedback**: The controller fails to stabilize the pendulum upright\u2014likely due to aggressive energy shaping or insufficient coupling between cart position and pole dynamics\u2014resulting in divergence rather than recovery from large initial angles.\n**Program Identifier:** Generation 34 - Patch Name nonlinear_adaptive_controller - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must include functional code to address the problem requirements.\n**Program Identifier:** Generation 35 - Patch Name None - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform its intended task; implementation must include functional code to address the problem requirements.\n**Program Identifier:** Generation 36 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physics-matched continuous-time LQR controller with smooth, differentiable gain scheduling via tanh-based adaptive gains on position and velocity errors. The linearized dynamics matrix precisely aligns with the nonlinear simulator\u2019s structure, including consistent friction and denominator terms.  \n- **Performance**: Achieved a high combined score of 4878.27, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 198), and full success bonus.  \n- **Feedback**: The precise alignment between the controller\u2019s linear model and the simulator\u2019s physics enabled robust control despite challenging parameters (heavy/long pole, high friction). Adaptive gain scheduling improved transient response without introducing discontinuities, contributing to both stability and low energy use.\n**Program Identifier:** Generation 37 - Patch Name adaptive_lqr_friction_aware - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, incorporating friction terms matching the simulator, and applies smooth gain scheduling via tanh-based adaptation on angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4831.64 with fast stabilization (205 steps), low energy use (total_energy=6.26), and full success bonus.  \n- **Feedback**: The adaptive gains significantly improved robustness to large initial angles and high friction, enabling stable control of an otherwise highly unstable system; precise friction modeling in the LQR design was key to performance.\n**Program Identifier:** Generation 38 - Patch Name corrected_friction_and_aggressive_tuning - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model matching the simulator\u2019s physics to compute LQR gains, augmented with smooth tanh-based gain scheduling for large-angle recovery and swing-up logic when |\u03b8| > 0.85 rad.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization (final \u03b8 error = 1.59 rad, x error = 1408 m), despite full simulation runtime (1000 steps).  \n- **Feedback**: The controller fails to stabilize the pendulum\u2014likely because the swing-up logic applies incorrect force direction (based on sign(\u03b8\u00b7d\u03b8)) and the linear LQR is insufficient for highly nonlinear regimes; energy usage is high but ineffective.\n**Program Identifier:** Generation 39 - Patch Name add_swingup_trigger - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with smooth, multiplicative gain scheduling based on pole angle, angular velocity, and falling direction to enable faster stabilization under high-friction, heavy-pole conditions.  \n- **Performance**: Achieved a high combined score of 4666.63 with rapid stabilization (228 steps), low energy use (7.01 total), and perfect final error metrics.  \n- **Feedback**: The adaptive gains significantly improved response to large initial angles and instability while maintaining energy efficiency; the controller robustly handled challenging dynamics from increased mass, length, and friction.\n**Program Identifier:** Generation 40 - Patch Name aggressive_angular_damping_tuned_gains - Correct Program: True\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based modulation on angle and angular velocity errors to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a combined score of 4878.27 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but modest stabilization ratio (0.20) and base score (392.34).  \n- **Feedback**: The conservative LQR weights and high-friction dynamics limited responsiveness and precision, resulting in slower stabilization; however, the adaptive gains improved robustness enough to maintain balance for nearly the full duration despite challenging initial conditions.\n**Program Identifier:** Generation 41 - Patch Name optimized_lqr_weights_with_adaptive_gain - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control**  \n- **Implementation**: Uses an augmented-state LQR controller with integral action for zero steady-state error and adaptive gain scheduling based on angle/velocity error dominance; includes friction modeling and nonlinear dynamics.  \n- **Performance**: Scored 0.00 due to failure to stabilize the pendulum (final theta error: 2.95 rad, x error: >4000 m).  \n- **Feedback**: Despite sophisticated control design, the system diverges\u2014likely due to inaccurate linearization for large initial angles (0.9 rad) combined with aggressive or mis-tuned gains that destabilize rather than correct motion.\n**Program Identifier:** Generation 42 - Patch Name adaptive_lqr_integral - Correct Program: False\n\n**Program Name: Adaptive LQR for Inverted Pendulum**\n\n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling based on pole angle and angular velocity via smooth tanh transitions; system parameters reflect a challenging environment (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 4893.84 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) and base score (393.45).  \n- **Feedback**: The conservative LQR weights and adaptive gains enabled stability under difficult dynamics, yielding high time and energy bonuses, but limited responsiveness resulted in slower stabilization and lower precision.\n**Program Identifier:** Generation 43 - Patch Name optimized_adaptive_lqr_with_tanh_scheduling - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 44 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, enhanced with smooth gain scheduling via tanh-based adaptive terms that increase control effort during large deviations or falling-away dynamics.  \n- **Performance**: Achieved a combined score of 4657.90 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization was slow (stabilization_ratio: 0.23).  \n- **Feedback**: The adaptive gains improved robustness in a challenging high-friction, long-pole environment, but conservative base LQR weights limited responsiveness\u2014evident in low time bonus despite eventual stability.\n**Program Identifier:** Generation 45 - Patch Name optimized_q_weights_and_earlier_gain_activation - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with smooth, state-dependent adaptive gains based on angle and angular velocity to handle the highly unstable, high-friction pendulum dynamics.  \n- **Performance**: Achieved a combined score of 4839.16, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (204 steps).  \n- **Feedback**: The refined gain scheduling significantly improved robustness against large initial angles and high friction, enabling stable control despite aggressive system parameters; precise friction modeling in the LQR design was critical for real-world alignment.\n**Program Identifier:** Generation 46 - Patch Name adaptive_lqr_optimized_qr - Correct Program: True\n\n**Program Name: Discrete LQR with Integral Control**  \n- **Implementation**: Uses a discrete-time LQR controller derived from linearized dynamics via matrix exponential, augmented with integral action on cart position and adaptive gain based on angle error. The controller matches the simulator\u2019s physics in its linearization and applies smooth error-weighted blending for robustness.  \n- **Performance**: Achieved a combined score of 1116.13 with perfect stabilization ratio (1.00) over 1000 steps but failed to balance the pole (final_theta_error: 0.91), resulting in zero success bonus.  \n- **Feedback**: Despite strong energy efficiency and sustained simulation time, the controller fails to stabilize the pole upright\u2014likely due to excessive reliance on integral action causing drift and insufficient handling of nonlinearities in the highly unstable system (long, heavy pole with high friction).\n**Program Identifier:** Generation 47 - Patch Name discrete_lqr_integral_control - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a 5-state linearized model with integral control for position regulation, combined with nonlinear gain scheduling based on angle and angular velocity to adaptively scale control effort. Friction terms are accurately modeled in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4284.15 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (278 steps), and full success bonus.  \n- **Feedback**: The integral action effectively eliminated steady-state position error, while adaptive gains improved robustness during large deviations; accurate friction modeling and tuned LQR weights enabled stable control despite the challenging pole parameters.\n**Program Identifier:** Generation 48 - Patch Name lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control**  \n- **Implementation**: Uses a discrete-time LQR controller with integral action on cart position, augmented by sigmoid-based gain scheduling that blends angle- and velocity-dependent adaptive gains for smoother control.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly challenging pendulum (long, heavy pole; high friction; large initial angle), likely due to inaccuracies in linearization, improper integral handling, or insufficient robustness to nonlinear dynamics.\n**Program Identifier:** Generation 49 - Patch Name friction_aware_lqr_v2 - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model of the inverted pendulum to compute LQR gains, enhanced with smooth, differentiable gain scheduling via tanh-based adaptive scaling on position and velocity errors. The controller enforces physics-consistent linearization and applies force clipping for actuator limits.  \n- **Performance**: Achieved a high combined score of 4920.14, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192), and full success bonus.  \n- **Feedback**: The adaptive gain scheduling significantly improved stabilization speed and robustness under challenging conditions (heavy, long pole; high friction; large initial angle). Precise alignment between simulation dynamics and LQR linearization ensured stable control without divergence.\n**Program Identifier:** Generation 50 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Gains**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, then applies adaptive scaling via tanh-based gain scheduling and adds swing-up assistance for large angles.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and fails validation tests.  \n- **Feedback**: The controller\u2019s intentionally lazy response (high R, low Q) and aggressive physical parameters cause instability; adaptive gains and swing-up logic are insufficient to compensate for poor baseline LQR performance under high difficulty.\n**Program Identifier:** Generation 51 - Patch Name discrete_lqr_zoh - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a friction-aware linearized model for LQR gain computation with adaptive, differentiable gain scheduling based on angle and angular velocity errors via sigmoid blending.  \n- **Performance**: Achieved a combined score of 3917.04 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite challenging pole parameters.  \n- **Feedback**: The controller stabilizes the pendulum reliably but is intentionally \"lazy\" (low Q/R tuning), resulting in slower response (stabilization_ratio: 0.33) and moderate base score; adaptive gains help maintain stability without excessive force.\n**Program Identifier:** Generation 52 - Patch Name smooth_error_mode_blending - Correct Program: True\n\n**Program Name: Adaptive LQR with Synergy Gain**  \n- **Implementation**: Uses a high-precision linearized model matching the simulator's physics to compute LQR gains, enhanced with adaptive multiplicative gains based on angle, angular velocity, and falling-away detection via smooth tanh-based scheduling.  \n- **Performance**: Achieved a combined score of 4864.32 with full success bonus, fast stabilization (200 steps), and low energy use (avg 0.01 per step).  \n- **Feedback**: The controller\u2019s accurate system modeling and adaptive gains enabled robust stabilization of an extremely challenging (long, heavy, high-friction) pendulum, balancing speed and efficiency while preventing divergence during large initial angles.\n**Program Identifier:** Generation 53 - Patch Name enhanced_adaptive_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**\n\n- **Implementation**: Uses a linear-quadratic regulator (LQR) controller on an extended state space that includes an integral term for cart position, combined with smooth gain scheduling via tanh-based adaptive gains to handle nonlinearities and large initial errors.\n- **Performance**: Achieved a high combined score of 4502.04, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 248), and full success bonus.\n- **Feedback**: The integral action effectively eliminated steady-state position error, while adaptive gain scheduling improved robustness to the system's challenging dynamics (heavy, long pole with high friction). The controller maintained stability even under aggressive initial conditions.\n**Program Identifier:** Generation 54 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a discrete-time LQR controller extended with integral action for cart position regulation, enhanced by adaptive gain scheduling based on pole angle, angular velocity, and falling direction. The linearized dynamics model matches the simulator\u2019s physics, including consistent treatment of friction and denominator terms.  \n- **Performance**: Achieved a high combined score of 4383.78, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, stabilizing within 264 steps.  \n- **Feedback**: The adaptive gains and integral action effectively handled the challenging setup (heavy, long pole; high friction; large initial angle), enabling stable control while minimizing energy use. Precise alignment between the controller\u2019s linear model and the nonlinear simulator was critical to performance.\n**Program Identifier:** Generation 55 - Patch Name improved_lqr_with_discrete_time_and_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with suboptimal gains and smooth gain scheduling via tanh-based adaptive scaling on angle and angular velocity errors.  \n- **Performance**: Achieved a combined score of 4878.24 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20).  \n- **Feedback**: The conservative LQR tuning prioritizes stability over responsiveness, resulting in low energy use and high success rate but slower convergence (low time bonus relative to max), indicating room for improved transient performance through more aggressive control near instability thresholds.\n**Program Identifier:** Generation 56 - Patch Name increase_angular_velocity_weight_in_q_matrix - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced by smooth tanh-based gain scheduling that increases control effort for large angle/velocity errors.  \n- **Performance**: Achieves a combined score of 0.0 and fails validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly unstable pendulum (long, heavy pole with high friction), likely due to inaccuracies in linearization or insufficient robustness to nonlinear dynamics at large initial angles.\n**Program Identifier:** Generation 57 - Patch Name refine_q_weights - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced by smooth tanh-based gain scheduling that increases control effort for large angle/velocity errors.  \n- **Performance**: Achieved a high combined score of 4920.14, with strong time, energy, and success bonuses indicating stable, efficient balancing.  \n- **Feedback**: The adaptive gain scheduling significantly improved robustness on the challenging high-friction, long/heavy-pole setup, enabling fast stabilization (192 steps) and near-zero final errors while using minimal energy.\n**Program Identifier:** Generation 58 - Patch Name optimized_q_and_multiplicative_gains - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physics-consistent linearized model for LQR control with optimized state weights and smooth tanh-based gain scheduling that adapts to angle and angular velocity errors.  \n- **Performance**: Achieved a high combined score of 4920.13, with strong stabilization (success bonus = 800), low energy use (avg 0.01/step), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains improved transient response without introducing discontinuities, while precise alignment between the controller\u2019s A matrix and simulator dynamics was critical for stability under challenging conditions (heavy, long pole; high friction).\n**Program Identifier:** Generation 59 - Patch Name optimized_lqr_smooth_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR Controller for Inverted Pendulum**  \n- **Implementation**: Uses a friction-aware linearized model with discrete-time LQR and smooth tanh-based gain scheduling to adaptively adjust control effort based on angle and angular velocity errors.  \n- **Performance**: Achieved a combined score of 4872.34 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20).  \n- **Feedback**: The adaptive gains improved responsiveness to large initial angles while maintaining stability; however, the conservative Q/R tuning limited time efficiency despite low energy use.\n**Program Identifier:** Generation 60 - Patch Name discrete_lqr_with_refined_gains - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based modulation on angle and angular velocity errors to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a combined score of 4878.25 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) indicating slow convergence.  \n- **Feedback**: The conservative LQR weights and high friction tolerance enabled stability under challenging dynamics, yet the \"lazy\" control response limited time-based rewards; adaptive gains improved large-error handling without introducing discontinuities.\n**Program Identifier:** Generation 61 - Patch Name increase_angular_velocity_penalty - Correct Program: True\n\n**Program Name: Phase-Based Adaptive Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-detection mechanism to switch between four specialized control strategies (Recovery, Swing-Back, Capture, Centering), combining LQR near equilibrium with nonlinear adaptive gains and integral cart-position control. The controller dynamically blends base LQR actions with phase-specific forces based on real-time state analysis.  \n- **Performance**: Achieved a high combined score of 4739.13, with strong time, energy, and success bonuses, stabilizing the pole in 218 steps and maintaining low final errors.  \n- **Feedback**: The phase-based adaptation effectively handles the challenging dynamics of a heavy, long pole with high friction, enabling both aggressive recovery and precise stabilization. Energy efficiency and smooth phase transitions contributed significantly to the high performance.\n**Program Identifier:** Generation 62 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a discretized LQR controller with friction-aware linearization, integral action activated near upright via tanh scheduling, and adaptive gain blending based on angle/velocity error dominance.  \n- **Performance**: Achieved a combined score of 3219.32 with moderate time and energy bonuses but low stabilization ratio (0.33).  \n- **Feedback**: The conservative LQR weights and high friction tolerance led to sluggish response and poor stabilization; integral action helped reduce steady-state position error but wasn't sufficient to maintain balance under challenging initial conditions (0.9 rad).\n**Program Identifier:** Generation 63 - Patch Name add_integral_action_with_tanh_activation - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, then applies smooth tanh-based gain scheduling on angle and angular velocity to adaptively scale control effort.  \n- **Performance**: Achieves a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive gains, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction); likely due to inaccurate linearization or insufficient robustness to nonlinearities and large initial angles.\n**Program Identifier:** Generation 64 - Patch Name fine_tune_q_matrix - Correct Program: False\n\n**Program Name: Adaptive Discrete-Time LQR Controller**  \n- **Implementation**: Uses a discrete-time LQR controller derived from linearized continuous dynamics with Zero-Order Hold discretization, augmented with adaptive gain scheduling based on pole angle and angular velocity. The physics simulation includes high friction, a long/heavy pole, and large initial angle for increased difficulty.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite accurate modeling and adaptive gains, the controller fails to stabilize the highly unstable system\u2014likely due to excessive nonlinearity beyond the LQR\u2019s linear approximation range and insufficient robustness to large initial deviations.\n**Program Identifier:** Generation 65 - Patch Name adaptive_lqr_with_discrete_tuning - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linearized model for LQR control with optimized Q weights and smooth tanh-based gain scheduling that adapts to pole angle and angular velocity errors.  \n- **Performance**: Achieves a combined score of 4920.14 with full success bonus, low energy use (6.45 total), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains significantly improve robustness under challenging conditions (heavy/long pole, high friction), while the accurate linearization and tuned LQR weights enable precise stabilization with minimal control effort.\n**Program Identifier:** Generation 66 - Patch Name adaptive_lqr_tuned_q - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, incorporating friction terms matching the simulator, and applies adaptive gain scheduling based on angle and angular velocity errors to boost responsiveness during large deviations.  \n- **Performance**: Achieved a high combined score of 4920.14 with fast stabilization (192 steps), low energy use (avg 0.01 per step), and perfect final error metrics.  \n- **Feedback**: The controller\u2019s precise modeling of friction and intelligent gain adaptation enabled robust handling of a challenging, heavy, and long pole while maintaining energy efficiency and rapid stabilization.\n**Program Identifier:** Generation 67 - Patch Name optimized_q_matrix_and_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linear quadratic regulator (LQR) with physically accurate system matrices and proven optimal state weights; enhances robustness via multiplicative gain scheduling using tanh-based adaptation on angle and angular velocity.  \n- **Performance**: Achieved a combined score of 4920.13, including full success bonus, low energy usage (total_energy=6.44), and fast stabilization (192 steps).  \n- **Feedback**: The exact LQR tuning and smooth gain scheduling enabled stable control despite challenging parameters (heavy/long pole, high friction); minor Q-matrix adjustment (Q[3]=3.28) improved damping without sacrificing performance.\n**Program Identifier:** Generation 68 - Patch Name energy_based_sliding_mode_control - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with adaptive gain scheduling via smooth tanh-based blending of position and velocity gains, tailored to a challenging high-friction, long/heavy-pole environment.  \n- **Performance**: Achieved a combined score of 4893.84 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but moderate stabilization ratio (0.20) indicating slow convergence.  \n- **Feedback**: The conservative LQR weights and adaptive gains enable stability under difficult dynamics but result in sluggish response (low time bonus relative to max), suggesting room for tuning aggressiveness without sacrificing robustness.\n**Program Identifier:** Generation 69 - Patch Name optimize_lqr_weights_and_gain_scheduling - Correct Program: True\n\n**Program Name: Adaptive Swing-Up LQR with Integral Control**  \n- **Implementation**: Combines discrete-time LQR on an extended state (including integral of position), physics-informed swing-up assistance using hyperbolic tangent switching, and soft-switched integral action that deactivates far from equilibrium. Uses accurate linearized dynamics and zero-order hold discretization.  \n- **Performance**: Achieved a combined score of 0.00 due to failure to stabilize the pendulum within limits (final theta error: 2.81 rad, final x error: 3498 m).  \n- **Feedback**: Despite aggressive swing-up and integral terms, the controller failed to stabilize the highly challenging system (heavy, long pole with high friction); large steady-state errors suggest insufficient control authority or poor tuning for this extreme configuration.\n**Program Identifier:** Generation 70 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model to compute LQR gains with optimized state weights, augmented by smooth gain scheduling via tanh and physics-informed swing-up feedforward based on angle and angular velocity.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests despite design efforts for high difficulty conditions.  \n- **Feedback**: The controller\u2019s linearization may be inaccurate for large angles (e.g., 0.9 rad initial condition), and the swing-up logic might introduce destabilizing forces; friction modeling in the linearized A matrix appears inconsistent with the nonlinear simulator dynamics.\n**Program Identifier:** Generation 71 - Patch Name additive_swing_assist_feedforward - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with gain scheduling based on pole angle and angular velocity, plus an additive swing-up term that activates for large angles to counteract falling dynamics.  \n- **Performance**: Achieved a high combined score of 4920.15 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The adaptive gains and physics-informed swing-up assist effectively handle the challenging heavy/long pole configuration, enabling robust stabilization while minimizing energy use; precise friction modeling in both simulation and controller contributes to real-world fidelity and performance.\n**Program Identifier:** Generation 72 - Patch Name additive_swing_assist_with_proven_base - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a friction-aware linearized model to compute LQR gains with suboptimal Q/R weights, enhanced by smooth tanh-based gain scheduling that increases control effort only when angle or angular velocity errors exceed thresholds.  \n- **Performance**: Achieved a combined score of 4920.14 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The conservative LQR tuning ensures stability under challenging physics (long/heavy pole, high friction), while adaptive gains improve transient response without sacrificing smoothness; however, delayed aggressive correction limits time-based rewards.\n**Program Identifier:** Generation 73 - Patch Name none - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR Controller**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation combined with hybrid gain scheduling that blends linear and nonlinear responses based on angle and angular velocity magnitude; includes robust angle normalization and force clipping.  \n- **Performance**: Achieved a high combined score of 4865.75 with strong energy efficiency, full success bonus, and excellent stabilization.  \n- **Feedback**: The adaptive gain scheduling significantly improved control responsiveness for large deviations while maintaining precision near equilibrium, enabling stable performance even under challenging conditions (heavy, long pole with high friction and large initial angle).\n**Program Identifier:** Generation 74 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**\n\n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based smooth transitions and additive physics-informed swing-up assistance for large angles. The system parameters reflect a highly unstable pendulum (long, heavy pole; high friction).\n\n- **Performance**: Achieves an initial score near 3000 but fails to stabilize consistently under validation tests, resulting in a combined score of 0.0.\n\n- **Feedback**: Despite sophisticated control logic, the controller is too \"lazy\" due to conservative LQR weights (low Q, high R), leading to insufficient correction for the challenging dynamics. Validation likely fails because the pendulum exceeds the 1.0 rad failure threshold before stabilization.\n**Program Identifier:** Generation 75 - Patch Name optimized_q_with_swing_assist - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with suboptimal gains, enhanced by physics-informed swing-up logic and adaptive gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 4724.67, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite slow stabilization (stabilization_ratio: 0.22).  \n- **Feedback**: The conservative LQR tuning prioritizes low energy use over fast response, resulting in high energy and success bonuses but modest time and base scores; the swing-up assist effectively handles large initial angles in a challenging high-friction, long-pole environment.\n**Program Identifier:** Generation 76 - Patch Name enhanced_swing_up_assist_with_adaptive_gain - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with tanh-based gain scheduling and additive swing-up control for large angles (>0.8 rad) to handle highly unstable configurations (long, heavy pole).  \n- **Performance**: Achieved a high combined score of 4920.15, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (192 steps).  \n- **Feedback**: The hybrid approach effectively balances optimal linear control near equilibrium with non-linear corrective actions during large deviations, enabling robust performance under extreme physical parameters; angle normalization and friction-aware dynamics modeling contributed to numerical stability and realism.\n**Program Identifier:** Generation 77 - Patch Name enhanced_swingup_assist_and_q_optimization - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in no functional logic or algorithm implementation.  \n- **Performance**: Combined score is 0.0, indicating complete failure to meet validation requirements.  \n- **Feedback**: The absence of any implementation means the program cannot process inputs, produce outputs, or satisfy basic correctness criteria. A valid solution must include core logic aligned with the problem specification.\n**Program Identifier:** Generation 78 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assistance**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based activation and continuous swing-up assistance for large angles, tailored to a highly unstable pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a combined score of 4920.19, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, despite slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The \"lazy\" LQR weights (low Q, high R) prioritize energy over speed, limiting time bonus but enabling robust stability; the smooth, differentiable swing-up logic effectively handles large initial deviations without abrupt control switches.\n**Program Identifier:** Generation 79 - Patch Name continuous_falling_factor_swing_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized friction-aware LQR controller with adaptive gain scheduling via tanh-based smooth transitions and an additive swing-up term for large angles (>0.8 rad), tailored to a challenging pendulum (long, heavy pole; high friction).  \n- **Performance**: Achieved a combined score of 4920.22, with strong energy efficiency (avg_energy_per_step=0.01) and full success bonus, but slow stabilization (stabilization_ratio=0.19).  \n- **Feedback**: The conservative LQR weights (low Q, high R) prioritize low energy use over fast response, resulting in delayed stabilization; however, the swing-up assist effectively recovers from large initial deviations, enabling consistent success despite difficult dynamics.\n**Program Identifier:** Generation 80 - Patch Name falling_severity_swing_assist - Correct Program: True\n\n**Program Name: Adaptive Momentum Compensation Controller**  \n- **Implementation**: Uses phase-aware switching between swing-up, transition, and fine-tuning control modes with momentum compensation, energy shaping, nonlinear damping, and friction compensation; includes integral action near equilibrium and smooth force limiting.  \n- **Performance**: Achieved a combined score of 0.00 due to failure to stabilize the pendulum (final theta error: 1.81 rad, x error: 20.07 m), despite high energy usage (total_energy: 123.20).  \n- **Feedback**: The controller fails to balance the highly unstable system (long, heavy pole with high friction); aggressive swing-up terms may destabilize early dynamics, and LQR-like gains appear insufficient for stabilization under these extreme parameters.\n**Program Identifier:** Generation 81 - Patch Name adaptive_momentum_compensation_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-consistent linearized LQR controller with adaptive gain scheduling and a momentum-based swing-up assist triggered for large angles (>0.8 rad), using normalized angular momentum and smooth tanh activations for differentiable control adjustments.  \n- **Performance**: Achieved a high combined score of 4920.12, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00), stabilizing in 192 steps.  \n- **Feedback**: The swing-up assist effectively handles the challenging initial condition (0.9 rad) with a heavy, long pole, while precise friction modeling and consistent linearization between simulation and LQR design ensure robust stabilization and minimal steady-state error.\n**Program Identifier:** Generation 82 - Patch Name none - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Action**  \n- **Implementation**: Combines a physically accurate linearized LQR controller with conditional integral action (activated near equilibrium), robust angle normalization, smooth gain scheduling via tanh, and physics-informed swing-up assist for large angles.  \n- **Performance**: Achieved a high combined score of 4691.82 with full success bonus, low energy use (avg 0.01/step), and rapid stabilization (224 steps).  \n- **Feedback**: The hybrid approach effectively balances optimal linear control near equilibrium with non-linear corrections during large deviations, enabling stable control despite challenging parameters (heavy/long pole, high friction). Conditional integration prevents windup while improving steady-state cart position accuracy.\n**Program Identifier:** Generation 83 - Patch Name hybrid_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Gated Integral Control**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, augmented with a gated integral term on cart position that activates only near upright equilibrium, and applies smooth gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 1278.20, with perfect stabilization ratio (1.00) over 1000 steps but zero success bonus due to failure to meet terminal position/angle thresholds.  \n- **Feedback**: Despite excellent energy efficiency and stability maintenance, the controller fails to center the cart (final_x_error = 25.87), indicating the integral action is insufficiently aggressive or misaligned with the true control objective; the lack of success bonus suggests the final state does not satisfy evaluation criteria for \"solved\" episodes.\n**Program Identifier:** Generation 84 - Patch Name adaptive_lqr_with_integrated_correction - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model to compute LQR gains, augmented with tanh-based gain scheduling, soft integral control, and swing-up assistance for large angles. The controller adapts force output based on pole angle and angular velocity deviations.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests despite sophisticated design.  \n- **Feedback**: Overly aggressive gain scheduling and incorrect swing-up logic likely destabilize the system under high-difficulty conditions (heavy/long pole, large initial angle). The integral term and friction modeling may also introduce instability rather than robustness.\n**Program Identifier:** Generation 85 - Patch Name soft_integral_cart_centering - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with adaptive gain scheduling based on angle and angular velocity, plus swing-up assistance for large deviations. Friction terms in the dynamics are carefully matched between simulation and controller.  \n- **Performance**: Achieved a high combined score of 4920.22 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The controller excels in balancing aggressive correction for large errors with minimal energy use near equilibrium; precise modeling of friction and well-tuned LQR weights were critical to success.\n**Program Identifier:** Generation 86 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-informed linearized LQR controller with adaptive gain scheduling and phase-aware swing-up assistance based on normalized angular momentum and pole dynamics. Uses precise friction modeling, consistent linearization around upright equilibrium, and smooth exponential transitions for gain adaptation.  \n- **Performance**: Achieved a high combined score of 4635.10 with full success bonus, low energy usage (avg 0.01/step), and fast stabilization (232 steps).  \n- **Feedback**: The swing-up assist effectively handles large initial deviations (0.9 rad) despite challenging pole parameters (heavy, long, high friction), while adaptive gains improve robustness without destabilizing the LQR core. Energy efficiency and stability were maintained through careful physics-consistent design.\n**Program Identifier:** Generation 87 - Patch Name phase_aware_swing_assist - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Action**  \n- **Implementation**: Combines a physically accurate linearized LQR controller with adaptive gain scheduling, swing-up assist for large angles, and soft-switched integral action to eliminate steady-state cart position error. Uses robust angle normalization and Euler integration matching the simulator dynamics exactly.  \n- **Performance**: Achieved a combined score of 1308.26 with perfect stabilization (1000 steps), low average energy per step (0.01), and high energy efficiency (energy_bonus: 2492.28).  \n- **Feedback**: The hybrid approach effectively stabilizes an extremely challenging system (heavy, long pole with high friction and large initial angle). Integral action and swing-up logic prevent drift and aid recovery, while adaptive gains enhance responsiveness without destabilizing the system near equilibrium.\n**Program Identifier:** Generation 88 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Combines energy-shaping swing-up control with LQR-based stabilization, using phase-based switching and friction compensation; includes adaptive gains and predictive terms for improved response.  \n- **Performance**: Achieved high energy usage (2499.93) but failed to stabilize the pendulum (final theta error: 2.93 rad), resulting in zero success bonus and low combined score (0.00).  \n- **Feedback**: Despite sophisticated control logic, the controller could not stabilize the highly challenging system (heavy, long pole with high friction); likely due to inaccurate dynamics modeling or insufficient robustness in transition between control phases.\n**Program Identifier:** Generation 89 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR Inverted Pendulum Controller**  \n- **Implementation**: Uses a suboptimal LQR controller with friction-aware linearization, adaptive gain scheduling via smooth tanh-based modulation, and soft-switched integral action for cart centering.  \n- **Performance**: Achieved a combined score of 1196.58 with perfect stabilization ratio (1.00) over 1000 steps but low base accuracy due to high position errors.  \n- **Feedback**: The conservative LQR tuning (low Q, high R) prioritizes stability over precision, resulting in high energy efficiency but poor final/state error metrics; adaptive gains help maintain balance under challenging dynamics (long/heavy pole, high friction).\n**Program Identifier:** Generation 90 - Patch Name add_soft_cart_integral - Correct Program: True\n\n**Program Name: Adaptive LQR with Synergy Gain**  \n- **Implementation**: Uses a high-precision discrete-time LQR controller derived from physically consistent linearized dynamics, enhanced with smooth adaptive gains based on pole angle, angular velocity, and falling-away detection via tanh-based scheduling.  \n- **Performance**: Achieved a combined score of 4842.54 with full success bonus, low energy use (5.86 total), and fast stabilization (203 steps).  \n- **Feedback**: The synergy gain effectively prevents divergence during large-angle excursions, while accurate friction-aware linearization ensures robustness; the adaptive gains significantly improve damping without destabilizing the system.\n**Program Identifier:** Generation 91 - Patch Name discrete_time_lqr_with_tuned_gains - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a high-precision linearized LQR controller derived from consistent physics equations, enhanced with smooth tanh-based gain scheduling that adaptively increases control effort for large angle or angular velocity errors.  \n- **Performance**: Achieved a combined score of 4920.13 with full success bonus, fast stabilization (192 steps), and very low energy use (total_energy=6.44).  \n- **Feedback**: The adaptive gains significantly improved stabilization speed without sacrificing energy efficiency, while precise alignment between the controller\u2019s linear model and the nonlinear simulator ensured robustness under challenging conditions (heavy, long pole; high friction; large initial angle).\n**Program Identifier:** Generation 92 - Patch Name increase_angular_velocity_weight - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a friction-aware linearized LQR controller with gain scheduling based on pole angle and angular velocity, plus an additive swing-up assist for large deviations (>0.8 rad) to recover from near-failure states.  \n- **Performance**: Achieved a combined score of 4920.15, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization was slow (stabilization_ratio: 0.19).  \n- **Feedback**: The adaptive gains and swing-up logic effectively prevented failure and enabled eventual stabilization despite aggressive initial conditions and challenging physics; however, conservative base LQR weights limited responsiveness, resulting in low time bonus and delayed convergence.\n**Program Identifier:** Generation 93 - Patch Name multiplicative_gain_with_swing_assist - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Energy Assist**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance and adaptive gain scheduling, using physically accurate system linearization and friction-aware dynamics.  \n- **Performance**: Achieved a high combined score of 4920.21 with strong stabilization (success_bonus: 800), low energy use (avg_energy_per_step: 0.01), and fast stabilization (stabilization_time: 192).  \n- **Feedback**: The controller effectively handles large initial angles and high system instability due to a heavy, long pole; energy-efficient swing-up logic and precise LQR tuning enable both rapid recovery and stable balance.\n**Program Identifier:** Generation 94 - Patch Name robust_swingup_lqr - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a high-precision linearized LQR controller with optimized Q weights, adaptive gain scheduling via tanh-based activation, swing-up assistance for large angles, and soft integral action on cart position with anti-windup.  \n- **Performance**: Achieved a combined score of 4913.35 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (193 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging heavy/long pole and high friction, while integral control improved steady-state positioning without causing windup during transients.\n**Program Identifier:** Generation 95 - Patch Name swing_up_assist_with_integral - Correct Program: True\n\n**Program Name: Phase-Based Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-based control architecture with smooth transitions between recovery, transition, and precision phases; integrates LQR gains computed from friction-aware linearization, adaptive swing-up assistance for large deviations, and gated integral action for cart centering.  \n- **Performance**: Achieved a high combined score of 4845.21 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (203 steps), and perfect final pole angle (0.00 rad).  \n- **Feedback**: The phase-blending strategy effectively handles the challenging dynamics of a heavy, long pole with high friction, while anti-windup and upright-gated integral action prevent instability and ensure precise steady-state positioning.\n**Program Identifier:** Generation 96 - Patch Name phase_based_controller - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control derived from physically accurate linearized dynamics with adaptive gain scheduling and a physics-informed swing-up assist for large angles, using smooth tanh-based activation and falling severity estimation.  \n- **Performance**: Achieved a high combined score of 4920.19 with full success bonus, low energy usage (total_energy=6.48), and rapid stabilization (stabilization_time=192).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the highly unstable configuration (heavy, long pole; high friction); precise angle normalization and divergence-aware assistance enabled robust recovery and stable balancing.\n**Program Identifier:** Generation 97 - Patch Name adaptive_lqr_with_swing_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized model with physically accurate friction terms to compute LQR gains, augmented with tanh-based adaptive gain scheduling and soft-switched integral action on cart position.  \n- **Performance**: Achieved a combined score of 0.0, failing all validation tests.  \n- **Feedback**: Despite sophisticated control design, the controller fails to stabilize the highly unstable pendulum (long, heavy pole with high friction); likely due to inaccurate linearization, poor handling of nonlinear dynamics, or inadequate robustness to large initial angles.\n**Program Identifier:** Generation 98 - Patch Name robust_arctan2_angle_norm - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a suboptimal LQR controller with gain scheduling, integral action gated near upright, and physics-informed swing-up assistance for large angles. The linearized model accounts for friction, and control gains are adaptively scaled based on pole angle and angular velocity.  \n- **Performance**: Achieved a combined score of 1313.02 with perfect stabilization ratio (1.0) over 1000 steps but low base accuracy due to conservative tuning.  \n- **Feedback**: High energy efficiency (avg 0.01 per step) and full time utilization indicate stable balancing, yet poor position/angle precision (final_x_error=7.98, stable_x_error=2.79) reveals intentionally lazy control from low Q/high R weights, limiting base score despite robustness.\n**Program Identifier:** Generation 99 - Patch Name adaptive_lqr_with_integral_action - Correct Program: True\n\n**Program Name: Adaptive LQR for Inverted Pendulum Control**  \n- **Implementation**: Uses a suboptimal LQR controller with friction-aware linearization and smooth gain scheduling via tanh-based adaptive gains to handle large pole angles and high friction.  \n- **Performance**: Achieves a combined score of 4920.14 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus despite challenging physics (long, heavy pole; high friction).  \n- **Feedback**: The conservative LQR weights result in slower response (low time bonus relative to max), but adaptive gains improve stability at large errors without discontinuities, enabling consistent stabilization within 192 steps.\n**Program Identifier:** Generation 100 - Patch Name update_to_best_lqr_weights - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a friction-aware linearized model for LQR gain computation, enhanced with smooth gain scheduling via tanh-based adaptive gains, swing-up assistance for large angles, and soft-switched integral action on cart position to improve steady-state accuracy.  \n- **Performance**: Achieved a combined score of 4920.10 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, though stabilization ratio was low (0.19).  \n- **Feedback**: The controller effectively balances the challenging pendulum (long, heavy pole; high friction) and achieves stability, but slow response due to conservative LQR weights limits time bonus; integral action and adaptive gains help maintain precision once stabilized.\n**Program Identifier:** Generation 101 - Patch Name integral_action_for_zero_drift - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR gain computation, enhanced with adaptive gain scheduling via tanh-based blending, swing-up assist for large angles, and gated integral action for position error correction.  \n- **Performance**: Achieved a combined score of 1308.25 with perfect stabilization (1000 steps, stabilization ratio 1.0) and very low energy usage (avg 0.01 per step).  \n- **Feedback**: The controller successfully stabilizes an extremely challenging inverted pendulum (long, heavy pole; high friction; large initial angle), demonstrating robustness through hybrid nonlinear adaptations while maintaining energy efficiency\u2014though it incurs a positional drift (final x_error = 7.01), indicating room for improved steady-state accuracy.\n**Program Identifier:** Generation 102 - Patch Name increase_angular_velocity_damping - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**  \n- **Implementation**: Uses a friction-aware linearized model for LQR control with adaptive gain scheduling, integral action gated by pole angle, and physics-informed swing-up assistance for large deviations.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and environmental difficulty, failing validation tests with a combined score of 0.0.  \n- **Feedback**: The controller\u2019s deliberate \"laziness\" (low Q/high R weights) causes slow response and poor precision; despite advanced features like gain scheduling and swing-up logic, it cannot stabilize the highly unstable system within required limits.\n**Program Identifier:** Generation 103 - Patch Name robust_angle_norm - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute discrete-time LQR gains, enhanced by adaptive gain scheduling based on pole angle and angular velocity, plus swing-up assistance for large deviations.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: Despite sophisticated modeling and adaptive control logic, the controller fails to stabilize the highly unstable system (long, heavy pole with high friction), likely due to inaccuracies in linearization or insufficient robustness in the gain adaptation under extreme initial conditions.\n**Program Identifier:** Generation 104 - Patch Name discrete_lqr_zoh - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Uses a high-precision linearized LQR controller with optimized Q weights, adaptive gain scheduling via tanh-based activation, swing-up assistance for large angles, and soft integral action on cart position with anti-windup and gating near equilibrium.  \n- **Performance**: Achieved a combined score of 4912.79 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (193 steps), and full success bonus.  \n- **Feedback**: The combination of physically consistent linearization, adaptive gains, and targeted swing-up logic effectively handles the challenging heavy/long pole and high-friction dynamics; integral action improves steady-state positioning without destabilizing transients.\n**Program Identifier:** Generation 105 - Patch Name integral_action_refinement - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 106 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a friction-aware linearized model for LQR control, enhanced with smooth gain scheduling via tanh, swing-up assist for large angles, and anti-windup integral action on cart position.  \n- **Performance**: Achieved a combined score of 4920.09 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle large initial angles and prevent failure, while conservative LQR weights and integral action improve steady-state accuracy without destabilizing the system. However, the controller\u2019s sluggish response limits time-based rewards.\n**Program Identifier:** Generation 107 - Patch Name integral_anti_windup - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with friction-aware dynamics, combined with gain scheduling based on pole angle/velocity and an additive swing-up term for large deviations.  \n- **Performance**: Achieved a high combined score of 4920.20 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The adaptive gains and physics-informed swing-up assist effectively handle the challenging heavy/long pole configuration, enabling robust recovery from large initial angles while minimizing energy use.\n**Program Identifier:** Generation 108 - Patch Name none - Correct Program: True\n\n**Program Name: Phase-Predictive LQR Controller**  \n- **Implementation**: Uses a discretized LQR controller with phase-aware switching between recovery, transition, and precision modes; includes predictive correction, swing-up assistance, and integral action for steady-state accuracy.  \n- **Performance**: Achieved a high combined score of 4782.29 with strong energy efficiency, fast stabilization (212 steps), and perfect final error metrics.  \n- **Feedback**: The adaptive phase logic and predictive corrections effectively handled the challenging high-friction, long-pole dynamics, enabling robust swing-up and precise stabilization while minimizing control effort.\n**Program Identifier:** Generation 109 - Patch Name phase_predictive_lqr - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action**  \n- **Implementation**: Uses a linearized friction-aware model for LQR control with adaptive gain scheduling via tanh-based smooth switching, integral action for position regulation (with anti-windup), and physics-informed swing-up assistance for large angles.  \n- **Performance**: Achieved a combined score of 4920.10 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus, but slow stabilization (stabilization_ratio: 0.19).  \n- **Feedback**: The controller effectively balances the challenging high-friction, long/heavy-pole system and avoids failure, but its conservative gains and delayed response limit time-based rewards despite excellent steady-state accuracy.\n**Program Identifier:** Generation 110 - Patch Name integral_action_with_soft_switching - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Gains**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, augmented with smooth tanh-based gain scheduling and soft-switched integral action for cart position.  \n- **Performance**: Achieves low initial score (~3000) due to conservative tuning and fails validation tests under challenging physics (long/heavy pole, high friction).  \n- **Feedback**: The controller\u2019s \"lazy\" Q/R weights and insufficient robustness to large initial angles cause poor stabilization; adaptive gains help but don\u2019t compensate for fundamental underperformance in hard conditions.\n**Program Identifier:** Generation 111 - Patch Name increase_dtheta_weight - Correct Program: False\n\n**Program Name: Energy-Phase Adaptive LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based adaptive gains, phase-specific strategies (recovery, precision, predictive damping), and integral action with anti-windup for steady-state accuracy. The controller dynamically adjusts force output based on normalized mechanical energy, angular velocity, and state magnitude to handle a highly unstable, heavy, and long inverted pendulum.  \n- **Performance**: Achieved a high combined score of 4755.01 with strong bonuses for success, time, and energy efficiency, stabilizing the system in 215 steps with near-zero final errors.  \n- **Feedback**: The adaptive gain scheduling and phase-aware logic effectively managed the challenging dynamics (high mass, length, friction), while predictive damping and integral control ensured precise stabilization without overshoot or windup.\n**Program Identifier:** Generation 112 - Patch Name energy_phase_controller - Correct Program: True\n\n**Program Name: Adaptive Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-based hybrid controller with energy-based swing-up for large angles, blended transition control, and LQR with integral action for stabilization; includes nonlinear modifications like velocity damping and control authority scaling.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization despite high energy usage (total_energy=8.22) and full simulation duration (1000 steps).  \n- **Feedback**: The controller failed to stabilize the pendulum (final_theta_error=2.77 rad), indicating ineffective mode transitions or insufficient robustness for the highly unstable system (long, heavy pole with high friction); integral windup or aggressive swing-up may have hindered convergence.\n**Program Identifier:** Generation 113 - Patch Name none - Correct Program: False\n\n**Program Name: Feedback Linearization with Adaptive LQR**  \n- **Implementation**: Combines discrete LQR control on a linearized model with exact nonlinear inverse dynamics to track a desired angular acceleration, augmented by adaptive gain scheduling and gated integral action for position regulation.  \n- **Performance**: Achieves a combined score of 0.0, failing validation tests.  \n- **Feedback**: The controller struggles with the highly unstable, high-friction, and heavy/long pole configuration; likely due to singularities in inverse dynamics near \u00b1\u03c0/2 and insufficient robustness in the feedback linearization under large initial angles (0.9 rad). The adaptive gains and integral term may not compensate adequately for model inaccuracies or aggressive nonlinearities.\n**Program Identifier:** Generation 114 - Patch Name fbl_discretelqr_integral - Correct Program: False\n\n**Program Name: Hybrid Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with optimized Q-matrix weights, enhanced by adaptive gain scheduling and a continuous swing-up assist that activates based on angle magnitude and falling severity.  \n- **Performance**: Achieved a combined score of 4920.22 with perfect stabilization (final theta error = 0.00) and high energy efficiency (avg_energy_per_step = 0.01).  \n- **Feedback**: The controller effectively balances aggressive correction for large deviations with fine-tuned stabilization near upright, while the swing-up logic ensures recovery from highly unstable initial conditions; the exact match between simulation dynamics and LQR model was critical to success.\n**Program Identifier:** Generation 115 - Patch Name none - Correct Program: True\n\n**Program Name: Suboptimal LQR with Adaptive Swing-Up**  \n- **Implementation**: Uses a linearized friction-aware model to compute LQR gains, then applies adaptive gain scheduling via tanh-based scaling and adds swing-up assistance for large angles (>0.8 rad).  \n- **Performance**: Achieves low initial score (~3000) due to conservative Q/R tuning and insufficient robustness in highly unstable conditions.  \n- **Feedback**: The controller fails validation tests because the linearized LQR is inadequate for the highly nonlinear, high-friction, long-pole regime; swing-up logic lacks smooth integration and doesn\u2019t prevent early failure beyond 1.0 rad.\n**Program Identifier:** Generation 116 - Patch Name fix_duplicate_init_remove_integral - Correct Program: False\n\n**Program Name: Phase-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Uses a three-phase control strategy (Recovery, Transition, Precision) with physics-informed LQR gains, adaptive swing-up logic, and anti-windup integral action for steady-state error correction.  \n- **Performance**: Achieved a high combined score of 4760.24 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps.  \n- **Feedback**: The aggressive recovery phase effectively handles large initial angles, while the gated integral term ensures precise stabilization without windup; high friction and pole mass make this a robust solution under challenging dynamics.\n**Program Identifier:** Generation 117 - Patch Name adaptive_lqr_with_integral_correction - Correct Program: True\n\n**Program Name: Adaptive Energy-Phase Inverted Pendulum Controller**  \n- **Implementation**: Combines LQR-based linear control with energy-aware swing-up logic, phase-specific gains, integral action for precision stabilization, and predictive damping to handle a highly unstable pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a high combined score of 4755.01 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps with minimal final errors.  \n- **Feedback**: The adaptive gain scheduling and phase-aware switching effectively managed the challenging dynamics, while integral control and predictive damping significantly improved steady-state accuracy and reduced overshoot near equilibrium.\n**Program Identifier:** Generation 118 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Uses an enhanced adaptive LQR controller with integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, and a swing-up assist activated during large deviations. The dynamics include high-friction, heavy/long pole parameters for increased difficulty.  \n- **Performance**: Achieved a combined score of 4927.10 with full success bonus, low energy usage (6.46 total), and stabilization after 191 steps.  \n- **Feedback**: The integral control effectively eliminated steady-state cart error when upright, while the swing-up logic and adaptive gains enabled recovery from highly unstable initial conditions (0.9 rad). Energy efficiency and stability were maintained despite aggressive physical challenges.\n**Program Identifier:** Generation 119 - Patch Name adaptive_lqr_integral_swingup - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines a physics-informed linear quadratic regulator (LQR) with adaptive gain scheduling and a smooth, momentum-based swing-up assist for large angles; includes soft-switched integral action to eliminate steady-state cart position error.  \n- **Performance**: Achieved a high combined score of 4920.25, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and perfect upright maintenance (final_theta_error: 0.00).  \n- **Feedback**: The swing-up assist effectively handles the challenging initial condition (0.9 rad) with a heavy, long pole, while adaptive gains and integral action improve robustness and precision without destabilizing the system.\n**Program Identifier:** Generation 120 - Patch Name increase_dtheta_q_weight - Correct Program: True\n\n**Program Name: Phase-Adaptive Discrete LQR Controller**  \n- **Implementation**: Uses discrete-time LQR with zero-order hold discretization, enhanced by phase-based swing-up assistance, adaptive gain scheduling, filtered state estimation, and integral control activated only during fine-tuning.  \n- **Performance**: Achieved a combined score of 0.0 and failed validation tests.  \n- **Feedback**: The controller\u2019s complex phase logic and energy-based swing-up may be misaligned with the highly unstable system dynamics (heavy, long pole); discretization via an incorrect matrix exponential approximation likely introduced instability, and aggressive clipping or filtering may degrade responsiveness.\n**Program Identifier:** Generation 121 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a physics-informed linearized model for LQR control with adaptive gain scheduling based on pole angle and velocity, plus swing-up assistance for large deviations and leaky integral action for cart position regulation.  \n- **Performance**: Achieved a high combined score of 4920.25 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 192 steps).  \n- **Feedback**: The controller effectively balances aggressive stabilization for large angles with precise upright maintenance, while friction-aware modeling and smooth gain transitions prevent instability and energy waste.\n**Program Identifier:** Generation 122 - Patch Name add_swing_up_assist_and_integral_control - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state error and a physics-informed swing-up assist that activates for large pole angles, using smooth gain scheduling via tanh-based activation functions.  \n- **Performance**: Achieved a high combined score of 4920.25, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging dynamics (heavy, long pole; high friction), while integral control ensured precise cart positioning once upright\u2014demonstrating robustness and efficiency in both transient and steady-state phases.\n**Program Identifier:** Generation 123 - Patch Name Add integral control for cart position and swing-up assist for large angles - Correct Program: True\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control, enhanced with adaptive gain scheduling based on angle and angular velocity, plus physics-informed swing-up assistance for large deviations.  \n- **Performance**: Achieved a high combined score of 4898.88 with strong energy efficiency (avg_energy_per_step: 0.01) and fast stabilization (stabilization_time: 195 steps).  \n- **Feedback**: The controller effectively balances aggressive correction for large errors with energy-efficient fine control near equilibrium; the swing-up assist and multiplicative gain adaptation were key to handling the challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 124 - Patch Name discrete_lqr_design - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance using predictive angle estimation, adaptive gain scheduling via tanh activation, and soft-switched integral action for steady-state error correction. The controller uses accurate linearized dynamics matching the nonlinear simulator and includes friction-aware modeling.  \n- **Performance**: Achieved a high combined score of 4872.08 with strong time, energy, and success bonuses, stabilizing the pendulum in 199 steps with minimal final errors.  \n- **Feedback**: The swing-up assist effectively handles large initial angles (0.9 rad), while adaptive gains and integral action improve robustness and precision; low average energy per step (0.01) indicates efficient control despite challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 125 - Patch Name predictive_feedforward_swing - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Energy Assist**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart positioning; uses physically accurate linearization and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4913.06 with full success bonus, low energy use (avg 0.01/step), and fast stabilization (193 steps).  \n- **Feedback**: The controller effectively handles the challenging pole parameters (heavy, long) by intelligently switching between swing-up and stabilization modes; integral control and adaptive gains ensure precise cart centering and robustness without excessive energy expenditure.\n**Program Identifier:** Generation 126 - Patch Name add_cart_vel_cross_coupling - Correct Program: True\n\n**Program Name: Discrete LQR with Energy-Aware Swing-Up**\n\n- **Implementation**: Uses a discrete-time LQR controller derived via zero-order hold to match Euler integration dynamics, enhanced with energy-based swing-up logic and adaptive integral control that decays aggressively when the pole is far from upright. The controller applies gain scheduling based on angle and angular velocity, and only activates swing-up assistance when energy falls below 85% of the upright potential energy.\n\n- **Performance**: Achieved a high combined score of 4892.07, with strong contributions from time bonus (2206.23), energy efficiency (2493.47), and full success bonus (800.00), stabilizing in 196 steps with near-zero final errors.\n\n- **Feedback**: The energy-aware swing-up prevents unnecessary force application while effectively recovering from large initial angles; adaptive integral decay and gain scheduling significantly improve robustness under challenging conditions (heavy, long pole with high friction).\n**Program Identifier:** Generation 127 - Patch Name discrete_lqr_energy_adaptive - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action for cart position, and physics-informed swing-up assistance triggered at large angles (>0.8 rad). Friction is accurately modeled in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4933.93 with excellent energy efficiency (avg_energy_per_step: 0.01) and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up assist effectively stabilized the highly challenging pendulum (long, heavy pole; large initial angle), while integral action ensured precise long-term positioning without compromising stability.\n**Program Identifier:** Generation 128 - Patch Name none - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) stabilization around the upright position with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart position error elimination; uses physically accurate linearization and directional momentum awareness to enhance swing-up efficiency.  \n- **Performance**: Achieved a high combined score of 4927.07 with full success bonus, low energy usage (total_energy=6.50), and fast stabilization (191 steps).  \n- **Feedback**: The controller effectively balances aggressive swing-up and precise stabilization, with integral action and adaptive gains contributing to zero final errors and robustness under challenging conditions (heavy/long pole, high friction).\n**Program Identifier:** Generation 129 - Patch Name directional_momentum_integral_control - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed gain scheduling, swing-up assistance for large angles, and soft-switched integral action to eliminate steady-state cart position error. The controller uses accurate linearized dynamics matching the simulator and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4920.25 with full success bonus, low energy use (total_energy=6.47), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging pole parameters (heavy, long, high friction), while integral control ensured precise upright stabilization without windup due to leaky integration and gating.\n**Program Identifier:** Generation 130 - Patch Name enhanced_swing_up_with_integral_control - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action gated by angle/velocity thresholds, and physics-informed swing-up assistance for large deviations. Friction-aware system modeling ensures accurate controller synthesis.  \n- **Performance**: Achieved a high combined score of 4926.67 with excellent energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up assist effectively handle the challenging heavy/long pole configuration, while gated integral action prevents destabilization during transients\u2014key to balancing speed, stability, and energy use.\n**Program Identifier:** Generation 131 - Patch Name joint_integral_gate_and_tuned_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed adaptive gain scheduling, swing-up assistance for large angles, and gated integral action for zero steady-state cart position error. The controller uses robust angle normalization, smooth activation functions (tanh), and leaky integration with angle-dependent decay.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and near-perfect stabilization.  \n- **Feedback**: The enhanced swing-up logic and adaptive gains effectively handled the challenging pole parameters (longer, heavier), while the gated integral term improved long-term positioning without destabilizing the system during large swings.\n**Program Identifier:** Generation 132 - Patch Name integral_windup_prevention_with_angle_velocity_gate - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines gain-scheduled LQR with robust swing-up assist, enhanced angular damping, and gated integral action that activates only near upright equilibrium to minimize drift.  \n- **Performance**: Achieved a high combined score of 4926.67 with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00).  \n- **Feedback**: The controller effectively stabilizes a highly challenging pendulum (long, heavy, high friction) by blending aggressive swing-up logic with precise integral correction, though stabilization ratio remains low (0.19), indicating delayed convergence.\n**Program Identifier:** Generation 133 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model to compute LQR gains, enhanced with adaptive gain scheduling, robust angle normalization via `arctan2`, soft-switched integral control gated by pole angle and velocity, and a swing-up assist that activates based on normalized falling severity.  \n- **Performance**: Achieved a high combined score of 4926.64, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and rapid stabilization (stabilization_time: 191).  \n- **Feedback**: The integral action effectively eliminated steady-state cart position error without destabilizing transients, while the swing-up logic and adaptive gains enabled reliable recovery from large initial angles (0.9 rad) despite challenging pole parameters (longer and heavier than standard).\n**Program Identifier:** Generation 134 - Patch Name refined_integral_gating_with_velocity - Correct Program: True\n\n**Program Name: Phase-Based Hybrid Inverted Pendulum Controller**  \n- **Implementation**: Uses a hybrid control strategy with energy-based swing-up for large angles, smooth transition blending, and LQR with integral action near equilibrium; includes friction modeling and Euler integration.  \n- **Performance**: Achieved a combined score of 0.00 due to failure in stabilization despite high energy usage (total_energy: 101.95) and full simulation duration (stabilization_time: 1000).  \n- **Feedback**: The controller failed to stabilize the pendulum (final_theta_error: 2.95 rad), indicating ineffective phase transitions or insufficient robustness for the challenging pole parameters (longer, heavier, high friction).\n**Program Identifier:** Generation 135 - Patch Name none - Correct Program: False\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code\u2014it is an empty Python file with only comments and whitespace.  \n- **Performance**: Combined score is 0.0, indicating complete failure to pass any validation tests.  \n- **Feedback**: The absence of any executable logic means the program cannot perform the intended task; implementation must be added to address the problem requirements.\n**Program Identifier:** Generation 136 - Patch Name None - Correct Program: False\n\n**Program Name: Energy-Shaping Hybrid Controller**  \n- **Implementation**: Combines energy-based swing-up control with LQR stabilization and smooth phase transitions based on angle, velocity, and energy thresholds; includes integral action with anti-windup and centrifugal compensation.  \n- **Performance**: Achieved a combined score of 34.79 but failed to stabilize the pendulum (success_bonus: 0.00), despite maintaining full simulation duration (stabilization_ratio: 1.00).  \n- **Feedback**: The controller could not balance the pole (final_theta_error: 3.12 rad), indicating ineffective transition or stabilization logic; high friction and aggressive pole parameters likely exposed weaknesses in the LQR design or phase-switching criteria.\n**Program Identifier:** Generation 137 - Patch Name energy_shaping_hybrid_controller - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and soft-switched integral action to eliminate steady-state cart position error, using gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and perfect final pole alignment (final_theta_error: 0.00).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handled the challenging high-mass, long-pole configuration, while integral control with angle/velocity gating prevented windup and improved precision without destabilizing the system.\n**Program Identifier:** Generation 138 - Patch Name enhanced_integral_suppression - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up**\n\n- **Implementation**: Uses an enhanced adaptive LQR controller featuring integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, and a swing-up assist activated during large deviations. Friction and nonlinear dynamics are explicitly modeled in simulation.\n\n- **Performance**: Achieved a high combined score of 4926.69 with full success bonus, low energy usage (total_energy=6.50), and perfect final stabilization (final_theta_error=0.00).\n\n- **Feedback**: The soft-switched integral control effectively eliminated steady-state cart drift without destabilizing transients, while the normalized swing-up logic provided reliable recovery from extreme angles. Optimized angular velocity damping (Q[3]=3.28) reduced oscillations and contributed to early stabilization (stabilization_time=191).\n**Program Identifier:** Generation 139 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Action and Swing-Up Assist**  \n- **Implementation**: Uses an enhanced adaptive Linear Quadratic Regulator (LQR) controller featuring integral action for steady-state cart position correction, robust angle normalization via `arctan2`, gain scheduling based on pole angle and angular velocity, and energy-aware swing-up assistance triggered when the pole is far from upright.  \n- **Performance**: Achieved a high combined score of 4906.68, with strong contributions from time bonus (2220.04), energy efficiency (2492.62), and full success bonus (800.00), stabilizing in 194 steps with near-zero final errors.  \n- **Feedback**: The integral control effectively eliminated steady-state cart drift without destabilizing the system due to intelligent gating by angle and velocity; the swing-up logic efficiently managed the challenging initial condition (0.9 rad) with a heavy, long pole, while adaptive gains improved phase-specific responsiveness.\n**Program Identifier:** Generation 140 - Patch Name energy_aware_swingup_assist - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in a complete lack of functionality for any intended algorithm or task.  \n- **Performance**: Combined score is 0.0, indicating failure to pass any validation tests.  \n- **Feedback**: An empty implementation cannot process inputs or produce outputs, leading to total failure in evaluation; actual logic must be implemented to address the problem requirements.\n**Program Identifier:** Generation 141 - Patch Name None - Correct Program: False\n\n**Program Name: Adaptive LQR with Swing-Up Assist**  \n- **Implementation**: Uses a linearized dynamics model with physically accurate friction terms to compute LQR gains, enhanced with adaptive gain scheduling, predictive correction using angular acceleration, physics-informed swing-up assistance for large angles, and gated integral control for cart position.  \n- **Performance**: Achieved a high combined score of 4787.89 with strong time, energy, and success bonuses, stabilizing in 211 steps with near-zero final errors.  \n- **Feedback**: The controller effectively balances aggressive early-phase control with energy-efficient stabilization; the swing-up assist and adaptive gains are critical for handling the challenging pole parameters (heavy, long, high friction), while integral action with gating prevents windup and improves steady-state accuracy.\n**Program Identifier:** Generation 142 - Patch Name aggressive_early_phase_control - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines gain-scheduled LQR with robust integral action, enhanced angular damping, and physics-informed swing-up assistance; uses precise linearized dynamics matching the simulator\u2019s friction model and adaptive gains based on pole angle/velocity.  \n- **Performance**: Achieved a high combined score of 4926.94 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large angles with fine stabilization near upright, while leaky integral action and soft-switched damping reduce overshoot and improve cart centering without destabilizing the pole.\n**Program Identifier:** Generation 143 - Patch Name optimized_integral_and_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and integral action for cart position regulation, using gain scheduling and energy-based scaling to handle a highly unstable, heavy, and long pole.  \n- **Performance**: Achieved a high combined score of 4907.13 with full success bonus, low energy usage, and stable final state.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large deviations with precise LQR stabilization near upright, while integral action eliminates steady-state position error without windup due to soft gating and leaky integration.\n**Program Identifier:** Generation 144 - Patch Name energy_aware_swing_up_assist - Correct Program: True\n\n**Program Name: Phase-Based Adaptive Inverted Pendulum Controller**  \n- **Implementation**: Uses a phase-switching strategy with four LQR controllers (swing-up, transition, balancing, fine-tune), energy shaping for swing-up, integral action with windup protection, predictive compensation, and nonlinear damping.  \n- **Performance**: Combined score of 0.0; fails to stabilize the pendulum under challenging initial conditions and system parameters.  \n- **Feedback**: Despite sophisticated control logic, the controller struggles with the highly unstable configuration (long, heavy pole; high friction; large initial angle), likely due to inadequate swing-up energy injection or poor phase transition timing.\n**Program Identifier:** Generation 145 - Patch Name none - Correct Program: False\n\n**Program Name: Adaptive LQR with Gain Scheduling**  \n- **Implementation**: Uses a physically accurate linearized model for LQR control with adaptive gain scheduling based on angle and angular velocity, enhanced by soft-switched integral action and energy-aware swing-up assistance.  \n- **Performance**: Achieved a high combined score of 4907.12 with strong time, energy, and success bonuses, stabilizing in 194 steps with minimal final errors.  \n- **Feedback**: The controller effectively balances aggressive stabilization for large deviations with energy efficiency near equilibrium; the inclusion of friction in the linearized model and intelligent gain adaptation were key to handling the challenging heavy/long pole configuration.\n**Program Identifier:** Generation 146 - Patch Name sharpen_gain_transitions - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, robust swing-up assistance based on mechanical energy deficit, and gated integral action for cart position stabilization. The controller uses a physically accurate linearized model and includes enhanced damping near equilibrium.  \n- **Performance**: Achieved a high combined score of 4926.90, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 191), and full success bonus.  \n- **Feedback**: The hybrid approach effectively handles the challenging dynamics of a heavy, long pole with high friction, demonstrating that adaptive gains and energy-based swing-up significantly improve robustness and stabilization speed, while leaky integral action prevents windup without degrading performance.\n**Program Identifier:** Generation 147 - Patch Name energy_based_swing_up - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, robust swing-up assistance based on falling severity, and soft-switched integral action for cart position regulation. The controller uses a physically accurate linearized model and dynamically adjusts gains based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4933.93, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (190 steps), and full success bonus.  \n- **Feedback**: The hybrid approach effectively stabilizes a highly challenging inverted pendulum (long, heavy pole; high friction; large initial angle). Integral action and swing-up logic are key to maintaining stability and achieving near-zero final errors, while adaptive gains prevent overreaction during large deviations.\n**Program Identifier:** Generation 148 - Patch Name simplified_integral_gating_with_optimal_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control derived from physically accurate linearized dynamics with adaptive gain scheduling, swing-up assistance based on normalized angular momentum, and dual-gated integral action that activates only near upright and low-velocity conditions.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 192 steps), and full success bonus.  \n- **Feedback**: The hybrid approach effectively handles the challenging pole parameters (heavy, long, high friction), with adaptive gains and swing-up logic enabling robust recovery from large initial angles, while gated integral control prevents windup and improves steady-state accuracy.\n**Program Identifier:** Generation 149 - Patch Name adaptive_lqr_integral_swingup_enhanced - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with physics-informed swing-up assistance for large angles and gated integral action to eliminate steady-state cart position error; uses adaptive gain scheduling based on pole angle and angular velocity.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (192 steps), and full success bonus.  \n- **Feedback**: The enhanced swing-up logic effectively handles the challenging initial conditions (heavy, long pole with large initial angle), while the soft-gated integral term prevents windup and improves final positioning without destabilizing the system.\n**Program Identifier:** Generation 150 - Patch Name adaptive_energy_aware_swingup - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Uses a refined LQR controller with gain scheduling, angle-only integral gating, and swing-up assistance for large deviations; includes physically accurate linearization and friction-aware dynamics.  \n- **Performance**: Achieved a high combined score of 4920.84 with full success bonus, low energy use (6.58 total), and fast stabilization (192 steps).  \n- **Feedback**: The adaptive gains and simplified integral control effectively balanced stability and efficiency, while the swing-up logic enabled recovery from large initial angles (0.9 rad); tight anti-windup and decay prevented integral windup without sacrificing responsiveness.\n**Program Identifier:** Generation 151 - Patch Name refined_adaptive_lqr_optimized - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control and Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with gain scheduling, integral action for cart position regulation, and physics-informed swing-up assistance triggered by pole angle and falling dynamics. Uses accurate friction modeling and Euler integration for simulation.  \n- **Performance**: Achieved a high combined score of 4934.54 with full success bonus, low energy use (6.61 total), and fast stabilization (190 steps).  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle large initial angles and system instability from heavy/long poles, while integral control ensures precise long-term positioning without windup due to velocity-gated activation.\n**Program Identifier:** Generation 152 - Patch Name sharper_gain_transitions_and_velocity_gated_integral - Correct Program: True\n\n**Program Name: Robust Swing-Up LQR with Integral Action**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, and integral action for cart position stabilization; uses physically accurate dynamics and directional momentum awareness to enhance recovery during large-angle excursions.  \n- **Performance**: Achieved a high combined score of 4933.59 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 190 steps), and perfect final state accuracy.  \n- **Feedback**: The controller effectively balances aggressive swing-up and precise stabilization, with integral action eliminating steady-state cart error without windup; directional momentum logic prevents overshoot during recovery, contributing to robustness under challenging pole parameters (heavy, long, high friction).\n**Program Identifier:** Generation 153 - Patch Name enhanced_integral_control_with_dynamic_gating - Correct Program: True\n\n**Program Name: Adaptive LQR with Swing-Up and Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state cart position error, adaptive gain scheduling via tanh-based angle/velocity thresholds, and physics-informed swing-up assist using energy deficit and angular momentum cues.  \n- **Performance**: Achieved a high combined score of 4926.95 with strong stability (final theta error \u2248 0), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The integration of swing-up logic enabled recovery from large initial angles (0.9 rad), while adaptive gains and integral control ensured precise stabilization; energy-efficient actuation and leaky integration prevented windup and maintained robustness under high-friction, heavy-pole conditions.\n**Program Identifier:** Generation 154 - Patch Name energy_scaled_swing_up_assist - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral and Swing-Up Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with integral action for zero steady-state error and a physics-informed swing-up assist that activates for large pole angles (>0.8 rad), using smooth tanh-based gain scheduling and dual-gated integral feedback.  \n- **Performance**: Achieved a high combined score of 4920.23, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00).  \n- **Feedback**: The adaptive gains and swing-up logic enabled rapid recovery from large initial angles (0.9 rad), while integral control ensured precise stabilization; the design effectively balanced aggressive correction with energy conservation under high-friction, unstable dynamics.\n**Program Identifier:** Generation 155 - Patch Name enhanced_integral_control_with_velocity_gating_and_adaptive_decay - Correct Program: True\n\n**Program Name: Phase-Aware Adaptive LQR Controller**  \n- **Implementation**: Uses three phase-specific LQR controllers (swing-up, transition, stabilize) combined with energy-based swing-up logic and adaptive integral action with anti-windup. Angle normalization and momentum-aware control enhance robustness under high-friction, heavy-pole conditions.  \n- **Performance**: Achieved a combined score of 4478.10 with full success bonus, low final errors, and efficient energy use (avg 0.01 per step).  \n- **Feedback**: The phase-adaptive strategy effectively handles the challenging dynamics (long, heavy pole; high friction), enabling rapid stabilization (253 steps) and maintaining balance with minimal energy expenditure. Integral action and smooth phase transitions prevent windup and ensure precision.\n**Program Identifier:** Generation 156 - Patch Name none - Correct Program: True\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with cross-coupled state weighting, adaptive gain scheduling, energy-based swing-up assistance for large angles, and enhanced integral action with angle/velocity gating and decay.  \n- **Performance**: Achieved a high combined score of 4953.84, with strong stabilization (final theta error \u2248 0), low energy use (avg 0.01 per step), and full success bonus.  \n- **Feedback**: The energy-aware swing-up and adaptive gains effectively handled the challenging heavy/long pole, while integral control with tight gating improved cart centering without destabilizing the system near equilibrium.\n**Program Identifier:** Generation 157 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Energy-Based Swing-Up**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, dual-gated integral action for cart position, and energy-based swing-up assistance triggered by large pole angles. The controller uses physically accurate linearization and robust angle normalization.  \n- **Performance**: Achieved a high combined score of 4947.50, with strong time, energy, and success bonuses, stabilizing the pendulum in 188 steps despite extreme parameters (heavy, long pole; high friction).  \n- **Feedback**: The energy-based swing-up and adaptive gains effectively handled large initial deviations, while dual-gated integral control improved steady-state cart positioning without destabilizing the pole. Evaluation confirms robustness and efficiency under challenging dynamics.\n**Program Identifier:** Generation 158 - Patch Name adaptive_lqr_integral_v2 - Correct Program: True\n\n**Program Name: Empty Program Analysis**  \n- **Implementation**: The provided program contains no code, resulting in a complete lack of functionality for any intended algorithm or task.  \n- **Performance**: Combined score is 0.0, indicating failure to pass any validation tests.  \n- **Feedback**: An empty implementation cannot perform computation or meet problem requirements; adding core logic is essential for correctness and scoring.\n**Program Identifier:** Generation 159 - Patch Name energy_coupled_lqr - Correct Program: False\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance, adaptive gain scheduling, predictive falling severity estimation, and enhanced integral action with state-dependent gating and decay. The controller uses a physically accurate linearized model and cross-coupled Q-matrix terms for coordinated cart-pole dynamics.  \n- **Performance**: Achieved a high combined score of 4953.84, with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (stabilization_time: 187 steps), and full success bonus.  \n- **Feedback**: The energy-aware swing-up logic and adaptive gains effectively handled large initial angles and system instability from the heavy, long pole. Integral control with tight velocity/angle gating prevented windup while improving steady-state cart positioning, contributing to low final errors and high time/energy bonuses.\n**Program Identifier:** Generation 160 - Patch Name enhanced_swingup_with_momentum_prediction - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines gain-scheduled LQR control with adaptive integral action, enhanced angular damping, and physics-informed swing-up assistance to stabilize a challenging inverted pendulum (heavy, long pole with high friction).  \n- **Performance**: Achieved a high combined score of 4933.93, with strong energy efficiency (avg_energy_per_step: 0.01) and full success bonus (800.00), stabilizing in 190 steps.  \n- **Feedback**: The adaptive gains and integral clamping effectively handled large initial angles and high system instability, while the swing-up logic provided critical recovery during aggressive falls\u2014key to robust performance under extreme parameters.\n**Program Identifier:** Generation 161 - Patch Name adaptive_integral_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines physically accurate linearized dynamics with gain-scheduled LQR, swing-up assistance for large angles, and gated integral action with anti-windup to handle a challenging high-friction, long/heavy-pole inverted pendulum.  \n- **Performance**: Achieved a high combined score of 4920.84 with full success bonus, low energy use (6.58 total), and fast stabilization (192 steps).  \n- **Feedback**: The sharpened gain scheduling and angle-gated integral control effectively balanced aggressive swing-up and precise stabilization, while the tuned LQR weights (notably Q[3]=3.2) and friction-aware modeling were critical to robustness under difficult physical parameters.\n**Program Identifier:** Generation 162 - Patch Name adaptive_integral_lqr_hybrid - Correct Program: True\n\n**Program Name: Adaptive Energy-Aware LQR with Swing-Up Control**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with adaptive gain scheduling, directional momentum-aware swing-up logic, and soft-switched integral action for cart position, all tuned for extreme pendulum parameters (heavy, long pole; high friction).  \n- **Performance**: Achieved a high combined score of 4927.07, with strong energy efficiency (avg_energy_per_step: 0.01), full success bonus, and stable final state.  \n- **Feedback**: The controller effectively balances aggressive swing-up for large deviations with precise LQR stabilization near upright, while integral action eliminates steady-state position error without causing overshoot\u2014key to handling the challenging system dynamics.\n**Program Identifier:** Generation 163 - Patch Name none - Correct Program: True\n\n**Program Name: Adaptive LQR with Energy-Based Swing-Up**\n\n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, dual-gated integral action for cart position, predictive momentum compensation, and energy-based swing-up assistance tailored to extreme pendulum parameters (heavy, long pole with high friction).\n\n- **Performance**: Achieved a high combined score of 4947.47, with strong bonuses for success (800), time (2261), and energy efficiency (2492), stabilizing in 188 steps with near-zero final errors.\n\n- **Feedback**: The controller effectively handles challenging initial conditions and system dynamics through intelligent nonlinear enhancements; the energy-based swing-up and adaptive gains were critical for rapid stabilization and robustness under high friction and instability.\n**Program Identifier:** Generation 164 - Patch Name predictive_momentum_compensation - Correct Program: True\n\n**Program Name: Energy-Aware Adaptive LQR with Integral Control**  \n- **Implementation**: Combines linear quadratic regulator (LQR) control with energy-based swing-up assistance, predictive falling severity estimation, and adaptive integral action that activates only near equilibrium; includes cross-coupled terms in the Q-matrix and state-dependent gain scheduling for robustness.  \n- **Performance**: Achieved a high combined score of 4953.83 with strong time, energy, and success bonuses, stabilizing the pendulum in 187 steps with minimal final errors.  \n- **Feedback**: The controller effectively handles the challenging pole parameters (longer, heavier) by blending aggressive swing-up logic with precise stabilization; integral control and predictive momentum compensation significantly improve centering and robustness without excessive energy use.\n**Program Identifier:** Generation 165 - Patch Name predictive_momentum_compensation - Correct Program: True\n\n**Program Name: Adaptive Integral LQR with Swing-Up Assist**  \n- **Implementation**: Combines linearized LQR control with adaptive gain scheduling, integral action gated by pole angle/velocity, and energy-based swing-up assistance for large deviations. Uses physically accurate friction modeling in both simulation and controller design.  \n- **Performance**: Achieved a high combined score of 4926.68 with strong energy efficiency (avg_energy_per_step: 0.01), fast stabilization (191 steps), and full success bonus.  \n- **Feedback**: The adaptive gains and swing-up logic effectively handle the challenging heavy/long pole configuration, while gated integral action prevents destabilization during transients\u2014key to balancing speed and stability.\n**Program Identifier:** Generation 166 - Patch Name enhanced_swing_up_with_energy_awareness - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines physically accurate linearized dynamics with LQR control (Q[3]=3.2), gain scheduling based on angle/velocity thresholds, energy-aware swing-up logic, and gated integral action with anti-windup for cart centering.  \n- **Performance**: Achieved a high combined score of 4920.85 with full success bonus, low energy use (avg 0.01/step), and rapid stabilization (192 steps).  \n- **Feedback**: The sharpened gain scheduling and energy-aware swing-up effectively handled the challenging heavy/long pole, while the simplified integral gating proved robust for steady-state accuracy without windup issues.\n**Program Identifier:** Generation 167 - Patch Name enhanced_swing_up_with_energy_awareness - Correct Program: True\n\n**Program Name: Adaptive LQR with Integral Control and Swing-Up**\n\n- **Implementation**: Uses an enhanced adaptive LQR controller with integral action for cart position error elimination, robust angle normalization via `arctan2`, gain scheduling based on pole angle/velocity, swing-up assistance triggered at large angles, and mid-swing angular damping. The dynamics include high-friction terms and a heavy, long pole to increase difficulty.\n\n- **Performance**: Achieved a combined score of 0.00 despite a base score of 7.16, failing to stabilize the pendulum (final theta error: 2.94 rad, final x error: 4432 m) over 1000 steps.\n\n- **Feedback**: The controller fails to stabilize the system due to excessive drift in cart position and inability to recover from large initial angles; integral action may be too weak or improperly gated, and swing-up logic might interfere with stabilization rather than assist it.\n**Program Identifier:** Generation 168 - Patch Name sharper_gain_scheduling - Correct Program: False\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines LQR with velocity-gated integral action, gentle gain scheduling, and swing-up assistance for large angles; uses physically accurate linearized dynamics and proven Q/R weights.  \n- **Performance**: Achieved a high combined score of 4954.47 with full success bonus, low energy use (6.51 total), and fast stabilization (187 steps).  \n- **Feedback**: The hybrid approach effectively balances aggressive swing-up for large deviations with precise LQR control near upright, while velocity-gated integration prevents windup and improves stability.\n**Program Identifier:** Generation 169 - Patch Name adaptive_integral_lqr_hybrid - Correct Program: True\n\n**Program Name: Adaptive Cross-Coupled LQR Controller**  \n- **Implementation**: Uses an adaptive LQR with state-dependent cross-coupling in the Q-matrix, mid-swing angular damping, predictive momentum compensation, and dual-gated integral control to stabilize a highly unstable inverted pendulum (long, heavy pole with high friction).  \n- **Performance**: Achieved a combined score of 5015.73 with full success bonus, low energy use (7.54 total), and fast stabilization (178 steps).  \n- **Feedback**: The controller effectively handles large initial angles and aggressive dynamics through adaptive gains and predictive corrections; the dual-gated integral term prevents windup while aiding precision near upright equilibrium.\n**Program Identifier:** Generation 170 - Patch Name adaptive_cross_coupled_lqr_with_midswing_damping - Correct Program: True\n\n**Program Name: Hybrid Adaptive Integral LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with gain scheduling, robust swing-up logic based on energy deficit and momentum prediction, velocity-gated integral action for cart centering, and enhanced damping near equilibrium. The controller uses a physically accurate linearized model and adaptive gains that increase responsiveness during large deviations.  \n- **Performance**: Achieved a high combined score of 4756.64 with strong time, energy, and success bonuses, stabilizing the pendulum in 215 steps with minimal final errors.  \n- **Feedback**: The hybrid approach effectively handles the challenging dynamics (heavy, long pole; high friction), with integral action and swing-up assist crucial for recovery and stabilization. Energy-efficient control and precise gain scheduling contributed to low average energy use and fast settling.\n**Program Identifier:** Generation 171 - Patch Name none - Correct Program: True\n\n**Program Name: Phase-Aware Momentum-Cancellation LQR Controller**  \n- **Implementation**: Combines linear-quadratic regulator (LQR) control with phase-based adaptive gains, momentum-aware integral action, and energy-injection recovery logic tailored to a highly unstable inverted pendulum (heavy, long pole with high friction). Uses stability-index-driven phase blending, predictive feedforward, and dual-gated integrals for precision stabilization.  \n- **Performance**: Achieved a high combined score of 4967.97, with strong time, energy, and success bonuses, stabilizing the system in 185 steps with near-zero final errors.  \n- **Feedback**: The controller effectively handles extreme initial conditions and physical challenges through its hybrid strategy; evaluation confirms robustness, efficient energy use, and precise stabilization, validating the novel momentum-integral and phase-aware design choices.\n**Program Identifier:** Generation 172 - Patch Name phase_aware_momentum_lqr - Correct Program: True\n\n**Program Name: Hybrid Adaptive Cross-Coupled LQR Controller**  \n- **Implementation**: Combines cross-coupled LQR with mid-swing damping, sharpened gain scheduling, predictive momentum compensation, and angle-gated integral control to stabilize a challenging inverted pendulum (heavy, long pole; high friction; large initial angle).  \n- **Performance**: Achieved a high combined score of 4983.31 with full success bonus, low energy use (7.54 total), and fast stabilization (183 steps).  \n- **Feedback**: The controller effectively balances aggressive swing-up for large angles with precise linear regulation near upright, while adaptive gains and predictive terms prevent overshoot and divergence; integral gating avoids windup during transients.\n**Program Identifier:** Generation 173 - Patch Name adaptive_crosscoupled_lqr_with_midswing_damping - Correct Program: True\n\n**Program Name: Adaptive LQR with Predictive Damping**\n\n- **Implementation**: Combines linear quadratic regulator (LQR) control with adaptive gain scheduling, predictive angular acceleration estimation for mid-swing damping, energy-aware integral action, and swing-up assistance for large angles. Uses physically accurate linearized dynamics and friction-aware modeling.\n\n- **Performance**: Achieved a high combined score of 4934.14, with strong time, energy, and success bonuses, stabilizing the pendulum in 190 steps (19% of max time) and maintaining near-zero final errors.\n\n- **Feedback**: The integration of predictive damping and energy-based integral gain adaptation significantly improved robustness under challenging conditions (heavy, long pole; high friction). Early stabilization and low energy usage indicate efficient, proactive control that balances swing-up and balancing phases effectively.\n**Program Identifier:** Generation 174 - Patch Name enhanced_adaptive_lqr_with_predictive_damping - Correct Program: True",
  "meta_scratch_pad": "## Successful Algorithmic Patterns\n\n- **State-dependent cross-coupling in the LQR Q-matrix significantly improves coordination between angle and angular velocity**, enabling faster stabilization under high-inertia conditions. The current best program (*Adaptive Cross-Coupled LQR Controller*, **5015.73**) introduces off-diagonal terms `Q[1,3] = Q[3,1] = 0.8`, dynamically boosted by `cross_coupling_boost = 1.0 + 0.5 * tanh(3.0 * abs(theta))`. This yields the fastest stabilization time (**178 steps**) among all evaluated programs\u2014beating even the previously top-performing Generation 165 (**187 steps**)\u2014and contributes directly to its record **time_bonus: 2328.30**.\n\n- **Gaussian-shaped mid-swing damping provides targeted momentum suppression without destabilizing near-equilibrium control**. The current best uses `mid_swing_activation = exp(-4.0 * (abs(theta) - 0.5)**2)` to apply damping only in the critical **0.25\u20130.85 rad** range, avoiding interference with swing-up or balancing phases. This contrasts with blunt constant damping and explains why this program achieves **stable_x_error: 0.05** despite aggressive swing-up, outperforming *Hybrid Adaptive Integral LQR* (**4756.64**) which lacks such phase-specific damping.\n\n- **Predictive momentum compensation using estimated angular acceleration enables earlier counteraction of divergence**, especially crucial for the heavy, long pole (`M_POLE=0.35`, `L_POLE=2.5`). The current best estimates `theta_acc_est` from gravity and joint friction, then computes `pred_divergence = theta * dtheta_pred / (omega_n * L_COM)` to generate a corrective term. This proactive strategy\u2014absent in Generations 171 and 174\u2014contributes to its near-zero final errors (**final_theta_error: 0.00**, **final_x_error: 0.00**) and supports its elite energy efficiency (**total_energy: 7.54**) by reducing corrective overactuation.\n\n- **Dual-gated integral control with exponential decay during inactivity maintains precision without windup**, building on prior successful patterns but refined for extreme dynamics. The current best uses identical gating (`angle_gate = tanh(12.0*(0.1\u2212|\u03b8|))`, `velocity_gate = tanh(8.0*(1.0\u2212|d\u03b8|))`) as the previous best (**4954.47**), but couples it with **integral decay (`*= 0.95`)** when inactive\u2014a technique validated in Generation 167. This synergy ensures rapid reactivation near equilibrium, achieving **stable_x_error: 0.05**, matching the precision of top predecessors while operating under more challenging physical parameters.\n\n## Ineffective Approaches\n\n- **Omission of predictive or mid-swing damping mechanisms leads to momentum overshoot and slower stabilization**, even with otherwise strong LQR foundations. *Hybrid Adaptive Integral LQR Controller* (**4756.64**)\u2014despite excellent integral gating and gain scheduling\u2014stabilizes in **215 steps**, **37 steps slower** than the current best, due to lack of explicit mid-phase damping or predictive correction. This confirms that reactive LQR alone is insufficient for the heaviest/longest poles tested.\n\n- **Swing-up logic that lacks falling-severity normalization wastes energy and delays transition to balancing**. While all top programs use swing-up, the current best\u2019s use of `falling_factor = 1.0 + tanh(2.0 * theta * dtheta / (L_COM * omega_n))` makes assistance directionally intelligent. In contrast, *Adaptive LQR with Predictive Damping* (**4934.14**) uses less refined swing-up, resulting in **190-step stabilization**\u20147 steps slower than the current best\u2014indicating that unnormalized swing-up is suboptimal under high inertia.\n\n- **Static or weakly adaptive cross-coupling fails to exploit state-dependent coordination benefits**. Earlier correct programs like Generation 171 (**4756.64**) and 172 (**4967.97**) omit cross-coupled Q-terms entirely, relying solely on diagonal weights and gain scheduling. Their scores\u2014while high\u2014are consistently **>48 points below** the current best (**5015.73**), demonstrating that cross-coupling is not merely optional but a key differentiator at the performance frontier.\n\n## Implementation Insights\n\n- **The current best program integrates four synergistic layers\u2014adaptive cross-coupled LQR, Gaussian mid-swing damping, predictive momentum compensation, and dual-gated integral control\u2014into a single coherent architecture**, each layer addressing a distinct phase of the control challenge. Unlike piecemeal approaches, this holistic design ensures smooth transitions: swing-up activates only beyond **0.8 rad**, mid-swing damping peaks at **0.5 rad**, predictive correction scales with divergence, and integral action engages only within **0.1 rad** and low velocity. This layered phase-awareness is absent in all lower-scoring programs.\n\n- **Precise friction-aware linearized dynamics in the A-matrix are non-negotiable for zero final angle error under high joint friction (`FRICTION_JOINT=0.25`)**. The current best explicitly models joint friction in `A[3,3] = -b_j / (m*l*denom0)`, matching the structure used in all correct programs. This fidelity\u2014combined with the proven Q-matrix `diag([4.5, 44.0, 0.6, 3.2])`\u2014enables perfect upright stabilization (**final_theta_error: 0.00**), whereas even minor model inaccuracies would cause residual oscillation under these extreme parameters.\n\n- **Force clipping redundancy (inside controller and simulation loop) ensures robustness during aggressive swing-up maneuvers**, such as the current best\u2019s `u_swing = 8.0 * ...` term. This dual safeguard prevents numerical overflow and guarantees actuator compliance, directly enabling its full **success_bonus: 800.00** across all test conditions\u2014a pattern consistently present in all correct programs but missing in failed ones like Generation 168.\n\n- **Natural frequency normalization (`omega_n = sqrt(G / L_COM)`) enables scalable, physics-informed decision-making across varying pole lengths and masses**. The current best uses `omega_n` to normalize both predictive divergence (`theta * dtheta_pred / (omega_n * L_COM)`) and falling severity, ensuring control intensity adapts to the actual dynamic timescale. This explains its ability to maintain **avg_energy_per_step: 0.01**\u2014matching saturated elite efficiency\u2014while stabilizing **9 steps faster** than Generation 174, which uses similar but less normalized predictive logic.\n\n## Performance Analysis\n\n- **The current best program (**5015.73**) sets a new performance ceiling, surpassing all predecessors by \u226532 points**, primarily through unprecedented speed (**178 steps**) rather than energy savings (which remain saturated at ~7.5 total). Its **time_bonus: 2328.30** exceeds the previous best (**2267.91**) by **60.39 points**, confirming that **swing-up and mid-phase efficiency\u2014not just balancing precision\u2014is now the dominant scoring factor** under extreme physical parameters.\n\n- **Cross-coupled LQR with predictive damping outperforms hybrid integral-LQR and phase-aware variants**, despite their sophistication. *Phase-Aware Momentum-Cancellation LQR* (**4967.97**) and *Hybrid Adaptive Cross-Coupled LQR* (**4983.31**) both score **>32 points below** the current best, indicating that **explicit cross-coupling in the cost function combined with Gaussian mid-swing damping provides superior transient control** compared to phase-blending or sharpened gain scheduling alone.\n\n- **Energy efficiency has plateaued across all correct implementations (total_energy \u2248 6.5\u20137.54)**, making time-to-stabilization the sole meaningful differentiator above **4920**. The current best\u2019s **178-step stabilization**\u2014**7\u201337 steps faster** than peers\u2014accounts for nearly all of its score advantage, as all top programs earn the same **success_bonus: 800.00** and achieve near-identical final errors.\n\n- **The combination of predictive compensation and mid-swing damping uniquely enables both fast swing-up and minimal overshoot**, resolving a key trade-off that limited earlier controllers. Programs without both features\u2014e.g., Generation 171 (no predictive/mid-swing) or Generation 174 (predictive but no Gaussian damping)\u2014show measurably slower convergence, proving that **proactive momentum management across the entire swing-up trajectory is essential for record performance**.",
  "meta_recommendations": "1. **Enhance predictive momentum compensation with joint-friction-aware angular acceleration estimation**: Replace the current simplified `theta_acc_est` with a full friction-inclusive model that accounts for both cart and joint friction terms in the denominator, mirroring the precision of the A-matrix used in LQR design. This would align predictive correction with the high-fidelity dynamics already proven essential for zero final error under `FRICTION_JOINT=0.25`, potentially enabling even earlier divergence counteraction and shaving 2\u20133 steps off the 178-step stabilization time.\n\n2. **Introduce state-dependent mid-swing damping gain modulated by predicted energy deficit**: Scale the `K_d_midswing = 2.5` coefficient using the normalized mechanical energy: `K_d = 2.5 * (1.0 + 0.4 * max(0, 1 - E_current / E_max))`, where `E_max = M_POLE*G*L_COM`. This leverages the Gaussian damping\u2019s phase specificity while making its intensity responsive to swing-up progress\u2014applying stronger damping when the pole is under-energized and prone to stall, which could reduce overshoot variability across extreme initial conditions without compromising balancing precision.\n\n3. **Make cross-coupling boost adaptive to both angle and angular velocity magnitude**: Extend the current `cross_coupling_boost = 1.0 + 0.5 * tanh(3.0 * abs(theta))` to include velocity dependence: `boost = 1.0 + 0.5 * tanh(3.0 * abs(theta)) * (1.0 + 0.3 * tanh(2.0 * abs(dtheta)))`. Since high `dtheta` during mid-swing exacerbates coordination challenges, this dual-state adaptation would intensify theta-dtheta coupling precisely when momentum is highest, improving transient response during the critical 0.3\u20130.8 rad transition zone identified as pivotal for record time_bonus.\n\n4. **Implement exponential decay on the integral state only when outside both gating thresholds**, rather than applying decay whenever the gate product is low. Specifically, decay `self.integral_x *= 0.95` only if `abs(theta) > 0.12 OR abs(dtheta) > 1.1`, preserving accumulated correction during brief excursions beyond equilibrium. This refines the current dual-gating strategy to better handle high-inertia recovery transients, potentially reducing `stable_x_error` below 0.05 while maintaining windup protection\u2014addressing the last remaining precision gap among elite controllers.\n\n5. **Replace the fixed Gaussian center (0.5 rad) in mid-swing damping with an inertia-scaled peak location**: Set the activation center as `theta_peak = 0.5 * np.sqrt(L_COM / 2.5)` so the damping maximum adapts to the actual pole length, consistent with the `omega_n` normalization already used in predictive logic. Given that longer poles exhibit delayed momentum peaks, this ensures damping aligns with the true high-risk region of the swing-up trajectory, preventing premature or late intervention that could otherwise waste energy or permit overshoot under extreme `L_POLE=2.5` conditions.",
  "meta_recommendations_history": [
    "1. **Refine the conservative LQR of Generation 0 with adaptive gain scheduling near upright equilibrium**: Since Generation 0\u2019s low-gain LQR stabilized the system slowly but successfully, introduce state-dependent gain adjustment\u2014e.g., increase feedback gains when |theta| < 0.5 rad\u2014to accelerate convergence while preserving stability during large deviations. This builds directly on its proven feasibility while addressing its slow stabilization.\n\n2. **Incorporate proper angle wrapping and high-fidelity dynamics into the LQR framework**: Generation 3\u2019s failure was partly attributed to missing state wrapping; integrate periodic angle normalization (e.g., `theta = (theta + \u03c0) % (2\u03c0) - \u03c0`) and use a more accurate discretization (e.g., Runge-Kutta instead of Euler) within the same conservative LQR structure as Generation 0 to improve model fidelity without sacrificing robustness.\n\n3. **Add a lightweight swing-up phase that hands off cleanly to the conservative LQR**: While hybrid energy-LQR approaches failed due to poor transition logic, design a simple energy-pumping controller (e.g., based on sign(\u03b8\u00b7\u03b8\u0307)) that activates only when |theta| > 0.8 rad and switches definitively to Generation 0\u2019s LQR once |theta| \u2264 0.7 rad\u2014ensuring the proven stabilizer operates within its effective domain.\n\n4. **Tune Q/R weights using trajectory-based optimization rather than heuristic selection**: Instead of arbitrary conservative weights, perform a coarse grid search or Bayesian optimization over Q (state cost) and R (control cost) matrices focused on minimizing final theta error under the constraint |u| \u2264 100 N, leveraging Generation 0\u2019s architecture as a stable baseline for parameter exploration.\n\n5. **Introduce integral action or disturbance rejection to counteract high friction effects**: Since friction compensation alone failed in other designs, augment Generation 0\u2019s LQR with an integrator on theta (or cart position) to reject steady-state bias caused by Coulomb friction\u2014implemented via state augmentation (e.g., adding \u222b\u03b8 dt to the state vector)\u2014to improve long-term balance without aggressive control.",
    "1. **Enhance Generation 0\u2019s conservative LQR with state-dependent gain scheduling that smoothly increases feedback gains as |theta| decreases below 0.7 rad**, using a sigmoid or piecewise-linear blending function to avoid discontinuities. This directly extends its proven stability while accelerating convergence in the final balancing phase\u2014addressing its main weakness (slow stabilization) without sacrificing robustness during large deviations.\n\n2. **Replace Euler integration in Generation 0\u2019s simulation loop with a semi-implicit or Runge-Kutta 4th-order integrator while preserving its clipped control law and low-gain structure**, to reduce discretization error that may accumulate over long stabilization horizons. Given that conservative gains already suppress fast dynamics, higher-fidelity integration should improve state prediction accuracy without destabilizing the controller.\n\n3. **Augment the state vector of Generation 0\u2019s LQR with an integral term on theta (e.g., \u222b\u03b8 dt) to form an LQI controller**, explicitly compensating for steady-state bias induced by high Coulomb friction. Since friction is a dominant disturbance in this system and Generation 0 already operates within physical actuator limits, integral action can eliminate residual angle drift without requiring aggressive proportional gains.\n\n4. **Implement strict angle wrapping (`theta = (theta + \u03c0) % (2\u03c0) - \u03c0`) in the state measurement pipeline before applying feedback**, ensuring consistent angular representation across all operating regimes. While Generation 0 succeeded without it, proper wrapping will improve reliability under wider initial conditions and prevent potential phase errors that could undermine future adaptive extensions.\n\n5. **Design a minimal swing-up pre-phase triggered only when |theta| > 0.85 rad, using a bang-bang or energy-pumping policy based on sign(\u03b8\u00b7\u03b8\u0307), with a hard handoff to Generation 0\u2019s LQR once |theta| \u2264 0.75 rad**, eliminating blending logic to avoid control conflicts. This isolates the proven stabilizer from large-angle dynamics while leveraging simple physics-based pumping to bring the pole into the LQR\u2019s effective domain\u2014addressing the limitation that Generation 0 cannot recover from very large initial angles.",
    "1. **Refine the friction-aware linearized model by incorporating velocity-dependent Coulomb friction approximations into the A matrix**, replacing constant damping terms with piecewise-linear representations that better capture stick-slip transitions\u2014this directly extends Generation 11\u2019s successful modeling insight while addressing residual steady-state errors observed in high-friction regimes.\n\n2. **Introduce a minimal swing-up pre-phase triggered only when |\u03b8| > 0.85 rad using energy-based pumping (e.g., u = k\u00b7sign(\u03b8\u00b7d\u03b8)) with hard handoff to the current LQR at |\u03b8| \u2264 0.75 rad**, preserving the proven stabilizer\u2019s integrity while enabling recovery from extreme initial angles like the 0.9 rad used in evaluation\u2014addressing the current program\u2019s limitation without adding fragile blending logic.\n\n3. **Optimize LQR weights via constrained Bayesian optimization focused on maximizing time_bonus and base_score while maintaining stabilization_ratio \u2265 0.4**, leveraging the current Q=[5,30,0.5,2], R=1.0 as a prior; this builds on the conservative tuning success but systematically explores higher-performance trade-offs within the stable regime identified by Generation 11.\n\n4. **Replace Euler integration in the controller\u2019s internal state prediction (not simulation) with a semi-implicit scheme for more accurate feedback computation**, while retaining Euler in the plant simulator\u2014this enhances control law fidelity without destabilizing the system, exploiting the margin provided by conservative gains to improve state estimation for feedback.\n\n5. **Add integral action *only* on the pole angle (\u03b8) to form an LQI extension of the current controller, but clamp the integrator state during large deviations (|\u03b8| > 0.7 rad) to prevent windup**, directly targeting the residual drift caused by high joint friction while respecting the insight that integral augmentation fails when active outside the linear domain.",
    "1. **Replace the current binary gain-scheduling threshold with a smooth, continuously differentiable gain modulation function based on |\u03b8|**\u2014specifically, adopt the exact formulation from Generation 17 (`gain = 1.0 + 0.5 * max(0, abs(theta) - 0.6)`) instead of the current abrupt `if abs(theta) > 0.6` block multiplied by a fixed `high_gain_factor`. This eliminates discontinuities in control effort that can excite nonlinear dynamics during Euler integration, directly leveraging the proven success of smooth adaptation in the highest-scoring run.\n\n2. **Refine the linearized A matrix to include joint friction as `-FRICTION_JOINT / (M_POLE * L_COM**2)` rather than `-FRICTION_JOINT / (M_CART * L_COM**2)`**, correcting the physical inconsistency in the current implementation where cart mass appears in the joint damping term. This aligns the model with the actual dissipative torque (`\u03c4 = -FRICTION_JOINT * dtheta`) acting on the pole\u2019s rotational inertia (`I = M_POLE * L_COM\u00b2`), ensuring LQR gains accurately reflect true system damping\u2014as validated by Generations 11 and 17.\n\n3. **Introduce a minimal energy-based swing-up logic triggered only when |\u03b8| > 0.85 rad**, using `u = k * sign(theta * dtheta)` to inject energy until |\u03b8| \u2264 0.75 rad, then seamlessly hand off to the current adaptive LQR without blending. This addresses the controller\u2019s marginal performance at the evaluation\u2019s extreme initial condition (0.9 rad) while preserving the stability of the core LQR\u2014extending the successful large-deviation recovery insight without introducing fragile mode-switching.\n\n4. **Add angle-dependent integrator action exclusively on \u03b8, active only when |\u03b8| \u2264 0.6 rad and clamped otherwise**, forming an LQI extension that compensates for steady-state drift caused by high joint friction. The integrator state should be reset or frozen during large deviations to prevent windup, directly targeting residual errors observed in high-friction regimes while respecting the insight that integral augmentation fails outside the linear domain.\n\n5. **Optimize LQR weights via constrained search around Q=[5,30,0.5,2], R=1.0 to maximize time_bonus while enforcing stabilization_ratio \u2265 0.42**, prioritizing increased theta weight (e.g., 35\u201340) and slightly reduced x weight (e.g., 4\u20134.5) to accelerate angular correction without destabilizing cart position. This builds on the conservative tuning foundation that enabled success, systematically exploring higher-performance trade-offs within the empirically validated stable regime.",
    "1. **Refine the joint friction term in the linearized A matrix to use pole inertia correctly**: Replace `-b_j / (m * l * denom0)` in `A[3,3]` with `-b_j / (m * l**2 * (4.0/3.0 - m / Mtot))`, which aligns the damping coefficient with the physical rotational inertia `I = m*l\u00b2`. This corrects a subtle modeling inconsistency in the current best program and matches the exact dissipative structure used successfully in Generations 11 and 17\u2014potentially improving LQR gain accuracy without sacrificing stability.\n\n2. **Introduce smooth, angle-dependent gain modulation while preserving the core conservative LQR**: Replace any abrupt switching logic with a continuously differentiable gain factor like `gain = 1.0 + 0.5 * max(0, abs(theta) - 0.6)`, applied multiplicatively to the control output. This builds directly on the current best\u2019s robust baseline by adding gentle adaptation for large angles\u2014mirroring Generation 17\u2019s proven success\u2014while avoiding discontinuities that could destabilize integration.\n\n3. **Tune LQR weights within the empirically stable regime to prioritize angular correction**: Systematically increase the theta weight from 30.0 to 35\u201340 and slightly reduce the x weight from 5.0 to 4.0\u20134.5 (e.g., `Q=[4.2, 38.0, 0.5, 2.0]`) to accelerate stabilization without triggering instability. This leverages the current best\u2019s validated friction-aware model and conservative foundation to potentially boost time_bonus while maintaining near-zero final error and low energy use.\n\n4. **Add a minimal swing-up trigger for extreme initial conditions (|\u03b8| > 0.85 rad)**: Implement a simple energy-pumping action `u = 80 * np.sign(theta * dtheta)` when `abs(theta) > 0.85`, active only until `abs(theta) \u2264 0.75`, then seamlessly revert to the base LQR. This addresses the marginal performance at \u03b8\u2080=0.9 rad observed in the current best\u2014extending its robustness to worst-case starts without complex mode blending or observer overhead.\n\n5. **Maintain strict angle normalization and force clipping but enhance state fidelity**: Keep the successful `((theta + np.pi) % (2*np.pi)) - np.pi` normalization and \u00b1100 N clipping, but ensure all internal state computations (especially in A/B derivation) consistently use the same `[x, theta, dx, dtheta]` ordering and physical constants as the simulator. This prevents silent indexing or scaling bugs during mutation and preserves the tight model-reality alignment that underpins the current best\u2019s reliability.",
    "1. **Refine the joint friction term in the A matrix to exactly match the simulator\u2019s dissipative structure**: Replace the current `A[3,3] = -b_j / (m * l**2 * (4.0/3.0 - m / Mtot))` with `-b_j / (m * l * denom0)`, where `denom0 = l * (4.0/3.0 - m / Mtot)`, to ensure consistency with the nonlinear dynamics\u2019 denominator. This subtle correction aligns the linear model\u2019s damping with the true rotational inertia used in simulation\u2014mirroring the physics fidelity of Gen 27 and potentially improving LQR gain accuracy under high joint friction.\n\n2. **Replace piecewise linear gain scheduling with a smooth, differentiable modulation function**: Substitute the current `max(0.0, abs(theta) - threshold)` logic with a tanh-based smoother like `pos_gain = 1.0 + 0.5 * np.tanh(k * (abs(theta) - 0.6))` (e.g., `k=5`) to eliminate potential control discontinuities during threshold crossing. This preserves the adaptive authority for large angles while enhancing numerical stability\u2014extending the successful pattern from Gen 17 without sacrificing responsiveness.\n\n3. **Introduce a minimal energy-pumping swing-up trigger for extreme starts (|\u03b8| > 0.85 rad)**: Add a conditional pre-stabilization mode that applies `u = 80 * np.sign(theta * dtheta)` only when `abs(theta) > 0.85`, deactivating once `abs(theta) \u2264 0.75`. This directly addresses the marginal recovery at \u03b8\u2080=0.9 rad by injecting controlled energy early\u2014leveraging the current best\u2019s robust baseline while extending its domain of attraction without complex observers or mode blending.\n\n4. **Slightly increase angular velocity weighting in Q to further accelerate convergence**: Adjust `Q = np.diag([4.2, 38.0, 0.5, 2.0])` to `[4.0, 38.0, 0.5, 2.5]` to penalize residual angular motion more heavily, reducing oscillation decay time. Given the current program\u2019s stable foundation and accurate friction modeling, this targeted tuning can boost time_bonus by shortening stabilization beyond 251 steps while maintaining near-zero final error and low energy use.\n\n5. **Maintain strict state ordering and angle normalization but add runtime validation of physical constants**: Embed assertions or inline checks ensuring `M_POLE`, `L_COM`, and friction coefficients used in A/B derivation exactly match those in `simulate_pendulum_step`. This prevents silent drift during mutation\u2014preserving the critical model-reality alignment that enables the current best\u2019s reliability, especially as pole mass and length increase challenge controller robustness.",
    "1. **Refine the joint friction term in the A matrix to use `denom0` consistently**: Replace the current expression `-b_j / (m * l**2 * (4.0/3.0 - m / Mtot))` with `-b_j / (m * l * denom0)` where `denom0 = l * (4.0/3.0 - m / Mtot)`, ensuring full alignment between the linearized model and the nonlinear simulator\u2019s denominator structure\u2014this subtle correction matches the physics fidelity seen in Gen 27 and may improve LQR gain accuracy under high joint friction.\n\n2. **Smooth the piecewise gain scheduling using a differentiable activation function**: Replace the `max(0.0, abs(theta) - threshold)` logic with a tanh-based smoother like `pos_gain = 1.0 + 0.5 * np.tanh(5 * (abs(theta) - 0.6))` to eliminate potential control discontinuities at thresholds while preserving adaptive authority\u2014extending the robustness of Gen 33\u2019s approach without sacrificing responsiveness during large deviations.\n\n3. **Add a minimal energy-pumping swing-up trigger for extreme initial angles**: Introduce a conditional pre-stabilization mode that applies `u = 80 * np.sign(theta * dtheta)` only when `abs(theta) > 0.85`, deactivating once `abs(theta) \u2264 0.75`. This directly addresses marginal recovery from \u03b8\u2080=0.9 rad by injecting controlled energy early, leveraging Gen 33\u2019s stable baseline to extend its domain of attraction without complex observers.\n\n4. **Increase angular velocity weighting in Q slightly to accelerate convergence**: Adjust `Q = np.diag([4.2, 38.0, 0.5, 2.0])` to `[4.2, 38.0, 0.5, 2.5]` to penalize residual angular motion more heavily, reducing oscillation decay time. Given the accurate friction modeling and stable foundation of Gen 33, this targeted tuning can boost time_bonus by shortening stabilization beyond 251 steps while maintaining low energy use.\n\n5. **Embed runtime validation of physical constants used in A/B derivation**: Add inline assertions or checks ensuring `M_POLE`, `L_COM`, and friction coefficients in the controller\u2019s `__init__` exactly match those in `simulate_pendulum_step`. This prevents silent drift during mutation\u2014preserving the critical model-reality alignment that enables Gen 33\u2019s reliability, especially as pole mass and length increase challenge controller robustness.",
    "1. **Refine adaptive gain scheduling by decoupling position and velocity gains with state-dependent blending**: Instead of multiplying `pos_gain` and `vel_gain`, use a weighted blend based on the dominant error mode (e.g., `adaptive_gain = w * pos_gain + (1-w) * vel_gain` where `w = sigmoid(k*(abs(theta) - abs(dtheta)))`). This builds on the successful tanh-based smoothness while allowing more nuanced adaptation\u2014potentially reducing overreaction when both angle and velocity are large but opposing.\n\n2. **Introduce integral action on cart position via augmented LQR to eliminate steady-state x-error**: Extend the state vector to include `\u222bx dt` and augment A/B matrices accordingly, then recompute LQR gains with a small weight on the integral state. Given that the current best achieves only `stable_x_error = 0.03 m`, this physics-consistent augmentation could drive final_x_error to true zero without destabilizing, leveraging the already precise linear model.\n\n3. **Optimize Q-matrix weights using gradient-free search around current high-performing values**: Systematically perturb `[4.0, 38.0, 0.5, 3.0]` within \u00b110% using CMA-ES or grid search, keeping R=1.0 fixed. The success of aggressive angular weighting (Q[1]=38.0) and strong damping (Q[3]=3.0) suggests nearby optima may yield faster stabilization (<198 steps) and higher time_bonus, especially since energy efficiency is already excellent.\n\n4. **Replace Euler integration in simulation-aware controller design with trapezoidal or RK2 for better discrete-time matching**: Although the controller uses continuous LQR, discretizing the system via zero-order hold (using `scipy.signal.cont2discrete`) before solving DARE could improve gain accuracy at DT=0.02s. This aligns with the program\u2019s physics-fidelity ethos and may further reduce transient overshoot during large-angle recovery.\n\n5. **Add a minimal, differentiable swing-up assist term active only near |\u03b8|\u2248\u03c0 using cos(\u03b8) shaping**: Inject a small corrective bias like `u_swing = \u03b1 * cos(theta) * dtheta` (\u03b1\u22485\u201310) into the control law when `abs(theta) > 0.85`, smoothly gated by a tanh envelope. Unlike Gen 39\u2019s failed sign-based logic, this energy-informed term would complement\u2014not replace\u2014the adaptive LQR, potentially accelerating initial swing-up from \u03b8\u2080=0.9 rad while preserving stability guarantees.",
    "1. **Refine the error-mode-weighted blending factor with a smoother, differentiable weighting function**: Replace the current `theta_weight = abs(theta)/(abs(theta)+abs(dtheta)*0.3+0.01)` with a sigmoid-based blend like `theta_weight = 1 / (1 + np.exp(-k*(abs(theta) - \u03b1*abs(dtheta))))`, where `k\u22488` and `\u03b1\u22480.3`. This maintains the physics-informed emphasis on dominant error modes while ensuring full differentiability\u2014potentially improving gradient-based tuning compatibility and reducing abrupt transitions during mode switches.\n\n2. **Augment the LQR state with a cart-position integral term to eliminate residual x-error**: Extend the state vector to `[x, \u03b8, dx, d\u03b8, \u222bx dt]`, augment A/B matrices accordingly using the same friction-aware linearization as in the current best program, and solve for new gains with a small Q weight (e.g., 0.1) on the integral state. Given that stable_x_error = 0.03 is the only non-zero terminal error, this minimal augmentation could drive final_x_error to true zero without destabilizing the already robust controller.\n\n3. **Optimize Q-matrix entries via local search around [4.5, 44.0, 0.6, 3.2] using a fine-grained grid or CMA-ES**: Systematically explore \u00b15% perturbations of each diagonal entry while keeping R=1.0 fixed, prioritizing improvements in angular penalty (Q[1]) and damping (Q[3]). The current best\u2019s high score sensitivity to these weights\u2014evidenced by Gen 41 vs Gen 43\u2014suggests even minor adjustments could reduce stabilization time below 196 steps and increase time_bonus.\n\n4. **Replace Euler integration in the control law\u2019s implicit assumptions with a zero-order hold discretization for more accurate discrete-time LQR gains**: Use `scipy.signal.cont2discrete((A, B, ...), DT, method='zoh')` to obtain discrete A_d, B_d, then solve the discrete algebraic Riccati equation (`solve_discrete_are`) instead of the continuous one. This better matches the actual simulation step (DT=0.02s) and aligns with the program\u2019s physics-fidelity principle, potentially reducing transient overshoot during large-angle recovery.\n\n5. **Introduce a differentiable, cos(\u03b8)-shaped swing-up assist term active only near |\u03b8| > 0.85 rad**: Add a small bias `u_swing = \u03b2 * np.tanh(\u03b3*(abs(theta) - 0.85)) * np.cos(theta) * dtheta` (with \u03b2\u22488, \u03b3\u224810) to the base LQR force, smoothly gated so it vanishes when |\u03b8| < 0.8. Unlike failed sign-based logic, this energy-aware term complements the adaptive LQR during early transients from \u03b8\u2080=0.9 rad, potentially accelerating initial upright convergence without compromising stability.",
    "1. **Replace the current piecewise linear blending factor with a smooth, differentiable sigmoid-based error-mode selector**: Implement `theta_weight = 1 / (1 + np.exp(-8*(abs(theta) - 0.3*abs(dtheta))))` to replace the existing `abs(theta)/(abs(theta)+0.3*abs(dtheta)+0.01)`. This maintains the physics-informed emphasis on dominant error modes while ensuring full differentiability and smoother transitions\u2014potentially improving robustness during rapid shifts between angle- and velocity-dominated regimes, as suggested by the marginal gains seen between Gen 41 and Gen 43.\n\n2. **Introduce a minimal integral augmentation for cart position without destabilizing angular control**: Extend the state vector to `[x, \u03b8, dx, d\u03b8, \u222bx dt]`, augment A and B matrices using the same friction-aware `denom0` structure, and assign a small Q weight (e.g., 0.1) to the integral state. Given that the current best program already achieves `final_theta_error = 0.00` but has `stable_x_error = 0.03`, this targeted augmentation could eliminate residual position drift while preserving the aggressive angular stabilization that enables its 196-step convergence.\n\n3. **Fine-tune Q-matrix entries via local search around [4.5, 44.0, 0.6, 3.2] with emphasis on angular damping**: Systematically explore \u00b13% perturbations of Q[1] (angle) and Q[3] (angular velocity), as these directly govern stabilization speed and overshoot. The 54-point gap between Gen 43 and Gen 46\u2014driven largely by a 8-step reduction in stabilization time\u2014confirms high sensitivity to these weights; even minor increases in Q[3] may further suppress early oscillations during recovery from \u03b8\u2080=0.9 rad.\n\n4. **Add a differentiable swing-up assist term gated by |\u03b8| > 0.85 using cos(\u03b8)-shaped energy injection**: Introduce `u_swing = 8.0 * np.tanh(10.0*(abs(theta) - 0.85)) * np.cos(theta) * dtheta` and add it to the base LQR force. Unlike binary logic, this smooth, physics-aligned term provides gentle momentum assistance during the critical initial swing-down phase from 0.9 rad, potentially reducing stabilization time below 196 steps without compromising stability once upright.\n\n5. **Switch from continuous-time LQR to zero-order hold discrete-time LQR matching DT=0.02s**: Use `scipy.signal.cont2discrete((A, B, ...), DT, method='zoh')` to compute discrete A_d and B_d, then solve the discrete algebraic Riccati equation (`solve_discrete_are`). Since the simulator uses Euler integration with fixed DT, this better aligns the controller\u2019s implicit model with actual dynamics\u2014reducing transient mismatch during large corrections and potentially improving early-phase energy efficiency, which directly boosts the energy bonus already maximized at 2494.16.",
    "1. **Refine Q-matrix weights via targeted local search around [4.5, 44.0, 0.6, 3.2], prioritizing slight increases to Q[3] (angular velocity penalty)**: Given that Generation 50\u2019s stabilization time (192 steps) is already near-optimal but still shows minor early oscillations during swing-down from 0.9 rad, a \u00b12% perturbation of Q[3]\u2014e.g., testing 3.25 or 3.3\u2014could further dampen angular overshoot without sacrificing responsiveness, as evidenced by the strong correlation between Q[3] and stabilization speed in Generations 43\u201350.\n\n2. **Replace the current multiplicative tanh gain scheduling with an additive, physics-informed swing-up assist term**: Instead of scaling the entire LQR force, introduce a separate smooth term like `u_swing = 6.0 * np.tanh(8.0 * (abs(theta) - 0.8)) * np.cos(theta) * dtheta` that injects energy only when |\u03b8| > 0.8. This builds on the success of smooth activation (tanh) while decoupling swing-up assistance from upright regulation, potentially reducing initial correction aggressiveness and saving energy\u2014aligning with Gen 50\u2019s efficiency but improving transient dynamics.\n\n3. **Transition from continuous-time LQR to discrete-time LQR using zero-order hold (ZOH) with DT=0.02s**: Since the simulator uses explicit Euler integration with fixed DT, the current continuous LQR solution introduces a small model mismatch during large transients. Recomputing A_d and B_d via `scipy.signal.cont2discrete((A, B), DT, method='zoh')` and solving the discrete Riccati equation will better align the controller\u2019s prediction horizon with actual dynamics, likely improving early-phase force accuracy and boosting the already high energy bonus.\n\n4. **Tighten angle normalization robustness by adding a small hysteresis buffer to prevent rapid gain toggling near \u00b1\u03c0**: Although modulo-based normalization works well, at angles near \u00b1\u03c0 (e.g., during fast rotations), numerical noise can cause sign flips that trigger unnecessary gain changes. Implement `theta = ((theta + np.pi + 1e-4) % (2 * np.pi)) - np.pi` or clamp extreme values post-normalization to ensure consistent error magnitude interpretation\u2014critical for stable tanh thresholding under worst-case initialization.\n\n5. **Introduce minimal cart-position integral action *only after* stabilization is detected, using a soft switch based on |\u03b8| < 0.1**: To address the residual `stable_x_error = 0.03` without destabilizing angular control, augment the state with \u222bx dt but activate its feedback gain via `K_int * np.tanh(10*(0.1 - abs(theta)))`. This ensures integral correction is suppressed during swing-up (preserving Gen 50\u2019s fast convergence) but eliminates steady-state drift once upright\u2014leveraging the proven physics-aware linear model while cautiously extending functionality.",
    "1. **Fine-tune the Q-matrix by slightly increasing the angular velocity penalty (Q[3]) to 3.25\u20133.30 while preserving other weights**: The current best uses Q = diag([4.5, 44.0, 0.6, 3.2]) and stabilizes in 192 steps, but minor early oscillations during swing-down from 0.9 rad suggest room for improved damping. Given the strong correlation between higher Q[3] and faster convergence in Generations 43\u201359, a targeted \u00b11.5% increase could reduce overshoot without compromising responsiveness or energy efficiency.\n\n2. **Replace multiplicative gain scheduling with an additive, physics-informed swing-up assist term activated only for |\u03b8| > 0.8**: Instead of scaling the entire LQR output, introduce a smooth auxiliary force like `u_swing = k * np.tanh(\u03b1*(abs(theta) - 0.8)) * np.sign(theta) * dtheta` that injects corrective energy during large deviations. This decouples aggressive swing-up from precise upright regulation\u2014preserving the base LQR\u2019s optimality near equilibrium while enhancing initial recovery robustness, as hinted by the success of tanh-based activation in top performers.\n\n3. **Implement discrete-time LQR using zero-order hold (ZOH) discretization with DT=0.02s to align controller prediction with simulator integration**: The current continuous-time LQR assumes infinitesimal time steps, but the simulator uses explicit Euler with fixed DT=0.02s, causing small model mismatch during transients. Recomputing A_d and B_d via `scipy.signal.cont2discrete((A, B), DT, method='zoh')` and solving the discrete Riccati equation will better match actual dynamics, likely improving early-force accuracy and boosting time/energy bonuses.\n\n4. **Add a soft-switched integral term on cart position that activates only when |\u03b8| < 0.1 to eliminate residual steady-state error (currently stable_x_error=0.03)**: Augment the state vector with \u222bx dt and compute an extended LQR gain, but scale its contribution by `np.tanh(10*(0.1 - abs(theta)))` to suppress it during swing-up. This leverages the proven physics-accurate linear model while cautiously addressing the only remaining performance gap in the best program\u2014without reintroducing the lag seen in full integral controllers like Gen 55.\n\n5. **Introduce hysteresis-aware angle normalization to prevent numerical sign flips near \u00b1\u03c0 during fast rotations**: Modify the normalization to `theta = ((theta + np.pi + 1e-5) % (2 * np.pi)) - np.pi` and optionally clamp `theta` to [-\u03c0+\u03b5, \u03c0-\u03b5] post-normalization. This ensures consistent magnitude interpretation for the tanh gain scheduler under worst-case initialization (e.g., rapid swings crossing \u00b1\u03c0), preventing spurious gain changes that could destabilize recovery\u2014building on the current program\u2019s robust normalization while hardening it against edge-case noise.",
    "1. **Increase the angular velocity weight in the Q-matrix from 3.2 to 3.25\u20133.30 while preserving other entries** to enhance damping during the swing-down phase from 0.9 rad. The current best program shows minor early oscillations, and historical data (Generations 43\u201359) confirms that slightly higher Q[3] correlates with faster convergence without degrading energy efficiency\u2014potentially reducing stabilization time below 192 steps.\n\n2. **Replace multiplicative gain scheduling with an additive, physics-informed swing-up assist term activated only for |\u03b8| > 0.8 rad**, such as `u_swing = k * np.tanh(\u03b1*(abs(theta) - 0.8)) * np.sign(theta) * dtheta`. This decouples aggressive large-angle recovery from precise upright regulation, leveraging the proven effectiveness of tanh-based activation while preserving the base LQR\u2019s optimality near equilibrium\u2014addressing a key limitation of purely scaling the control output.\n\n3. **Implement discrete-time LQR using zero-order hold (ZOH) discretization aligned with DT=0.02s** by computing `(A_d, B_d) = cont2discrete((A, B), DT, method='zoh')` and solving the discrete Riccati equation. The current continuous-time LQR introduces small model mismatch due to Euler integration in simulation; correcting this will improve transient force accuracy, likely boosting time and energy bonuses through better early-stage control.\n\n4. **Add a soft-switched integral action on cart position that activates only when |\u03b8| < 0.1 rad**, augmenting the state with \u222bx dt and scaling its LQR contribution by `np.tanh(10*(0.1 - abs(theta)))`. This directly targets the remaining stable_x_error of 0.03 without reintroducing lag during swing-up\u2014building on the best program\u2019s robustness while cautiously eliminating steady-state drift, unlike failed full-integral approaches.\n\n5. **Introduce hysteresis-aware angle normalization with epsilon padding** via `theta = ((theta + np.pi + 1e-5) % (2 * np.pi)) - np.pi` and optional clamping to [-\u03c0+1e-4, \u03c0-1e-4]. This hardens the existing normalization against numerical sign flips near \u00b1\u03c0 during rapid rotations, ensuring consistent gain scheduler behavior under worst-case dynamics\u2014extending the current program\u2019s reliable tanh activation logic to edge-case scenarios.",
    "1. **Fine-tune the Q-matrix by incrementally increasing the angular velocity weight from 3.2 to 3.28** while keeping other entries fixed, based on historical evidence (Generations 43\u201359) that slightly higher Q[3] reduces early oscillations during swing-down from 0.9 rad without degrading energy efficiency\u2014potentially shaving steps off the current 192-step stabilization time.\n\n2. **Replace multiplicative gain scheduling with an additive swing-up assist term**: Introduce a physics-informed feedforward term like `u_swing = 8.0 * np.tanh(6.0 * max(0.0, abs(theta) - 0.8)) * np.sign(theta) * dtheta` that activates only for large angles (>0.8 rad), decoupling aggressive recovery from precise upright regulation and preserving the base LQR\u2019s optimality near equilibrium\u2014building on the proven tanh activation logic while avoiding gain-induced distortion of optimal feedback.\n\n3. **Implement discrete-time LQR using zero-order hold (ZOH) discretization aligned with DT=0.02s**: Replace the continuous-time Riccati solution with `(A_d, B_d) = cont2discrete((A, B), DT, method='zoh')` and solve the discrete algebraic Riccati equation to eliminate model mismatch between controller design and Euler-integrated simulation dynamics\u2014enhancing transient force accuracy during critical early steps when |\u03b8| \u2248 0.9 rad.\n\n4. **Add soft-switched integral action on cart position activated only when |\u03b8| < 0.1 rad**: Augment the state vector with \u222bx dt and scale its LQR contribution by `np.tanh(12.0 * (0.1 - abs(theta)))`, directly targeting the residual stable_x_error of 0.03 without reintroducing lag during swing-up\u2014leveraging the best program\u2019s robustness to safely eliminate steady-state drift where prior full-integral attempts failed.\n\n5. **Enhance angle normalization with epsilon padding and clamping**: Modify normalization to `theta = ((theta + np.pi + 1e-6) % (2 * np.pi)) - np.pi` followed by optional clamping to [-\u03c0+1e-5, \u03c0-1e-5], hardening against numerical sign flips near \u00b1\u03c0 during rapid rotations and ensuring consistent tanh scheduler behavior under worst-case dynamics\u2014extending the current program\u2019s reliable gain activation to edge cases without altering core logic.",
    "1. **Refine the additive swing-up assist with angle-velocity phase-aware modulation**: Enhance the current `u_swing` term by replacing the binary `theta * dtheta > 0` check with a continuous falling-severity factor like `np.tanh(3.0 * theta * dtheta)`, which smoothly scales assistance based on how rapidly the pole is diverging from upright. This builds on the proven physics-informed assist while better matching the nonlinear energy dynamics during critical early swing-up from 0.9 rad.\n\n2. **Tune the LQR Q-matrix\u2019s angular velocity weight (Q[3]) from 3.2 to 3.28** as suggested by historical trends (Generations 43\u201359), which showed reduced oscillation during swing-down without increasing energy use. Given the current best program already achieves minimal energy consumption (0.01/step), this small increase in damping weight could further reduce stabilization time below 192 steps and improve terminal cart position error (<0.03).\n\n3. **Replace continuous-time LQR with discrete-time ZOH-based design aligned to DT=0.02s**: Recompute the gain matrix using `scipy.signal.cont2discrete` with zero-order hold and solve the discrete algebraic Riccati equation. This eliminates model mismatch between the controller\u2019s assumed dynamics and the Euler-integrated simulator\u2014especially impactful during high-acceleration swing-up phases where timing precision directly affects force accuracy and time bonus.\n\n4. **Introduce soft-switched integral action on cart position, active only near equilibrium**: Augment the state vector with \u222bx dt and scale its feedback contribution by `np.tanh(12.0 * (0.1 - abs(theta)))`, directly targeting the residual `stable_x_error: 0.03` without reintroducing lag during large-angle recovery. The current program\u2019s robustness to 0.9 rad provides a safe foundation to eliminate steady-state drift where earlier full-integral attempts failed.\n\n5. **Strengthen angle normalization with epsilon padding to prevent sign ambiguity near \u00b1\u03c0**: Modify normalization to `theta = ((theta + np.pi + 1e-6) % (2 * np.pi)) - np.pi` before computing `abs(theta)` for gain scheduling and swing-up activation. This hardens the controller against numerical edge cases during rapid rotations\u2014ensuring consistent tanh scheduler behavior under worst-case transients while preserving all successful logic from the current best program.",
    "1. **Enhance the falling-severity factor in swing-up assist with normalized angular momentum**: Replace the current `theta * dtheta / (abs(theta) + 0.1)` term with a physically normalized divergence metric like `(theta * dtheta) / (L_COM * np.sqrt(G / L_COM))`, which scales the falling rate by the pendulum\u2019s natural frequency. This preserves the continuous, differentiable nature of the current best program\u2019s assist while making the modulation dimensionally consistent and more robust across varying pole lengths\u2014directly addressing the challenge posed by the unusually long (2.5 m) pole.\n\n2. **Implement discrete-time LQR using zero-order hold to align with Euler integration**: Recompute the gain matrix `K` using `scipy.signal.cont2discrete((A, B, Q, R), DT, method='zoh')` followed by solving the discrete algebraic Riccati equation. The current continuous-time LQR introduces a subtle timing mismatch during high-acceleration phases (e.g., early swing-up from 0.9 rad), and correcting this\u2014as suggested by prior insight #3\u2014could shave critical steps off the 192-step stabilization time, directly boosting the time bonus which is the key differentiator among top performers.\n\n3. **Introduce soft-switched integral action on cart position activated only near equilibrium**: Augment the state vector in `get_action` with an integrated cart error `x_int += x * DT`, but scale its feedback contribution by `np.tanh(12.0 * (0.1 - abs(theta)))` so it only engages when `|\u03b8| < 0.1`. This directly targets the residual `stable_x_error: 0.03` without destabilizing large-angle recovery, leveraging the current program\u2019s robustness as a foundation\u2014consistent with successful hybrid architectures that separate transient and steady-state control.\n\n4. **Tune the angular velocity weight in Q-matrix from 3.2 to 3.28 based on historical damping trends**: Adjust `Q[3] = 3.28` in the diagonal Q-matrix to slightly increase damping on `dtheta`, as observed in Generations 43\u201359 to reduce post-swing-down oscillations. Given the current program already achieves ultra-low energy use (0.01/step), this minimal increase won\u2019t penalize efficiency but could improve terminal cart positioning and marginally accelerate convergence\u2014pushing beyond the 0.04-point edge seen in Generation 79.\n\n5. **Strengthen angle normalization with explicit handling of \u00b1\u03c0 discontinuity during rapid rotation**: Modify the normalization step to `theta = np.arctan2(np.sin(theta), np.cos(theta))` instead of modulo arithmetic. While the current `((theta + \u03c0 + 1e-8) % 2\u03c0) - \u03c0` works for slow dynamics, `arctan2` is numerically superior under fast rotations (possible during aggressive swing-up), ensuring `abs(theta)` remains accurate even if the pole spins\u2014preserving correct activation of both gain scheduling and swing-up logic under worst-case transients.",
    "1. **Replace angle normalization with `np.arctan2(np.sin(theta), np.cos(theta))` for robustness during rapid rotation**: The current modulo-based normalization works well under slow dynamics but may misrepresent angle magnitude if the pole spins rapidly during aggressive swing-up from 0.9 rad. Using `arctan2` ensures accurate `abs(theta)` estimation even under worst-case transients, preserving correct activation of both gain scheduling and swing-up logic\u2014directly supporting the hybrid architecture\u2019s reliance on precise large-angle detection.\n\n2. **Implement discrete-time LQR via zero-order hold to eliminate timing mismatch in high-acceleration phases**: Recompute the gain matrix using `scipy.signal.cont2discrete((A, B, Q, R), DT, method='zoh')` followed by solving the discrete algebraic Riccati equation. This aligns the controller with the Euler integration used in simulation, reducing phase lag during early swing-up and potentially shaving critical steps off the current 192-step stabilization time\u2014directly boosting the decisive time bonus.\n\n3. **Augment control with soft-switched integral action on cart position, active only near equilibrium**: Introduce an integrated cart error term `x_int += x * DT` and feed it back through a tanh-scheduled gain `np.tanh(12.0 * (0.1 - abs(theta)))`, ensuring contribution only when `|\u03b8| < 0.1`. This targets the residual `stable_x_error: 0.03` without destabilizing large-angle recovery, leveraging the current program\u2019s robust transient handling while enforcing terminal accuracy needed for full success bonus retention.\n\n4. **Refine the falling-severity factor using dimensionless angular momentum normalized by natural frequency**: Replace `theta * dtheta` in the falling-severity term with `(theta * dtheta) / (L_COM * np.sqrt(G / L_COM))`, which scales the product by the pendulum\u2019s characteristic dynamics. This maintains differentiability and continuity while improving generalization across the unusually long (2.5 m) pole, making swing-up assist more physically consistent and effective under extreme parameters.\n\n5. **Slightly increase angular velocity weight in Q-matrix from 3.2 to 3.28 to damp post-swing-down oscillations**: Adjust `Q[3] = 3.28` based on historical trends showing reduced overshoot and faster convergence with marginally higher damping on `dtheta`. Given the current ultra-low energy use (0.01/step), this minimal change won\u2019t penalize efficiency but could improve terminal cart positioning and accelerate final convergence\u2014pushing beyond the current 0.1-point performance ceiling observed between Generations 80 and 82.",
    "1. **Replace modulo-based angle normalization with `np.arctan2(np.sin(theta), np.cos(theta))` to ensure robust large-angle detection during aggressive swing-up**: The current best program uses modulo arithmetic for angle wrapping, which can misrepresent the true angular magnitude if rapid rotation occurs during swing-up from 0.9 rad. Switching to `arctan2` guarantees a physically consistent `abs(theta)` estimate even under worst-case transients, preserving correct activation of both gain scheduling and swing-up logic\u2014critical for maintaining the hybrid architecture\u2019s reliability.\n\n2. **Implement discrete-time LQR via zero-order hold (`scipy.signal.cont2discrete`) instead of continuous-time Riccati solution**: The simulator uses Euler integration (effectively a discrete-time system), while the current controller solves continuous-time LQR. Recomputing gains using ZOH discretization aligns the control law with the actual dynamics, reducing phase lag in early swing-up and potentially accelerating stabilization below 192 steps\u2014directly boosting the decisive time bonus without affecting energy efficiency.\n\n3. **Introduce soft-switched integral action on cart position that activates only when |\u03b8| < 0.1 rad using tanh scheduling**: Although the current program achieves near-perfect terminal accuracy, residual `stable_x_error: 0.03` suggests minor steady-state drift. Adding an integrated cart error term gated by `np.tanh(12.0 * (0.1 - abs(theta)))` would eliminate this offset without destabilizing large-angle recovery, leveraging the existing robust transient handling while securing full success bonus retention across all seeds.\n\n4. **Refine the falling-severity factor to use dimensionless angular momentum normalized by natural frequency**: Replace `theta * dtheta` in `falling_severity = 1.0 + np.tanh(3.0 * theta * dtheta)` with `(theta * dtheta) / (L_COM * np.sqrt(G / L_COM))`. This normalization accounts for the unusually long pole (2.5 m) and ensures swing-up assist scales appropriately with the system\u2019s intrinsic dynamics, improving physical consistency and effectiveness under extreme parameters without sacrificing differentiability.\n\n5. **Slightly increase the angular velocity weight in Q-matrix from 3.2 to 3.28 to damp post-swing-down oscillations**: Historical data shows marginal increases in `Q[3]` reduce overshoot and accelerate final convergence without penalizing energy use (which remains at 0.01/step). Given the current ultra-low energy consumption, this minimal tuning could further reduce stabilization time and improve terminal cart positioning\u2014pushing total score beyond the current 4920.22 ceiling.",
    "1. **Replace modulo-based angle normalization with `np.arctan2(np.sin(theta), np.cos(theta))` to ensure robust large-angle estimation during aggressive swing-up transients**  \n   The current best program uses modulo arithmetic for angle wrapping, which can misrepresent angular magnitude under rapid rotation from the 0.9 rad start. Switching to `arctan2` guarantees a physically consistent `abs(theta)` for both gain scheduling and swing-up activation, preserving the reliability of the multiplicative adaptive logic under worst-case dynamics.\n\n2. **Recompute LQR gains using discrete-time zero-order hold (ZOH) discretization instead of continuous-time Riccati solution**  \n   Since the simulator uses Euler integration (a discrete-time system), aligning the controller\u2019s linear model via `scipy.signal.cont2discrete` will reduce phase lag in early swing-up response. This small but critical alignment could accelerate stabilization below 192 steps\u2014directly boosting the dominant time bonus without affecting energy efficiency.\n\n3. **Refine the falling-severity factor to use dimensionless angular momentum normalized by the pole\u2019s natural frequency**  \n   Replace `theta * dtheta` in `falling_severity = 1.0 + np.tanh(3.0 * theta * dtheta)` with `(theta * dtheta) / (L_COM * np.sqrt(G / L_COM))`. This normalization accounts for the unusually long pole (2.5 m), ensuring swing-up assist scales correctly with intrinsic dynamics while maintaining smooth differentiability\u2014enhancing physical consistency without destabilizing control.\n\n4. **Slightly increase the angular velocity weight in the Q-matrix from 3.2 to 3.28 to damp post-swing-down oscillations**  \n   Historical data shows this minimal increase reduces overshoot and accelerates final convergence without increasing energy consumption (which remains at 0.01/step). Given the ultra-low energy usage and residual `stable_x_error: 0.03`, this tuning could shave off a few stabilization steps and improve terminal accuracy\u2014pushing the score beyond 4920.22.\n\n5. **Introduce soft-switched integral action on cart position gated by upright proximity using tanh scheduling**  \n   Add an integrated cart error term scaled by `np.tanh(12.0 * (0.1 - abs(theta)))` to eliminate the minor steady-state drift (`stable_x_error: 0.03`) without interfering with large-angle recovery. This leverages the existing robust transient handling while securing perfect terminal positioning across all seeds\u2014maximizing success bonus retention without sacrificing speed or energy.",
    "1. **Replace modulo-based angle normalization with `np.arctan2(np.sin(theta), np.cos(theta))` to ensure robust angular magnitude estimation during aggressive transients**  \n   The current best uses modulo arithmetic which can misrepresent angle magnitude under rapid rotation from the 0.9 rad start. Switching to `arctan2` guarantees a physically consistent `abs(theta)` for both gain scheduling and swing-up activation, preserving the reliability of the multiplicative adaptive logic under worst-case dynamics\u2014directly addressing a subtle vulnerability shared by all top performers.\n\n2. **Recompute LQR gains using discrete-time zero-order hold (ZOH) discretization via `scipy.signal.cont2discrete` before solving the Riccati equation**  \n   Since the simulator uses Euler integration (a discrete-time system), aligning the controller\u2019s linear model with ZOH will reduce phase lag in early swing-up response. This small but critical alignment could accelerate stabilization below 192 steps\u2014directly boosting the dominant time bonus without affecting energy efficiency, as the current continuous-time LQR is slightly mismatched to the simulation step.\n\n3. **Refine the falling-severity factor to use dimensionless angular momentum normalized by the pole\u2019s natural frequency: replace `theta * dtheta` with `(theta * dtheta) / (L_COM * np.sqrt(G / L_COM))`**  \n   Given the unusually long pole (2.5 m), this normalization ensures swing-up assist scales correctly with intrinsic dynamics while maintaining smooth differentiability. It enhances physical consistency of the additive swing-up term\u2014already a key success factor\u2014without destabilizing control, potentially improving recovery speed from extreme initial conditions.\n\n4. **Slightly increase the angular velocity weight in the Q-matrix from 3.2 to 3.28 to damp post-swing-down oscillations and reduce `stable_x_error` from 0.03 to near-zero**  \n   Historical data shows this minimal increase reduces overshoot and accelerates final convergence without increasing energy consumption (which remains at 0.01/step). Given the ultra-low energy usage and residual position drift, this tuning could shave off a few stabilization steps and improve terminal accuracy\u2014pushing the score beyond 4920.22.\n\n5. **Introduce soft-switched integral action on cart position gated by upright proximity using tanh scheduling: add `K_i * np.tanh(12.0 * (0.1 - abs(theta))) * integral_x` to the force output**  \n   This leverages the existing robust transient handling while eliminating the minor steady-state drift (`stable_x_error: 0.03`) without interfering with large-angle recovery. The gating ensures integral action only activates when the pole is nearly upright (<0.1 rad), securing perfect terminal positioning across all seeds\u2014maximizing success bonus retention without sacrificing speed or energy.",
    "1. **Replace modulo-based angle normalization with `np.arctan2(np.sin(theta), np.cos(theta))` to ensure robust angular magnitude estimation during aggressive transients**  \n   The current best uses modulo arithmetic which can misrepresent angle magnitude under rapid rotation from the 0.9 rad start. Switching to `arctan2` guarantees a physically consistent `abs(theta)` for both gain scheduling and swing-up activation, preserving the reliability of the multiplicative adaptive logic under worst-case dynamics\u2014directly addressing a subtle vulnerability shared by all top performers.\n\n2. **Recompute LQR gains using discrete-time zero-order hold (ZOH) discretization via `scipy.signal.cont2discrete` before solving the Riccati equation**  \n   Since the simulator uses Euler integration (a discrete-time system), aligning the controller\u2019s linear model with ZOH will reduce phase lag in early swing-up response. This small but critical alignment could accelerate stabilization below 192 steps\u2014directly boosting the dominant time bonus without affecting energy efficiency, as the current continuous-time LQR is slightly mismatched to the simulation step.\n\n3. **Refine the falling-severity factor to use dimensionless angular momentum normalized by the pole\u2019s natural frequency: replace `theta * dtheta` with `(theta * dtheta) / (L_COM * np.sqrt(G / L_COM))`**  \n   Given the unusually long pole (2.5 m), this normalization ensures swing-up assist scales correctly with intrinsic dynamics while maintaining smooth differentiability. It enhances physical consistency of the additive swing-up term\u2014already a key success factor\u2014without destabilizing control, potentially improving recovery speed from extreme initial conditions.\n\n4. **Slightly increase the angular velocity weight in the Q-matrix from 3.2 to 3.28 to damp post-swing-down oscillations and reduce `stable_x_error` from 0.03 to near-zero**  \n   Historical data shows this minimal increase reduces overshoot and accelerates final convergence without increasing energy consumption (which remains at 0.01/step). Given the ultra-low energy usage and residual position drift, this tuning could shave off a few stabilization steps and improve terminal accuracy\u2014pushing the score beyond 4920.22.\n\n5. **Introduce soft-switched integral action on cart position gated by upright proximity using tanh scheduling: add `K_i * np.tanh(12.0 * (0.1 - abs(theta))) * integral_x` to the force output**  \n   This leverages the existing robust transient handling while eliminating the minor steady-state drift (`stable_x_error: 0.03`) without interfering with large-angle recovery. The gating ensures integral action only activates when the pole is nearly upright (<0.1 rad), securing perfect terminal positioning across all seeds\u2014maximizing success bonus retention without sacrificing speed or energy.",
    "1. **Replace modulo-based angle normalization with `np.arctan2(np.sin(theta), np.cos(theta))` to ensure robust angular magnitude estimation during aggressive transients**  \n   The current best uses modulo arithmetic which can misrepresent angle magnitude under rapid rotation from the 0.9 rad start. Switching to `arctan2` guarantees a physically consistent `abs(theta)` for both gain scheduling and swing-up activation, preserving the reliability of the multiplicative adaptive logic under worst-case dynamics\u2014directly addressing a subtle vulnerability shared by all top performers.\n\n2. **Recompute LQR gains using discrete-time zero-order hold (ZOH) discretization via `scipy.signal.cont2discrete` before solving the Riccati equation**  \n   Since the simulator uses Euler integration (a discrete-time system), aligning the controller\u2019s linear model with ZOH will reduce phase lag in early swing-up response. This small but critical alignment could accelerate stabilization below 192 steps\u2014directly boosting the dominant time bonus without affecting energy efficiency, as the current continuous-time LQR is slightly mismatched to the simulation step.\n\n3. **Refine the falling-severity factor to use dimensionless angular momentum normalized by the pole\u2019s natural frequency: replace `theta * dtheta` with `(theta * dtheta) / (L_COM * np.sqrt(G / L_COM))`**  \n   Given the unusually long pole (2.5 m), this normalization ensures swing-up assist scales correctly with intrinsic dynamics while maintaining smooth differentiability. It enhances physical consistency of the additive swing-up term\u2014already a key success factor\u2014without destabilizing control, potentially improving recovery speed from extreme initial conditions.\n\n4. **Slightly increase the angular velocity weight in the Q-matrix from 3.2 to 3.28 to damp post-swing-down oscillations and reduce `stable_x_error` from 0.03 to near-zero**  \n   Historical data shows this minimal increase reduces overshoot and accelerates final convergence without increasing energy consumption (which remains at 0.01/step). Given the ultra-low energy usage and residual position drift, this tuning could shave off a few stabilization steps and improve terminal accuracy\u2014pushing the score beyond 4920.22.\n\n5. **Introduce soft-switched integral action on cart position gated by upright proximity using tanh scheduling: add `K_i * np.tanh(12.0 * (0.1 - abs(theta))) * integral_x` to the force output**  \n   This leverages the existing robust transient handling while eliminating the minor steady-state drift (`stable_x_error: 0.03`) without interfering with large-angle recovery. The gating ensures integral action only activates when the pole is nearly upright (<0.1 rad), securing perfect terminal positioning across all seeds\u2014maximizing success bonus retention without sacrificing speed or energy.",
    "1. **Replace modulo-based angle normalization with `np.arctan2(np.sin(theta), np.cos(theta))` to ensure robust angular magnitude estimation during aggressive transients**  \n   The current best uses modulo arithmetic which can misrepresent angle magnitude under rapid rotation from the 0.9 rad start. Switching to `arctan2` guarantees a physically consistent `abs(theta)` for both gain scheduling and swing-up activation, preserving the reliability of the multiplicative adaptive logic under worst-case dynamics\u2014directly addressing a subtle vulnerability shared by all top performers.\n\n2. **Recompute LQR gains using discrete-time zero-order hold (ZOH) discretization via `scipy.signal.cont2discrete` before solving the Riccati equation**  \n   Since the simulator uses Euler integration (a discrete-time system), aligning the controller\u2019s linear model with ZOH will reduce phase lag in early swing-up response. This small but critical alignment could accelerate stabilization below 192 steps\u2014directly boosting the dominant time bonus without affecting energy efficiency, as the current continuous-time LQR is slightly mismatched to the simulation step.\n\n3. **Refine the falling-severity factor to use dimensionless angular momentum normalized by the pole\u2019s natural frequency: replace `theta * dtheta` with `(theta * dtheta) / (L_COM * np.sqrt(G / L_COM))`**  \n   Given the unusually long pole (2.5 m), this normalization ensures swing-up assist scales correctly with intrinsic dynamics while maintaining smooth differentiability. It enhances physical consistency of the additive swing-up term\u2014already a key success factor\u2014without destabilizing control, potentially improving recovery speed from extreme initial conditions.\n\n4. **Slightly increase the angular velocity weight in the Q-matrix from 3.2 to 3.28 to damp post-swing-down oscillations and reduce `stable_x_error` from 0.03 to near-zero**  \n   Historical data shows this minimal increase reduces overshoot and accelerates final convergence without increasing energy consumption (which remains at 0.01/step). Given the ultra-low energy usage and residual position drift, this tuning could shave off a few stabilization steps and improve terminal accuracy\u2014pushing the score beyond 4920.22.\n\n5. **Introduce soft-switched integral action on cart position gated by upright proximity using tanh scheduling: add `K_i * np.tanh(12.0 * (0.1 - abs(theta))) * integral_x` to the force output**  \n   This leverages the existing robust transient handling while eliminating the minor steady-state drift (`stable_x_error: 0.03`) without interfering with large-angle recovery. The gating ensures integral action only activates when the pole is nearly upright (<0.1 rad), securing perfect terminal positioning across all seeds\u2014maximizing success bonus retention without sacrificing speed or energy.",
    "1. **Replace continuous-time LQR with discrete-time ZOH-equivalent gains using `scipy.signal.cont2discrete`**  \n   The current best uses a continuous-time Riccati solution despite the simulator employing Euler integration (a discrete system). Discretizing the `(A, B)` model via zero-order hold before solving the DARE will align controller dynamics with simulation timesteps, reducing phase lag during early swing-up and potentially shaving 1\u20132 steps off the stabilization time (191 \u2192 ~189), directly boosting the dominant time bonus.\n\n2. **Refine normalized falling severity to use signed angular momentum scaled by natural frequency: `(theta * dtheta) / (L_COM * omega_n)` instead of raw `angular_momentum`**  \n   While the current best already normalizes severity, explicitly preserving the sign of `theta * dtheta` ensures swing-up assistance directionally opposes energy-deficient motion (e.g., negative assist when pole is falling left with negative momentum). This physically consistent refinement could improve recovery efficiency from the 0.9 rad start, marginally increasing time bonus without affecting energy.\n\n3. **Introduce state-dependent integral gain decay: scale leak factor by `1 - np.tanh(k * abs(theta))` to accelerate windup prevention during large deviations**  \n   The current leaky integration (`*= 0.95`) is static. Making the decay rate adaptive\u2014faster leakage when `|\u03b8|` is large\u2014would more aggressively suppress integral buildup during swing-up phases, reducing risk of overshoot upon reactivation near upright. This enhances robustness of the soft-switched integral action while preserving its steady-state benefits.\n\n4. **Tune Q-matrix position/velocity coupling by adding small off-diagonal terms (e.g., Q[0,2]=Q[2,0]=0.1) to penalize cart velocity during position correction**  \n   Current diagonal Q assumes decoupled state penalties, but physical coupling exists: correcting x-error often induces dx. Adding mild cross-terms could yield smoother cart motion during final convergence, potentially reducing `stable_x_error` below 0.05 and improving terminal accuracy\u2014critical now that success bonus is consistently achieved.\n\n5. **Implement dual-mode swing-up activation: combine angle-based tanh trigger with energy deficit detection (`E_current < 0.8 * E_upright`)**  \n   The current swing-up activates purely on angle (>0.8 rad), but sometimes the pole has sufficient energy yet wrong phase. Augmenting with an energy-based condition ensures assist only when truly needed, preventing unnecessary force application that wastes energy. Given the plateaued energy efficiency (~6.46 total), this could unlock marginal energy bonus gains while maintaining fast stabilization.",
    "1. **Discretize the LQR controller using zero-order hold to align with Euler integration dynamics**  \n   Replace the continuous-time `solve_continuous_are` with discrete-time LQR via `scipy.signal.cont2discrete((A, B, Q, R), DT, method='zoh')` followed by `solve_discrete_are`. This directly addresses the model-simulator mismatch in time representation, reducing phase lag during early swing-up and potentially improving stabilization time from 191 steps\u2014critical since time bonus dominates scoring.\n\n2. **Enhance normalized falling severity with signed energy-aware momentum: use `(theta * dtheta) / (L_COM * self.omega_n)` instead of raw `angular_momentum`**  \n   The current best already normalizes severity but discards sign information by using `angular_momentum = theta * dtheta` without directional context in assistance scaling. Preserving the sign ensures swing-up force opposes actual energy-deficient motion (e.g., negative assist when falling left), improving recovery efficiency from the challenging 0.9 rad start without increasing energy use.\n\n3. **Implement adaptive integral decay based on pole angle deviation: scale leak factor as `0.95 ** (1 + 5 * np.tanh(8 * abs(theta)))`**  \n   While the current program uses static leaky integration (`*= 0.95`) when |\u03b8| > 0.1, making decay intensity grow with angular deviation would more aggressively suppress windup during aggressive swing-up phases. This builds on the proven soft-switched integral design while enhancing robustness against overshoot upon reactivation near upright.\n\n4. **Introduce mild cross-coupling in Q-matrix between cart position and velocity (e.g., Q[0,2] = Q[2,0] = 0.15)**  \n   The current diagonal Q penalizes states independently, ignoring physical coupling where correcting x-error induces dx. Adding small off-diagonal terms encourages smoother cart motion during final convergence, potentially reducing `stable_x_error` below 0.05\u2014a key differentiator now that success bonus is consistently achieved and steady-state accuracy lags behind perfect final error.\n\n5. **Augment swing-up activation with energy deficit detection: trigger assist only if `E_current < 0.85 * E_upright`, where `E = 0.5*m*(dtheta*L_COM)^2 + m*g*L_COM*(1 - cos(theta))`**  \n   The current angle-only trigger (>0.8 rad) may apply unnecessary force when the pole has sufficient energy but wrong phase. Combining it with an energy threshold ensures assistance is applied only when truly needed, preserving the high time bonus while marginally reducing total energy (currently plateaued at ~6.46), potentially unlocking extra energy bonus points.",
    "1. **Refine integral gating with angle-velocity joint condition to prevent premature activation**: Replace the current angle-only integral gate (`np.tanh(12.0 * (0.1 - abs(theta)))`) with a combined condition that also requires low angular velocity (e.g., `gate = np.tanh(12.0 * (0.1 - abs(theta))) * np.tanh(8.0 * (1.0 - abs(dtheta)))`). This prevents integral buildup during fast-passing upright transients, reducing overshoot risk while preserving steady-state accuracy\u2014building directly on the proven soft-switched integral pattern.\n\n2. **Tune Q-matrix with slightly increased angular velocity weighting to accelerate damping**: Increase the fourth diagonal entry of Q from `3.2` to `3.6\u20133.8` to penalize residual oscillations more heavily. Given the current best already achieves perfect final errors, this minor adjustment could shave 1\u20132 steps off stabilization time (currently 190), directly boosting the dominant time bonus without compromising energy efficiency, as seen in the narrow performance gaps among Generations 125\u2013129.\n\n3. **Replace static leaky decay (\u00d70.95) with angle-proportional decay for stronger windup suppression**: Implement dynamic integral decay using `self.integral_x *= (0.90 + 0.1 * np.exp(-10 * abs(theta)))`, which intensifies decay as |\u03b8| grows beyond 0.1 rad. This extends the successful leaky integration insight by more aggressively neutralizing windup during extreme swing-up phases, enhancing robustness against destabilizing integral surges upon reactivation near equilibrium.\n\n4. **Add energy-aware swing-up deactivation to avoid over-assistance near upright**: Augment the existing swing-up logic with an energy threshold check: only apply `u_swing` if the pendulum\u2019s mechanical energy `E = 0.5*m*(dtheta*L_COM)**2 + m*g*L_COM*(1 - np.cos(theta))` is below `0.85 * m*g*L_COM`. This prevents unnecessary corrective impulses when the pole has sufficient energy but wrong phase, marginally reducing total energy use and potentially unlocking extra energy bonus points beyond the current plateau.\n\n5. **Introduce cross-state coupling in LQR via off-diagonal Q terms for smoother cart convergence**: Add small symmetric off-diagonal terms to Q, specifically `Q[0,2] = Q[2,0] = 0.12`, to account for physical coupling between cart position and velocity. This encourages coordinated correction that minimizes abrupt velocity changes during final approach, targeting a reduction in `stable_x_error` below 0.05\u2014a key differentiator now that success and energy bonuses are saturated among top performers.",
    "1. **Enhance integral gating with joint angle-velocity condition to prevent transient windup**: Replace the current angle-only integral gate (`np.tanh(12.0 * (0.1 - abs(theta)))`) with a multiplicative gate that also requires low angular velocity: `gate = np.tanh(12.0 * (0.1 - abs(theta))) * np.tanh(8.0 * (1.0 - abs(dtheta)))`. This builds directly on the proven soft-switched integral pattern while preventing premature integral buildup during fast upright crossings, which could further reduce `stable_x_error` below 0.05 without sacrificing stability.\n\n2. **Dynamically scale swing-up assist intensity using mechanical energy deficit**: Augment the existing falling-severity-aware swing-up logic by modulating `u_swing` based on energy shortfall: compute `E_current = 0.5*M_POLE*(dtheta*L_COM)**2 + M_POLE*G*L_COM*(1 - np.cos(theta))` and scale the assist by `max(0.0, 1.0 - E_current/(M_POLE*G*L_COM))`. This extends the physics-informed swing-up pattern to avoid over-assistance when the pole already has sufficient energy, potentially reducing total energy use and unlocking marginal gains beyond the current efficiency plateau.\n\n3. **Implement angle-proportional integral decay for stronger windup suppression during extreme swings**: Replace the static `self.integral_x *= 0.95` decay with an adaptive decay factor like `self.integral_x *= (0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1)))`, which intensifies decay as the pole deviates further from upright. This refines the successful leaky integration insight by more aggressively neutralizing windup during large-angle excursions, enhancing robustness when reactivating integral control near equilibrium.\n\n4. **Introduce cross-coupled LQR state penalties via off-diagonal Q-matrix terms**: Add symmetric off-diagonal terms to the Q matrix\u2014specifically `Q[0,2] = Q[2,0] = 0.12` and `Q[1,3] = Q[3,1] = 0.8`\u2014to explicitly account for physical coupling between cart position/velocity and pole angle/angular velocity. This leverages the exact linearization fidelity of the current best to encourage smoother, coordinated corrections that minimize abrupt velocity changes, targeting improved steady-state cart positioning.\n\n5. **Fine-tune adaptive gain scheduling with steeper tanh transitions for sharper mode separation**: Increase the tanh slopes in gain scheduling from `5.0`/`4.0` to `7.0`/`6.0` for position and velocity gains respectively (`pos_gain = 1.0 + 0.5 * np.tanh(7.0 * max(0.0, abs(theta) - 0.6))`). This sharpens the transition between aggressive swing-up correction and gentle stabilization phases, potentially shaving 1\u20132 steps off the current 190-step stabilization time to boost the dominant time bonus without increasing energy consumption.",
    "1. **Refine integral gating with joint angle-velocity dependency to suppress transient windup during fast upright crossings**: Replace the current angle-only gate `np.tanh(12.0 * (0.1 - abs(theta)))` with a multiplicative form that also requires low angular velocity: `gate = np.tanh(12.0 * (0.1 - abs(theta))) * np.tanh(8.0 * (1.0 - abs(dtheta)))`. This directly extends the proven soft-switched integral pattern in the current best program, preventing premature integral buildup when the pole swings rapidly through upright\u2014potentially reducing `stable_x_error` below 0.05 without destabilizing convergence.\n\n2. **Dynamically modulate swing-up assist intensity using mechanical energy deficit for more efficient recovery**: Augment the existing falling-severity-aware swing-up term by scaling `u_swing` based on energy shortfall: compute `E_current = 0.5*M_POLE*(dtheta*L_COM)**2 + M_POLE*G*L_COM*(1 - np.cos(theta))` and multiply the assist by `max(0.0, 1.0 - E_current/(M_POLE*G*L_COM))`. This builds on the physics-informed swing-up logic of the current best while avoiding over-assistance when the pole already has sufficient energy, potentially shaving off marginal energy use beyond the current plateau.\n\n3. **Implement angle-proportional integral decay to aggressively suppress windup during large deviations**: Replace the static `self.integral_x *= 0.95` with an adaptive decay factor like `self.integral_x *= (0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1)))`, which intensifies decay as `|theta|` increases beyond 0.1 rad. This refines the successful leaky integration insight from the current best program, ensuring integral readiness is preserved near equilibrium while eliminating accumulated error during extreme swings.\n\n4. **Introduce cross-coupled state penalties in the LQR Q-matrix to coordinate cart-pole dynamics**: Add symmetric off-diagonal terms to the Q matrix\u2014specifically `Q[0,2] = Q[2,0] = 0.12` and `Q[1,3] = Q[3,1] = 0.8`\u2014to explicitly penalize coupled deviations between position/angle and their velocities. Leveraging the exact linearization fidelity of the current best, this encourages smoother, physically coherent corrections that minimize abrupt cart movements, targeting further reduction in steady-state `x_error`.\n\n5. **Sharpen adaptive gain transitions with steeper tanh slopes for crisper mode separation between swing-up and balance**: Increase the tanh coefficients in gain scheduling from `5.0`/`4.0` to `7.0`/`6.0`: `pos_gain = 1.0 + 0.5 * np.tanh(7.0 * max(0.0, abs(theta) - 0.6))` and similarly for velocity. This fine-tunes the multiplicative adaptive gain pattern of the current best to enable faster deactivation of aggressive correction once near equilibrium, potentially reducing stabilization time from 190 to 188\u2013189 steps and boosting the dominant time bonus.",
    "1. **Enhance integral gating with joint angle-velocity dependency to suppress windup during rapid upright crossings**: Replace the current angle-only integral gate `np.tanh(12.0 * (0.1 - abs(theta)))` with a multiplicative form that also requires low angular velocity: `gate = np.tanh(12.0 * (0.1 - abs(theta))) * np.tanh(8.0 * (1.0 - abs(dtheta)))`. This builds directly on the current best\u2019s soft-switched integral pattern and addresses transient windup when the pole swings quickly through upright\u2014potentially reducing `stable_x_error` below 0.05 without destabilizing convergence.\n\n2. **Dynamically scale swing-up assist using mechanical energy deficit for more efficient impulse delivery**: Augment the existing falling-severity-aware swing-up term by computing the pendulum\u2019s current mechanical energy `E_current = 0.5*M_POLE*(dtheta*L_COM)**2 + M_POLE*G*L_COM*(1 - np.cos(theta))` and scaling `u_swing` by `max(0.0, 1.0 - E_current/(M_POLE*G*L_COM))`. This refines the physics-informed swing-up logic of the current best to avoid over-assistance when energy is sufficient, potentially improving time-to-stabilization or energy efficiency beyond the current plateau.\n\n3. **Implement angle-proportional integral decay to aggressively suppress windup during large deviations**: Replace the static leaky integration factor (`*= 0.95`) with an adaptive decay based on pole angle: `self.integral_x *= (0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1)))`. This sharpens the current best\u2019s successful leaky integration strategy by intensifying decay as the pole deviates further from upright, preserving integral readiness near equilibrium while eliminating accumulated error during extreme swings.\n\n4. **Introduce cross-coupled state penalties in the LQR Q-matrix to coordinate cart-pole dynamics**: Modify the diagonal Q-matrix to include off-diagonal terms that penalize coupled deviations\u2014specifically set `Q[0,2] = Q[2,0] = 0.12` and `Q[1,3] = Q[3,1] = 0.8`. Leveraging the current best\u2019s exact linearization fidelity, this encourages smoother, physically coherent corrections that minimize abrupt cart movements, targeting further reduction in steady-state `x_error` and potentially faster stabilization.\n\n5. **Sharpen adaptive gain transitions with steeper tanh slopes for crisper separation between swing-up and balance modes**: Increase the tanh coefficients in gain scheduling from `5.0`/`4.0` to `7.0`/`6.0`: `pos_gain = 1.0 + 0.5 * np.tanh(7.0 * max(0.0, abs(theta) - 0.6))` and similarly for velocity. This fine-tunes the current best\u2019s multiplicative adaptive gain pattern to enable faster deactivation of aggressive correction once near equilibrium, potentially reducing stabilization time from 190 to 188\u2013189 steps and boosting the dominant time bonus.",
    "1. **Refine integral gating with joint angle-velocity dependency to suppress transient windup during rapid upright crossings**: Replace the current angle-only integral gate `np.tanh(12.0 * (0.1 - abs(theta)))` with a multiplicative form that also requires low angular velocity: `gate = np.tanh(12.0 * (0.1 - abs(theta))) * np.tanh(8.0 * (1.0 - abs(dtheta)))`. This builds directly on the current best\u2019s soft-switched integral pattern and addresses potential windup when the pole swings quickly through upright\u2014likely reducing `stable_x_error` below 0.05 without destabilizing convergence.\n\n2. **Dynamically scale swing-up assist using mechanical energy deficit for more efficient impulse delivery**: Augment the existing falling-severity-aware swing-up term by computing the pendulum\u2019s current mechanical energy `E_current = 0.5*M_POLE*(dtheta*L_COM)**2 + M_POLE*G*L_COM*(1 - np.cos(theta))` and scaling `u_swing` by `max(0.0, 1.0 - E_current/(M_POLE*G*L_COM))`. This refines the physics-informed swing-up logic of the current best to avoid over-assistance when energy is sufficient, potentially improving time-to-stabilization or maintaining the energy efficiency plateau while accelerating recovery.\n\n3. **Implement angle-proportional integral decay to aggressively suppress windup during large deviations**: Replace the static leaky integration factor (`*= 0.95`) with an adaptive decay based on pole angle: `self.integral_x *= (0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1)))`. This sharpens the current best\u2019s successful leaky integration strategy by intensifying decay as the pole deviates further from upright, preserving integral readiness near equilibrium while eliminating accumulated error during extreme swings\u2014critical under high-friction conditions.\n\n4. **Introduce cross-coupled state penalties in the LQR Q-matrix to coordinate cart-pole dynamics**: Modify the diagonal Q-matrix to include off-diagonal terms that penalize coupled deviations\u2014specifically set `Q[0,2] = Q[2,0] = 0.12` and `Q[1,3] = Q[3,1] = 0.8`. Leveraging the current best\u2019s exact linearization fidelity, this encourages smoother, physically coherent corrections that minimize abrupt cart movements, targeting further reduction in steady-state `x_error` and potentially faster stabilization through better dynamic coordination.\n\n5. **Sharpen adaptive gain transitions with steeper tanh slopes for crisper separation between swing-up and balance modes**: Increase the tanh coefficients in gain scheduling from `5.0`/`4.0` to `7.0`/`6.0`: `pos_gain = 1.0 + 0.5 * np.tanh(7.0 * max(0.0, abs(theta) - 0.6))` and similarly for velocity. This fine-tunes the current best\u2019s multiplicative adaptive gain pattern to enable faster deactivation of aggressive correction once near equilibrium, potentially reducing stabilization time from 190 to 188\u2013189 steps and boosting the dominant time bonus.",
    "1. **Enhance integral gating with adaptive decay based on angular deviation**: Replace the static 5% leaky integration (`*= 0.95`) with an angle-dependent decay factor such as `self.integral_x *= (0.85 + 0.15 * np.exp(-8.0 * max(0.0, abs(theta) - 0.1)))`. This sharpens windup suppression during large swings while preserving integral readiness near equilibrium\u2014directly extending Generation 152\u2019s dual-gating success and addressing latent windup risks under high joint/cart friction.\n\n2. **Introduce cross-coupled penalties in the LQR Q-matrix to coordinate cart-pole motion**: Modify the diagonal Q-matrix to include off-diagonal terms: set `Q[0,2] = Q[2,0] = 0.12` and `Q[1,3] = Q[3,1] = 0.8`. Leveraging the current best\u2019s exact linearization fidelity, this encourages smoother, dynamically coherent corrections that minimize abrupt cart movements\u2014potentially reducing `stable_x_error` below 0.03 and improving time-to-stabilization.\n\n3. **Refine swing-up assist using mechanical energy deficit for impulse efficiency**: Compute the pendulum\u2019s current mechanical energy `E_current = 0.5*M_POLE*(dtheta*L_COM)**2 + M_POLE*G*L_COM*(1 - np.cos(theta))` and scale the swing-up term by `max(0.0, 1.0 - E_current/(M_POLE*G*L_COM))`. This builds on Generation 152\u2019s falling-severity normalization to avoid over-assistance when energy is sufficient, potentially shaving steps off stabilization time without sacrificing energy efficiency.\n\n4. **Tighten velocity gating thresholds for integral activation to further suppress transient windup**: Reduce the velocity gate\u2019s tolerance from `tanh(8.0 * (1.0 - abs(dtheta)))` to `tanh(10.0 * (0.8 - abs(dtheta)))`, requiring slower pole motion before enabling integral accumulation. This fine-tunes the already-successful dual-gate mechanism to better handle rapid upright crossings in heavy/long-pole regimes, targeting sub-0.03 `stable_x_error`.\n\n5. **Implement state-dependent adaptive gain saturation to prevent overcorrection near equilibrium**: Cap the multiplicative adaptive gain (`pos_gain * vel_gain`) at a maximum value (e.g., 1.8) when `abs(theta) < 0.3` and `abs(dtheta) < 0.5`. This prevents excessive LQR amplification during late-stage balancing\u2014complementing the steeper tanh slopes already used\u2014while preserving aggressive response during swing-up, potentially improving steady-state precision without destabilizing convergence.",
    "1. **Refine swing-up activation with dynamic angle thresholds based on pole inertia**: Replace the fixed 0.8 rad swing-up trigger with a normalized threshold using `theta_threshold = 0.8 * np.sqrt(L_COM / 2.5)` to adapt to varying pole lengths. This extends the current best\u2019s natural frequency normalization principle to the activation boundary itself, potentially improving swing-up timing for different pole configurations while preserving energy efficiency.\n\n2. **Introduce derivative-based damping augmentation during mid-swing transitions**: Add a term `-K_d * dtheta * (1 - np.tanh(4.0 * (abs(theta) - 0.5)))` to the control force that provides extra angular damping when the pole is between 0.3\u20130.7 rad\u2014where momentum peaks but LQR alone may under-damp. This builds on the current best\u2019s cart-damping near equilibrium but targets the high-energy transition phase, potentially reducing overshoot and shaving steps off stabilization time.\n\n3. **Implement adaptive integral gain scaling tied to energy deficit**: Modulate `K_i` as `K_i_base * (1.0 + 0.3 * energy_deficit)` so integral action intensifies when the system is far from upright energy levels. This synergizes the current best\u2019s energy-aware swing-up with its integral control, accelerating cart centering during aggressive recovery without increasing windup risk near equilibrium due to existing tight gating.\n\n4. **Enhance cross-coupling in Q-matrix with state-dependent weights**: Make the off-diagonal terms `Q[0,2]` and `Q[1,3]` adaptive: scale them by `1.0 + 0.5 * np.tanh(3.0 * abs(theta))` to strengthen coordination penalties during large deviations. This leverages the proven benefit of cross-coupling while making it responsive to instability severity, potentially improving transient smoothness without over-constraining near balance.\n\n5. **Add predictive momentum compensation using short-horizon state extrapolation**: Estimate next-step angular momentum as `dtheta_pred = dtheta + theta_acc_est * DT`, where `theta_acc_est` uses current state and friction models, then inject a corrective term `-0.15 * (theta * dtheta_pred) / (self.omega_n * L_COM)` into the force. This extends the current best\u2019s falling-severity logic into a feedforward-like correction, anticipating momentum buildup before it destabilizes the system.",
    "1. **Implement state-dependent cross-coupling in the Q-matrix by scaling off-diagonal terms with angular deviation**: Extend the current best\u2019s static cross-coupled LQR (`Q[0,2]=Q[2,0]=0.12`, `Q[1,3]=Q[3,1]=0.8`) by making these terms adaptive: e.g., `Q[1,3] = 0.8 * (1.0 + 0.5 * np.tanh(3.0 * abs(theta)))`. This leverages the proven benefit of coordination penalties while intensifying them during large deviations\u2014where transient coupling matters most\u2014potentially improving swing-up smoothness and reducing stabilization time beyond the current 187 steps.\n\n2. **Add derivative-based angular damping during mid-swing (0.3\u20130.7 rad) to suppress momentum overshoot**: Introduce a term `-K_d * dtheta * (1 - np.tanh(4.0 * (abs(theta) - 0.5)))` into the control force, active primarily when the pole is falling through the high-energy transition zone. This complements the existing near-equilibrium cart damping and builds on the current best\u2019s physics-aware design, targeting the phase where angular momentum peaks but LQR feedback may lag\u2014potentially shaving 1\u20133 steps off stabilization time.\n\n3. **Scale integral gain dynamically using energy deficit to accelerate cart centering during aggressive recovery**: Modify `K_i` as `K_i = 0.85 * (1.0 + 0.3 * max(0, 1 - E_current / E_ref))`, so integral action strengthens when mechanical energy is low (i.e., during swing-up). This synergizes the current best\u2019s energy-aware swing-up logic with its tightly gated integral control, enhancing x-error correction without increasing windup risk\u2014thanks to the existing dual gating (`angle_gate`, `velocity_gate`) that deactivates integral near instability.\n\n4. **Replace fixed 0.8 rad swing-up threshold with inertia-normalized activation based on natural frequency**: Set swing-up trigger as `theta_threshold = 0.8 * np.sqrt(L_COM / 2.5)` so activation adapts to actual pole length. Since the current best already uses `omega_n = sqrt(G / L_COM)` for momentum normalization, extending this principle to the swing-up boundary ensures consistent timing across varying pole dynamics, preserving energy efficiency while improving responsiveness for non-standard configurations.\n\n5. **Inject predictive momentum compensation via short-horizon angular acceleration estimate**: Compute `theta_acc_est` using the current state and friction model, then extrapolate `dtheta_pred = dtheta + theta_acc_est * DT`. Add a corrective term `-0.15 * (theta * dtheta_pred) / (self.omega_n * L_COM)` to anticipate destabilizing momentum buildup. This extends the current best\u2019s reactive falling-severity logic into a feedforward-like mechanism, potentially enabling earlier counteraction and further reducing time_bonus gaps.",
    "1. **Enhance integral action with energy-aware gain scaling**: Building on the current best\u2019s velocity-gated integral control, dynamically scale the integral gain `K_i` using the system\u2019s mechanical energy deficit: `K_i = 0.8 * (1.0 + 0.25 * max(0, 1 - (0.5*M_POLE*L_COM**2*dtheta**2 + M_POLE*G*L_COM*(1 - np.cos(theta))) / (M_POLE*G*L_COM)))`. This leverages the proven energy-normalization insight to intensify cart centering during low-energy swing-up phases while preserving windup protection via existing dual gating\u2014potentially reducing stable_x_error below 0.04 without destabilizing.\n\n2. **Introduce derivative-based damping in the mid-swing zone (0.3\u20130.8 rad)**: Add a term `-0.18 * dtheta * (1.0 - np.tanh(5.0 * (abs(theta) - 0.55)))` to the control force, active primarily when the pole is falling through the high-momentum transition region. This extends the current best\u2019s physics-aware design by proactively counteracting angular velocity peaks before they require aggressive LQR correction, which could shave 1\u20132 steps off the 187-step stabilization time\u2014the key differentiator among elite controllers.\n\n3. **Make LQR cross-coupling terms adaptive based on angular deviation**: Replace the static Q-matrix with state-dependent off-diagonal terms: set `Q[1,3] = Q[3,1] = 0.8 * (1.0 + 0.6 * np.tanh(2.5 * abs(theta)))` while keeping diagonal weights fixed. This builds on the current best\u2019s accurate linearized dynamics and gentle gain scheduling by intensifying angle-velocity coordination penalties precisely when large deviations demand tighter coupling\u2014potentially improving swing-up smoothness and further minimizing time_bonus gaps.\n\n4. **Replace the fixed 0.8 rad swing-up threshold with an inertia-scaled boundary**: Compute the swing-up activation threshold as `theta_thresh = 0.8 * np.sqrt(L_COM / 2.5)` so it adapts to the actual pole length. Since the current best already uses `omega_n = sqrt(G / L_COM)` for falling severity normalization, extending this principle to the swing-up trigger ensures consistent phase timing across varying pole dynamics, preserving the elite energy efficiency (avg_energy_per_step: 0.01) while improving responsiveness for extreme configurations.\n\n5. **Implement predictive momentum compensation using short-horizon angular acceleration**: Estimate angular acceleration via the friction-aware dynamics model (`theta_acc_est = (G*np.sin(theta) - ...)/denom`) and compute a predicted angular velocity `dtheta_pred = dtheta + theta_acc_est * DT`. Inject a feedforward-like correction `-0.12 * (theta * dtheta_pred) / (self.omega_n * L_COM)` into the control force. This upgrades the current best\u2019s reactive falling-factor logic into a proactive mechanism, enabling earlier counteraction of destabilizing momentum buildup and potentially achieving sub-187 stabilization times.",
    "1. **Enhance predictive momentum compensation with joint-friction-aware angular acceleration estimation**: Replace the current simplified `theta_acc_est` with a full friction-inclusive model that accounts for both cart and joint friction terms in the denominator, mirroring the precision of the A-matrix used in LQR design. This would align predictive correction with the high-fidelity dynamics already proven essential for zero final error under `FRICTION_JOINT=0.25`, potentially enabling even earlier divergence counteraction and shaving 2\u20133 steps off the 178-step stabilization time.\n\n2. **Introduce state-dependent mid-swing damping gain modulated by predicted energy deficit**: Scale the `K_d_midswing = 2.5` coefficient using the normalized mechanical energy: `K_d = 2.5 * (1.0 + 0.4 * max(0, 1 - E_current / E_max))`, where `E_max = M_POLE*G*L_COM`. This leverages the Gaussian damping\u2019s phase specificity while making its intensity responsive to swing-up progress\u2014applying stronger damping when the pole is under-energized and prone to stall, which could reduce overshoot variability across extreme initial conditions without compromising balancing precision.\n\n3. **Make cross-coupling boost adaptive to both angle and angular velocity magnitude**: Extend the current `cross_coupling_boost = 1.0 + 0.5 * tanh(3.0 * abs(theta))` to include velocity dependence: `boost = 1.0 + 0.5 * tanh(3.0 * abs(theta)) * (1.0 + 0.3 * tanh(2.0 * abs(dtheta)))`. Since high `dtheta` during mid-swing exacerbates coordination challenges, this dual-state adaptation would intensify theta-dtheta coupling precisely when momentum is highest, improving transient response during the critical 0.3\u20130.8 rad transition zone identified as pivotal for record time_bonus.\n\n4. **Implement exponential decay on the integral state only when outside both gating thresholds**, rather than applying decay whenever the gate product is low. Specifically, decay `self.integral_x *= 0.95` only if `abs(theta) > 0.12 OR abs(dtheta) > 1.1`, preserving accumulated correction during brief excursions beyond equilibrium. This refines the current dual-gating strategy to better handle high-inertia recovery transients, potentially reducing `stable_x_error` below 0.05 while maintaining windup protection\u2014addressing the last remaining precision gap among elite controllers.\n\n5. **Replace the fixed Gaussian center (0.5 rad) in mid-swing damping with an inertia-scaled peak location**: Set the activation center as `theta_peak = 0.5 * np.sqrt(L_COM / 2.5)` so the damping maximum adapts to the actual pole length, consistent with the `omega_n` normalization already used in predictive logic. Given that longer poles exhibit delayed momentum peaks, this ensures damping aligns with the true high-risk region of the swing-up trajectory, preventing premature or late intervention that could otherwise waste energy or permit overshoot under extreme `L_POLE=2.5` conditions."
  ],
  "total_programs_meta_processed": 175
}