<adaptive_integral_lqr>
I'll implement an adaptive integral LQR controller that combines the proven LQR approach with integral action to eliminate steady-state errors, especially in cart position. This approach will help achieve better precision in the final state while maintaining fast stabilization.
</adaptive_integral_lqr>

<DESCRIPTION>
I'm implementing an adaptive integral LQR controller that adds integral action to eliminate steady-state errors. The key innovations are:

1. Adding integral state tracking for cart position to eliminate steady-state error
2. Using a soft-switching mechanism for integral action that only activates when the pole is nearly upright
3. Implementing leaky integration to prevent windup when the integral action is inactive
4. Maintaining all the successful elements from the previous best controller

This should improve the stable_x_error metric which is currently at 0.03, helping achieve better precision for the success bonus.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
# EVOLVE-BLOCK-START
class Controller:
    """
    Adaptive LQR Controller with Velocity-Aware Gain Scheduling
    Combines physically accurate friction modeling with intelligent gain adaptation
    for ultra-fast stabilization while maintaining energy efficiency.
    """

    def __init__(self):
        # System parameters
        m = M_POLE
        M = M_CART
        l = L_COM
        g = G
        Mtot = M + m
        denom0 = l * (4.0 / 3.0 - m / Mtot)
        b_c = FRICTION_CART
        b_j = FRICTION_JOINT

        # Physically accurate linearized A matrix with corrected joint friction
        A = np.zeros((4, 4))
        A[0, 2] = 1.0
        A[1, 3] = 1.0

        # theta_acc row (3) - using corrected joint friction term matching simulator
        A[3, 1] = g / denom0
        A[3, 2] = b_c / (Mtot * denom0)
        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0

        # x_acc row (2)
        A[2, 1] = -(m * l / Mtot) * A[3, 1]
        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]
        A[2, 3] = b_j / (Mtot * denom0)

        # B matrix
        B = np.zeros((4, 1))
        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)
        B[3, 0] = -1.0 / (Mtot * denom0)

        # Proven optimal LQR weights from best performer (4920.14 score)
        Q = np.diag([4.5, 44.0, 0.6, 3.2])
        R = np.array([[1.0]])

        # Solve LQR gains
        self.K = self.solve_lqr(A, B, Q, R)

    def solve_lqr(self, A, B, Q, R):
        """Solve continuous-time LQR"""
        from scipy.linalg import solve_continuous_are
        P = solve_continuous_are(A, B, Q, R)
        K = np.linalg.inv(R) @ B.T @ P
        return K

    def get_action(self, state):
        """Adaptive LQR with additive swing-up assist for large angles"""
        x, theta, dx, dtheta = state

        # Normalize angle to [-pi, pi]
        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi

        state_vec = np.array([x, theta, dx, dtheta])
        base_force = -self.K @ state_vec

        # Proven optimal gain scheduling from best performer
        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))
        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))

        # Combined multiplicative gain
        adaptive_gain = pos_gain * vel_gain

        force = base_force * adaptive_gain

        # Additive swing-up assist: only activates for large angles (>0.8 rad)
        # Provides extra corrective impulse when pole is falling away from equilibrium
        if abs(theta) > 0.8:
            # Physics-informed assist: push cart in direction to catch falling pole
            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))
            # If theta*dtheta > 0, pole is falling further - need stronger correction
            # Normalized by natural frequency of the pendulum for better scaling
            natural_freq = np.sqrt(G / L_COM)
            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * natural_freq))
            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor
            force = force + u_swing

        return float(force[0])

# Initialize controller
controller = Controller()

def get_control_action(state):
    force = controller.get_action(state)
    # Apply force limits
    return float(np.clip(force, -100.0, 100.0))
# EVOLVE-BLOCK-END
=======
# EVOLVE-BLOCK-START
class Controller:
    """
    Adaptive Integral LQR Controller with Velocity-Aware Gain Scheduling
    Combines physically accurate friction modeling with intelligent gain adaptation
    and integral action for ultra-fast stabilization while maintaining energy efficiency.
    """

    def __init__(self):
        # System parameters
        m = M_POLE
        M = M_CART
        l = L_COM
        g = G
        Mtot = M + m
        denom0 = l * (4.0 / 3.0 - m / Mtot)
        b_c = FRICTION_CART
        b_j = FRICTION_JOINT

        # Physically accurate linearized A matrix with corrected joint friction
        A = np.zeros((4, 4))
        A[0, 2] = 1.0
        A[1, 3] = 1.0

        # theta_acc row (3) - using corrected joint friction term matching simulator
        A[3, 1] = g / denom0
        A[3, 2] = b_c / (Mtot * denom0)
        A[3, 3] = -b_j / (m * l * denom0)  # Matches simulator: f_joint/(m*l) / denom0

        # x_acc row (2)
        A[2, 1] = -(m * l / Mtot) * A[3, 1]
        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]
        A[2, 3] = b_j / (Mtot * denom0)

        # B matrix
        B = np.zeros((4, 1))
        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)
        B[3, 0] = -1.0 / (Mtot * denom0)

        # Proven optimal LQR weights from best performer (4920.14 score)
        Q = np.diag([4.5, 44.0, 0.6, 3.2])
        R = np.array([[1.0]])

        # Solve LQR gains
        self.K = self.solve_lqr(A, B, Q, R)

        # Integral control parameters
        self.integral_x = 0.0
        self.K_i = 0.8  # Integral gain for cart position

        # Natural frequency for normalized falling severity
        self.omega_n = np.sqrt(G / L_COM)

    def solve_lqr(self, A, B, Q, R):
        """Solve continuous-time LQR"""
        from scipy.linalg import solve_continuous_are
        P = solve_continuous_are(A, B, Q, R)
        K = np.linalg.inv(R) @ B.T @ P
        return K

    def get_action(self, state):
        """Adaptive LQR with integral action and additive swing-up assist for large angles"""
        x, theta, dx, dtheta = state

        # Normalize angle to [-pi, pi]
        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi

        state_vec = np.array([x, theta, dx, dtheta])
        base_force = -self.K @ state_vec

        # Proven optimal gain scheduling from best performer
        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))
        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))

        # Combined multiplicative gain
        adaptive_gain = pos_gain * vel_gain

        force = base_force * adaptive_gain

        # Additive swing-up assist: only activates for large angles (>0.8 rad)
        # Provides extra corrective impulse when pole is falling away from equilibrium
        if abs(theta) > 0.8:
            # Physics-informed assist: push cart in direction to catch falling pole
            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))
            # If theta*dtheta > 0, pole is falling further - need stronger correction
            # Normalized by natural frequency of the pendulum for better scaling
            natural_freq = np.sqrt(G / L_COM)
            falling_factor = 1.0 + np.tanh(2.0 * theta * dtheta / (L_COM * natural_freq))
            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor
            force = force + u_swing

        # Soft-switched integral action for cart position
        # Only activates when pole is nearly upright (<0.1 rad)
        integral_gate = np.tanh(12.0 * (0.1 - abs(theta)))
        if integral_gate > 0.1:  # Only update integral when gate is significant
            self.integral_x += x * DT
        else:
            self.integral_x *= 0.95  # Leaky integration when inactive

        integral_force = self.K_i * integral_gate * self.integral_x
        force = force + integral_force

        return float(force[0])

# Initialize controller
controller = Controller()

def get_control_action(state):
    force = controller.get_action(state)
    # Apply force limits
    return float(np.clip(force, -100.0, 100.0))
# EVOLVE-BLOCK-END
>>>>>>> REPLACE

</DIFF>