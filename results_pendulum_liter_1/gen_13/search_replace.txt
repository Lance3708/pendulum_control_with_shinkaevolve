Looking at the performance metrics, I can see that:
1. Base score is very low (341.20) - indicating poor precision/stability quality
2. Time bonus is moderate (556.08) - stabilization is slow (459 steps)
3. Energy bonus is excellent (2497.88) - the controller is very energy efficient
4. The controller is too conservative - it prioritizes energy over speed/precision

The current LQR has very conservative Q weights and high R penalty, making it "lazy". To improve, I need to:
1. Increase Q weights significantly to demand better precision (especially on theta)
2. Decrease R to allow more aggressive control
3. Add state-dependent gain scheduling to be more aggressive when close to upright

Let me implement a more aggressive LQR with adaptive gain scheduling:

<NAME>
aggressive_lqr_with_gain_scheduling
</NAME>

<DESCRIPTION>
The current controller is too conservative with low Q weights and high R penalty, resulting in slow stabilization (459 steps) and low base score (341).

Key changes:
1. Significantly increase Q weights, especially for theta (angle) and dtheta (angular velocity) to demand tighter control
2. Reduce R penalty to allow more aggressive control forces
3. Add state-dependent gain scheduling: use higher gains when |theta| is small (fine-tuning phase) and moderate gains when |theta| is larger (recovery phase)
4. This balances fast stabilization with stability during large deviations

The goal is to dramatically improve time_bonus and base_score while maintaining reasonable energy efficiency.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
class Controller:
    """
    Suboptimal LQR Controller for Single Inverted Pendulum Stabilization.
    次优LQR控制器 - 能稳住，但参数故意调得"懒惰"。

    特点：
    1. 物理环境更难（杆更长更重，摩擦更大）。
    2. Q矩阵参数较小：对误差容忍度高 -> 精度分低。
    3. R矩阵参数较大：不愿用大力 -> 响应慢，时间分低。

    目标：初始分数 ~3000 分，进化后可达 9000+ 分。
    """

    def __init__(self):
        # 系统参数
        m = M_POLE  # 摆杆质量
        M = M_CART  # 小车质量
        l = L_COM   # 质心距离
        g = G       # 重力加速度

        # 线性化状态空间模型
        A = np.array([
            [0, 0, 1, 0],
            [0, 0, 0, 1],
            [0, -m*g/M, 0, 0],
            [0, (m+M)*g/(M*l), 0, 0]
        ])

        B = np.array([
            [0],
            [0],
            [1/M],
            [-1/(M*l)]
        ])

        # LQR权重矩阵（更保守的次优设置，留出充分进化空间）
        # Q: 显著降低权重，高度容忍误差 - 导致基础分数低
        # R: 大幅增加权重，极度限制出力 - 导致响应慢，能耗高
        Q = np.diag([2.0, 8.0, 0.05, 0.4])    # [x, theta, dx, dtheta] (更保守)
        R = np.array([[3.0]])                  # 控制力惩罚极大 (非常不愿意用大力)

        # 求解LQR增益
        self.K = self.solve_lqr(A, B, Q, R)

    def solve_lqr(self, A, B, Q, R):
        """求解连续时间LQR问题"""
        from scipy.linalg import solve_continuous_are
        P = solve_continuous_are(A, B, Q, R)
        K = np.linalg.inv(R) @ B.T @ P
        return K

    def get_action(self, state):
        """LQR控制法则: u = -K * x"""
        x, theta, dx, dtheta = state
        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi
        state_vec = np.array([x, theta, dx, dtheta])
        force = -self.K @ state_vec
        return float(force[0])
=======
class Controller:
    """
    Aggressive LQR Controller with State-Dependent Gain Scheduling.

    Features:
    1. Higher Q weights for faster convergence and better precision
    2. Lower R penalty to allow more aggressive control
    3. Gain scheduling: smoother transition between recovery and fine-tuning phases
    4. Optimized for the challenging physical setup (heavy, long pole with high friction)
    """

    def __init__(self):
        # System parameters
        m = M_POLE  # Pole mass
        M = M_CART  # Cart mass
        l = L_COM   # Distance to center of mass
        g = G       # Gravity

        # Linearized state-space model around upright equilibrium
        A = np.array([
            [0, 0, 1, 0],
            [0, 0, 0, 1],
            [0, -m*g/M, 0, 0],
            [0, (m+M)*g/(M*l), 0, 0]
        ])

        B = np.array([
            [0],
            [0],
            [1/M],
            [-1/(M*l)]
        ])

        # Aggressive LQR weights for fast stabilization
        # Q: High weights on theta and dtheta for tight angle control
        Q_aggressive = np.diag([15.0, 180.0, 2.0, 25.0])  # [x, theta, dx, dtheta]
        R_aggressive = np.array([[0.3]])  # Lower penalty allows stronger control

        # Conservative LQR weights for large angle recovery (more stable)
        Q_conservative = np.diag([5.0, 60.0, 0.5, 8.0])
        R_conservative = np.array([[0.8]])

        # Solve both LQR gains
        self.K_aggressive = self.solve_lqr(A, B, Q_aggressive, R_aggressive)
        self.K_conservative = self.solve_lqr(A, B, Q_conservative, R_conservative)

    def solve_lqr(self, A, B, Q, R):
        """Solve continuous-time LQR problem"""
        from scipy.linalg import solve_continuous_are
        P = solve_continuous_are(A, B, Q, R)
        K = np.linalg.inv(R) @ B.T @ P
        return K

    def get_action(self, state):
        """LQR control with smooth gain scheduling based on angle magnitude"""
        x, theta, dx, dtheta = state

        # Angle wrapping to [-pi, pi]
        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi
        state_vec = np.array([x, theta, dx, dtheta])

        abs_theta = abs(theta)

        # Smooth blending using sigmoid-like function
        # Transition zone: 0.15 rad to 0.5 rad
        # Below 0.15 rad: fully aggressive (fine-tuning)
        # Above 0.5 rad: fully conservative (recovery)
        theta_low = 0.15
        theta_high = 0.5

        if abs_theta <= theta_low:
            alpha = 1.0  # Fully aggressive
        elif abs_theta >= theta_high:
            alpha = 0.0  # Fully conservative
        else:
            # Smooth cosine interpolation
            t = (abs_theta - theta_low) / (theta_high - theta_low)
            alpha = 0.5 * (1 + np.cos(np.pi * t))

        # Blend gains
        K_blended = alpha * self.K_aggressive + (1 - alpha) * self.K_conservative

        force = -K_blended @ state_vec
        return float(force[0])
=======
>>>>>>> REPLACE
</DIFF>