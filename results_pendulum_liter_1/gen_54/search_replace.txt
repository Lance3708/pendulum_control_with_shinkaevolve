<NAME>
adaptive_lqr_with_integral_action
</NAME>

<DESCRIPTION>
This improvement introduces integral action for the cart position to eliminate steady-state error. The key insight is that while the LQR controller excellently handles stabilization, it lacks integral action to drive the cart position to exactly zero. By augmenting the state space with an integral term for cart position and properly tuning its weight in the Q matrix, we can achieve better long-term positioning accuracy without sacrificing stability. This addresses the persistent small x-error observed in stable states.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
# EVOLVE-BLOCK-START
class Controller:
    """
    High-Precision Adaptive LQR with Optimized LQR Weights for Faster Stabilization
    """

    def __init__(self):
        # System parameters
        m = M_POLE
        M = M_CART
        l = L_COM
        g = G
        Mtot = M + m
        denom0 = l * (4.0 / 3.0 - m / Mtot)
        b_c = FRICTION_CART
        b_j = FRICTION_JOINT

        # Runtime validation of physics consistency
        assert abs(FRICTION_CART - 0.35) < 1e-6, "Cart friction mismatch"
        assert abs(FRICTION_JOINT - 0.25) < 1e-6, "Joint friction mismatch"
        assert abs(L_COM - 1.25) < 1e-6, "Pole length mismatch"

        # Physically accurate linearized A matrix with consistent denom0 usage
        A = np.zeros((4, 4))
        A[0, 2] = 1.0
        A[1, 3] = 1.0

        # theta_acc row (3) - fully consistent with simulator denominator structure
        A[3, 1] = g / denom0
        A[3, 2] = b_c / (Mtot * denom0)
        A[3, 3] = -b_j / (m * l * denom0)  # Now perfectly matches simulator physics

        # x_acc row (2)
        A[2, 1] = -(m * l / Mtot) * A[3, 1]
        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]
        A[2, 3] = b_j / (Mtot * denom0)

        # B matrix
        B = np.zeros((4, 1))
        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)
        B[3, 0] = -1.0 / (Mtot * denom0)

        # Optimized LQR weights: increased penalties for faster stabilization
        Q = np.diag([4.5, 44.0, 0.6, 3.2])  # Increased from [4.0, 38.0, 0.5, 3.0]
        R = np.array([[1.0]])

        # Solve LQR gains
        self.K = self.solve_lqr(A, B, Q, R)

    def solve_lqr(self, A, B, Q, R):
        """Solve continuous-time LQR"""
        from scipy.linalg import solve_continuous_are
        P = solve_continuous_are(A, B, Q, R)
        K = np.linalg.inv(R) @ B.T @ P
        return K

    def get_action(self, state):
        """Adaptive LQR control with smooth, differentiable gain scheduling"""
        x, theta, dx, dtheta = state

        # Normalize angle to [-pi, pi]
        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi

        state_vec = np.array([x, theta, dx, dtheta])
        base_force = -self.K @ state_vec

        # Smooth, differentiable gain scheduling using tanh activation
        # Eliminates discontinuities while preserving strong response at large errors
        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))
        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))

        # Combined multiplicative gain
        adaptive_gain = pos_gain * vel_gain

        force = base_force * adaptive_gain

        return float(force[0])

# Initialize controller
controller = Controller()

def get_control_action(state):
    force = controller.get_action(state)
    # Apply force limits
    return float(np.clip(force, -100.0, 100.0))
# EVOLVE-BLOCK-END
=======
# EVOLVE-BLOCK-START
class Controller:
    """
    High-Precision Adaptive LQR with Integral Action for Cart Position Control
    """

    def __init__(self):
        # System parameters
        m = M_POLE
        M = M_CART
        l = L_COM
        g = G
        Mtot = M + m
        denom0 = l * (4.0 / 3.0 - m / Mtot)
        b_c = FRICTION_CART
        b_j = FRICTION_JOINT

        # Runtime validation of physics consistency
        assert abs(FRICTION_CART - 0.35) < 1e-6, "Cart friction mismatch"
        assert abs(FRICTION_JOINT - 0.25) < 1e-6, "Joint friction mismatch"
        assert abs(L_COM - 1.25) < 1e-6, "Pole length mismatch"

        # Extended state space: [x, theta, dx, dtheta, integral_x]
        # Physically accurate linearized A matrix with consistent denom0 usage
        A = np.zeros((5, 5))
        A[0, 2] = 1.0
        A[1, 3] = 1.0
        A[4, 0] = 1.0  # integral_x_dot = x

        # theta_acc row (3) - fully consistent with simulator denominator structure
        A[3, 1] = g / denom0
        A[3, 2] = b_c / (Mtot * denom0)
        A[3, 3] = -b_j / (m * l * denom0)  # Now perfectly matches simulator physics

        # x_acc row (2)
        A[2, 1] = -(m * l / Mtot) * A[3, 1]
        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]
        A[2, 3] = b_j / (Mtot * denom0)

        # B matrix (5x1)
        B = np.zeros((5, 1))
        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)
        B[3, 0] = -1.0 / (Mtot * denom0)

        # Optimized LQR weights: increased penalties for faster stabilization
        # Added weight for integral action (Q[4]) to eliminate steady-state error
        Q = np.diag([4.5, 44.0, 0.6, 3.2, 0.1])  # Added integral term weight
        R = np.array([[1.0]])

        # Solve LQR gains
        self.K = self.solve_lqr(A, B, Q, R)
        self.integral_x = 0.0  # Internal state for integral term

    def solve_lqr(self, A, B, Q, R):
        """Solve continuous-time LQR"""
        from scipy.linalg import solve_continuous_are
        P = solve_continuous_are(A, B, Q, R)
        K = np.linalg.inv(R) @ B.T @ P
        return K

    def get_action(self, state):
        """Adaptive LQR control with integral action and smooth gain scheduling"""
        x, theta, dx, dtheta = state

        # Update integral of x
        self.integral_x += x * DT

        # Normalize angle to [-pi, pi]
        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi

        # Extended state vector with integral term
        state_vec = np.array([x, theta, dx, dtheta, self.integral_x])
        base_force = -self.K @ state_vec

        # Smooth, differentiable gain scheduling using tanh activation
        # Eliminates discontinuities while preserving strong response at large errors
        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))
        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))

        # Combined multiplicative gain
        adaptive_gain = pos_gain * vel_gain

        force = base_force * adaptive_gain

        return float(force[0])

# Initialize controller
controller = Controller()

def get_control_action(state):
    force = controller.get_action(state)
    # Apply force limits
    return float(np.clip(force, -100.0, 100.0))
# EVOLVE-BLOCK-END
>>>>>>> REPLACE

</DIFF>