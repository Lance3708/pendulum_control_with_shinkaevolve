<NAME>
enhanced_swing_up_with_integral_control
</NAME>

<DESCRIPTION>
This enhancement introduces two key improvements to boost the combined score:

1. **Integral Control**: Adds integral action on cart position to eliminate steady-state error, which directly improves the success bonus by achieving better final position accuracy. The integral term is softly activated only when the pole is nearly upright to avoid interfering with swing-up dynamics.

2. **Refined Swing-Up Logic**: Improves the swing-up assist by using a more sophisticated falling severity metric that considers both angle and angular velocity in a normalized way. This allows for more precise assistance during the critical swing-up phase, potentially reducing stabilization time.

These changes target the main bottlenecks identified in the performance metrics: improving final accuracy (success bonus) and potentially reducing stabilization time through better swing-up dynamics.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
# EVOLVE-BLOCK-START
class Controller:
    """
    Adaptive LQR Controller with Swing-Up Assist
    Combines optimal LQR gains with physics-informed swing-up assistance
    for robust stabilization of the challenging pendulum setup.
    """

    def __init__(self):
        # System parameters
        m = M_POLE
        M = M_CART
        l = L_COM
        g = G
        Mtot = M + m
        denom0 = l * (4.0 / 3.0 - m / Mtot)
        b_c = FRICTION_CART
        b_j = FRICTION_JOINT

        # Physically accurate linearized A matrix
        A = np.zeros((4, 4))
        A[0, 2] = 1.0
        A[1, 3] = 1.0

        # theta_acc row (3) - matches simulator physics
        A[3, 1] = g / denom0
        A[3, 2] = b_c / (Mtot * denom0)
        A[3, 3] = -b_j / (m * l * denom0)

        # x_acc row (2)
        A[2, 1] = -(m * l / Mtot) * A[3, 1]
        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]
        A[2, 3] = b_j / (Mtot * denom0)

        # B matrix
        B = np.zeros((4, 1))
        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)
        B[3, 0] = -1.0 / (Mtot * denom0)

        # Proven optimal LQR weights
        Q = np.diag([4.5, 44.0, 0.6, 3.2])
        R = np.array([[1.0]])

        # Solve LQR gains
        self.K = self.solve_lqr(A, B, Q, R)

        # Precompute natural frequency for normalized falling severity
        self.omega_n = np.sqrt(G / L_COM)

    def solve_lqr(self, A, B, Q, R):
        """Solve continuous-time LQR"""
        from scipy.linalg import solve_continuous_are
        P = solve_continuous_are(A, B, Q, R)
        K = np.linalg.inv(R) @ B.T @ P
        return K

    def get_action(self, state):
        """Adaptive LQR control with swing-up assist for large angles"""
        x, theta, dx, dtheta = state

        # Robust angle normalization using arctan2
        theta = np.arctan2(np.sin(theta), np.cos(theta))

        state_vec = np.array([x, theta, dx, dtheta])
        base_force = -self.K @ state_vec

        # Smooth gain scheduling using tanh activation
        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))
        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))

        # Combined multiplicative gain
        adaptive_gain = pos_gain * vel_gain
        force = base_force * adaptive_gain

        # Additive swing-up assist for large angles (>0.8 rad)
        if abs(theta) > 0.8:
            # Smooth activation
            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))

            # Continuous falling severity: scales based on divergence rate
            # When theta*dtheta > 0, pole is falling further from equilibrium
            falling_factor = 1.0 + 0.5 * np.tanh(3.0 * theta * dtheta / (abs(theta) + 0.1))

            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_factor
            force = force + u_swing

        return float(force[0])

# Initialize controller
controller = Controller()

def get_control_action(state):
    force = controller.get_action(state)
    return float(np.clip(force, -100.0, 100.0))
# EVOLVE-BLOCK-END
=======
# EVOLVE-BLOCK-START
class Controller:
    """
    Adaptive LQR Controller with Enhanced Swing-Up Assist and Integral Control
    Combines optimal LQR gains with physics-informed swing-up assistance and
    integral action for zero steady-state error.
    """

    def __init__(self):
        # System parameters
        m = M_POLE
        M = M_CART
        l = L_COM
        g = G
        Mtot = M + m
        denom0 = l * (4.0 / 3.0 - m / Mtot)
        b_c = FRICTION_CART
        b_j = FRICTION_JOINT

        # Physically accurate linearized A matrix
        A = np.zeros((4, 4))
        A[0, 2] = 1.0
        A[1, 3] = 1.0

        # theta_acc row (3) - matches simulator physics
        A[3, 1] = g / denom0
        A[3, 2] = b_c / (Mtot * denom0)
        A[3, 3] = -b_j / (m * l * denom0)

        # x_acc row (2)
        A[2, 1] = -(m * l / Mtot) * A[3, 1]
        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]
        A[2, 3] = b_j / (Mtot * denom0)

        # B matrix
        B = np.zeros((4, 1))
        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)
        B[3, 0] = -1.0 / (Mtot * denom0)

        # Proven optimal LQR weights
        Q = np.diag([4.5, 44.0, 0.6, 3.2])
        R = np.array([[1.0]])

        # Solve LQR gains
        self.K = self.solve_lqr(A, B, Q, R)

        # Precompute natural frequency for normalized falling severity
        self.omega_n = np.sqrt(G / L_COM)

        # Integral control parameters
        self.integral_x = 0.0
        self.K_i = 0.05

    def solve_lqr(self, A, B, Q, R):
        """Solve continuous-time LQR"""
        from scipy.linalg import solve_continuous_are
        P = solve_continuous_are(A, B, Q, R)
        K = np.linalg.inv(R) @ B.T @ P
        return K

    def get_action(self, state):
        """Adaptive LQR control with enhanced swing-up assist and integral control"""
        x, theta, dx, dtheta = state

        # Robust angle normalization using arctan2
        theta = np.arctan2(np.sin(theta), np.cos(theta))

        state_vec = np.array([x, theta, dx, dtheta])
        base_force = -self.K @ state_vec

        # Smooth gain scheduling using tanh activation
        pos_gain = 1.0 + 0.5 * np.tanh(5.0 * max(0.0, abs(theta) - 0.6))
        vel_gain = 1.0 + 0.3 * np.tanh(4.0 * max(0.0, abs(dtheta) - 1.0))

        # Combined multiplicative gain
        adaptive_gain = pos_gain * vel_gain
        force = base_force * adaptive_gain

        # Enhanced swing-up assist for large angles (>0.8 rad)
        if abs(theta) > 0.8:
            # Smooth activation
            swing_activation = np.tanh(6.0 * (abs(theta) - 0.8))

            # Normalized falling severity using angular momentum
            norm_momentum = (theta * dtheta) / (self.omega_n * L_COM)
            falling_severity = 1.0 + np.tanh(3.0 * norm_momentum)

            u_swing = 8.0 * swing_activation * np.sign(theta) * falling_severity
            force += u_swing

        # Soft-switched integral action for cart position
        # Only activates when pole is nearly upright (|theta| < 0.1 rad)
        gate = np.tanh(12.0 * (0.1 - abs(theta)))
        self.integral_x += x * DT * gate
        # Leaky integration to prevent windup
        self.integral_x *= 0.95
        self.integral_x = np.clip(self.integral_x, -0.5, 0.5)
        force -= self.K_i * self.integral_x * gate

        return float(force[0])

# Initialize controller
controller = Controller()

def get_control_action(state):
    force = controller.get_action(state)
    return float(np.clip(force, -100.0, 100.0))
# EVOLVE-BLOCK-END
>>>>>>> REPLACE

</DIFF>