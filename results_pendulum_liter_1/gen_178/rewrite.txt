import numpy as np

# --- Physics Constants ---
M_CART = 1.0
M_POLE = 0.35
L_POLE = 2.5
L_COM = L_POLE / 2
G = 9.81
FRICTION_CART = 0.35
FRICTION_JOINT = 0.25
DT = 0.02
MAX_STEPS = 1000

def simulate_pendulum_step(state, force, dt):
    x, theta, dx, dtheta = state
    sin_theta = np.sin(theta)
    cos_theta = np.cos(theta)
    M_total = M_CART + M_POLE
    f_cart = -FRICTION_CART * dx
    f_joint = -FRICTION_JOINT * dtheta
    temp = (force + f_cart + M_POLE * L_COM * dtheta**2 * sin_theta) / M_total
    theta_acc = (G * sin_theta - cos_theta * temp + f_joint / (M_POLE * L_COM)) / \
                (L_COM * (4.0/3.0 - M_POLE * cos_theta**2 / M_total))
    x_acc = temp - (M_POLE * L_COM * theta_acc * cos_theta) / M_total
    next_x = x + dx * dt
    next_theta = theta + dtheta * dt
    next_dx = dx + x_acc * dt
    next_dtheta = dtheta + theta_acc * dt
    return np.array([next_x, next_theta, next_dx, next_dtheta])


# EVOLVE-BLOCK-START
class Controller:
    """
    MPC-Inspired Phase-Switching Controller with Momentum Nullification
    Uses short-horizon trajectory evaluation combined with LQR for precision control.
    """

    def __init__(self):
        # Precompute LQR gains for precision phase
        m, M, l, g = M_POLE, M_CART, L_COM, G
        Mtot = M + m
        denom0 = l * (4.0 / 3.0 - m / Mtot)
        b_c, b_j = FRICTION_CART, FRICTION_JOINT

        A = np.zeros((4, 4))
        A[0, 2] = 1.0
        A[1, 3] = 1.0
        A[3, 1] = g / denom0
        A[3, 2] = b_c / (Mtot * denom0)
        A[3, 3] = -b_j / (m * l * denom0)
        A[2, 1] = -(m * l / Mtot) * A[3, 1]
        A[2, 2] = -b_c / Mtot - (m * l / Mtot) * A[3, 2]
        A[2, 3] = b_j / (Mtot * denom0)

        B = np.zeros((4, 1))
        B[2, 0] = 1.0 / Mtot + (m * l) / (Mtot**2 * denom0)
        B[3, 0] = -1.0 / (Mtot * denom0)

        Q = np.diag([4.5, 44.0, 0.6, 3.2])
        R = np.array([[1.0]])
        self.K = self._solve_lqr(A, B, Q, R)

        self.integral_x = 0.0
        self.K_i = 0.8
        self.omega_n = np.sqrt(G / L_COM)
        
        # MPC parameters
        self.horizon = 3
        self.n_candidates = 7

    def _solve_lqr(self, A, B, Q, R):
        from scipy.linalg import solve_continuous_are
        P = solve_continuous_are(A, B, Q, R)
        return np.linalg.inv(R) @ B.T @ P

    def _predict_trajectory(self, state, force, steps):
        """Simulate forward trajectory"""
        s = state.copy()
        cost = 0.0
        for _ in range(steps):
            s = simulate_pendulum_step(s, force, DT)
            # Weighted cost emphasizing angle stability
            cost += 44.0 * s[1]**2 + 4.5 * s[0]**2 + 0.6 * s[2]**2 + 3.2 * s[3]**2
        return cost, s

    def _mpc_search(self, state, base_force):
        """Search for best control action using trajectory evaluation"""
        best_force = base_force
        best_cost = float('inf')
        
        # Generate candidate forces around base_force
        spread = min(30.0, 10.0 + 20.0 * abs(state[1]))
        candidates = np.linspace(base_force - spread, base_force + spread, self.n_candidates)
        candidates = np.clip(candidates, -100.0, 100.0)
        
        for f in candidates:
            cost, final_state = self._predict_trajectory(state, f, self.horizon)
            # Add control effort penalty
            cost += 0.5 * f**2 * self.horizon
            # Penalty for predicted failure
            if abs(final_state[1]) > 0.95 or abs(final_state[0]) > 9.0:
                cost += 10000.0
            if cost < best_cost:
                best_cost = cost
                best_force = f
        
        return best_force

    def get_action(self, state):
        x, theta, dx, dtheta = state
        theta = np.arctan2(np.sin(theta), np.cos(theta))
        
        state_vec = np.array([x, theta, dx, dtheta])
        
        # Compute base LQR force
        base_force = float((-self.K @ state_vec)[0])
        
        # Phase detection
        abs_theta = abs(theta)
        abs_dtheta = abs(dtheta)
        
        # Recovery phase: large angles
        if abs_theta > 0.5:
            # Adaptive gain scheduling
            pos_gain = 1.0 + 0.6 * np.tanh(5.0 * (abs_theta - 0.5))
            vel_gain = 1.0 + 0.35 * np.tanh(4.0 * (abs_dtheta - 0.8))
            force = base_force * pos_gain * vel_gain
            
            # MPC refinement for critical situations
            if abs_theta > 0.7:
                force = self._mpc_search(state_vec, force)
            
            # Momentum-aware swing assist
            if abs_theta > 0.8:
                swing_act = np.tanh(6.0 * (abs_theta - 0.8))
                # Normalized momentum term
                norm_mom = (theta * dtheta) / (self.omega_n * L_COM)
                falling_factor = 1.0 + np.tanh(2.5 * norm_mom)
                force += 8.0 * swing_act * np.sign(theta) * falling_factor
        
        # Transition phase: moderate angles
        elif abs_theta > 0.15:
            # Smooth gain scheduling
            pos_gain = 1.0 + 0.4 * np.tanh(6.0 * (abs_theta - 0.15))
            vel_gain = 1.0 + 0.25 * np.tanh(5.0 * (abs_dtheta - 0.5))
            force = base_force * pos_gain * vel_gain
            
            # Sliding mode inspired damping
            lambda_s = 2.0 * self.omega_n
            s = dtheta + lambda_s * theta
            force += -0.6 * np.tanh(2.0 * s)
        
        # Precision phase: small angles
        else:
            force = base_force
            
            # Dual-gated integral control
            angle_gate = np.tanh(15.0 * (0.12 - abs_theta))
            vel_gate = np.tanh(10.0 * (1.0 - abs_dtheta))
            gate = angle_gate * vel_gate
            
            if gate > 0.1:
                self.integral_x += x * DT
            else:
                self.integral_x *= 0.92
            
            force += self.K_i * gate * self.integral_x
            
            # Precision cart damping
            cart_damp = -0.12 * dx * (0.15 - abs_theta) / 0.15
            force += cart_damp
            
            # Angular momentum nullification
            momentum_correction = -0.08 * dtheta * np.sign(theta + 1e-8)
            force += momentum_correction

        return float(force)

controller = Controller()

def get_control_action(state):
    force = controller.get_action(state)
    return float(np.clip(force, -100.0, 100.0))
# EVOLVE-BLOCK-END

def run_simulation(seed=None):
    if seed is not None:
        np.random.seed(seed)
    state = np.array([0.0, 0.9, 0.0, 0.0])
    states = [state]
    forces = []
    for _ in range(MAX_STEPS):
        force = get_control_action(state)
        force = np.clip(force, -100.0, 100.0)
        next_state = simulate_pendulum_step(state, force, DT)
        states.append(next_state)
        forces.append(force)
        state = next_state
    return np.array(states), np.array(forces)