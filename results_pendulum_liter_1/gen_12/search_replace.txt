<NAME>
gain_scheduling_lqr
</NAME>

<DESCRIPTION>
This change implements gain scheduling to use more aggressive LQR gains when the pole is near vertical (|theta| < 0.3 rad). The current conservative gains work well for large angles but are too slow for fine stabilization. By switching to higher-gain LQR when the system is close to equilibrium, we can achieve faster convergence while maintaining robustness during large-angle phases.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
class Controller:
    """
    Suboptimal LQR Controller for Single Inverted Pendulum Stabilization.
    次优LQR控制器 - 能稳住，但参数故意调得"懒惰"。

    特点：
    1. 物理环境更难（杆更长更重，摩擦更大）。
    2. Q矩阵参数较小：对误差容忍度高 -> 精度分低。
    3. R矩阵参数较大：不愿用大力 -> 响应慢，能耗高。

    目标：初始分数 ~3000 分，进化后可达 9000+ 分。
    """

    def __init__(self):
        # 系统参数
        m = M_POLE  # 摆杆质量
        M = M_CART  # 小车质量
        l = L_COM   # 质心距离
        g = G       # 重力加速度

        # 线性化状态空间模型
        A = np.array([
            [0, 0, 1, 0],
            [0, 0, 0, 1],
            [0, -m*g/M, 0, 0],
            [0, (m+M)*g/(M*l), 0, 0]
        ])

        B = np.array([
            [0],
            [0],
            [1/M],
            [-1/(M*l)]
        ])

        # LQR权重矩阵（更保守的次优设置，留出充分进化空间）
        # Q: 显著降低权重，高度容忍误差 - 导致基础分数低
        # R: 大幅增加权重，极度限制出力 - 导致响应慢，能耗高
        Q = np.diag([2.0, 8.0, 0.05, 0.4])    # [x, theta, dx, dtheta] (更保守)
        R = np.array([[3.0]])                  # 控制力惩罚极大 (非常不愿意用大力)

        # 求解LQR增益
        self.K = self.solve_lqr(A, B, Q, R)

    def solve_lqr(self, A, B, Q, R):
        """求解连续时间LQR问题"""
        from scipy.linalg import solve_continuous_are
        P = solve_continuous_are(A, B, Q, R)
        K = np.linalg.inv(R) @ B.T @ P
        return K

    def get_action(self, state):
        """LQR控制法则: u = -K * x"""
        x, theta, dx, dtheta = state
        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi
        state_vec = np.array([x, theta, dx, dtheta])
        force = -self.K @ state_vec
        return float(force[0])
=======
class Controller:
    """
    Gain-Scheduled LQR Controller for Single Inverted Pendulum Stabilization.
    Uses conservative gains for large angles and aggressive gains near vertical.

    特点：
    1. 大角度时使用保守增益保证稳定性
    2. 小角度时使用激进增益加速收敛
    3. 平滑切换避免抖动
    """

    def __init__(self):
        # 系统参数
        m = M_POLE  # 摆杆质量
        M = M_CART  # 小车质量
        l = L_COM   # 质心距离
        g = G       # 重力加速度

        # 线性化状态空间模型
        A = np.array([
            [0, 0, 1, 0],
            [0, 0, 0, 1],
            [0, -m*g/M, 0, 0],
            [0, (m+M)*g/(M*l), 0, 0]
        ])

        B = np.array([
            [0],
            [0],
            [1/M],
            [-1/(M*l)]
        ])

        # 保守LQR权重（大角度使用）
        Q_conservative = np.diag([2.0, 8.0, 0.05, 0.4])
        R_conservative = np.array([[3.0]])

        # 激进LQR权重（小角度使用）- 更高精度和更快响应
        Q_aggressive = np.diag([5.0, 20.0, 0.1, 1.0])
        R_aggressive = np.array([[1.0]])

        # 求解两组LQR增益
        self.K_conservative = self.solve_lqr(A, B, Q_conservative, R_conservative)
        self.K_aggressive = self.solve_lqr(A, B, Q_aggressive, R_aggressive)

        # 切换阈值
        self.theta_threshold = 0.3  # rad

    def solve_lqr(self, A, B, Q, R):
        """求解连续时间LQR问题"""
        from scipy.linalg import solve_continuous_are
        P = solve_continuous_are(A, B, Q, R)
        K = np.linalg.inv(R) @ B.T @ P
        return K

    def get_action(self, state):
        """增益调度LQR控制法则"""
        x, theta, dx, dtheta = state
        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi

        # 根据角度大小选择增益
        if abs(theta) < self.theta_threshold:
            K = self.K_aggressive
        else:
            K = self.K_conservative

        state_vec = np.array([x, theta, dx, dtheta])
        force = -K @ state_vec
        return float(force[0])
>>>>>>> REPLACE
</DIFF>